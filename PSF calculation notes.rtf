{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww18940\viewh16860\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Test against Matlab\ulnone \
\uc0\u8730  Confirmed that psfLine matches (both for z starting at -0.5 and z starting at -5.0)\
\uc0\u8730  Confirmed that calcML matches (for z starting at -5.0 - though I think it does not depend on z)\
\uc0\u8730  Confirmed that calcPSF_p matches - psfWAVE_STACK and LFpsfWAVE_STACK agree with the Matlab. Required a bug fix to Matlab code for z=0.5Um\
\uc0\u8730  Confirmed that H matches\
\uc0\u8730  Confirmed that Ht matches. I have also written new code to calculate Ht, which is astronomically faster than their original code (since I eliminate the convolution and a lot of other associated unnecessary work)\
\
\ul Bug in their code\
\ulnone There is a bug in their code when z=0, whereby the max() operator does not work as they intend in calcPSF. I have added a workaround for that bug in my copy of their code (and fixed it in my python code!)\
\
\ul Be aware\ulnone \
- Because there is a step where they normalise H by max(H), the scaling of H will depend on what z range the PSF is generated with. I don\'92t actually know why they do that - it doesn\'92t seem to be logical as an actual [flux] normalization. It is however something I will have to bear in mind if I ever intend to fuse together multiple short sections of PSF into one big PSF file. If the scaling matters (and surely it does\'85?) then I will only be able to know how to fuse them if there is an overlap in the z range between the different PSF sections.\
I now save the maximum value encountered (pre-normalisation) in the .mat file. That should allow me to fuse PSFs I have generated myself, without accumulating successive rounding errors. This seems like a low-impact way of making things better.\
\ul \
Investigate\ulnone \
- I seem to remember seeing some slightly odd effects in the off-axis reconstruction, where it was reconstructing a rectangle with a single-pixel-wide strip of a different intensity at the edge. It\'92s not impossible that that could be an off-by-one bug in their PSF generation, so I should investigate that I think\'85\
\
\
\ul Performance - stage 1\
\ulnone - This is a massively parallel problem that should be easily threadable - PSF is computed independently for each point.\
- Remember that I am having to do the real and imag part integrals separately, which will inevitably slow things down by perhaps a factor of 2. Possibly more since it will obsess over e.g. the accuracy of imaginary parts when the real part is actually substantially larger.\
- I doubt I can do anything more to exploit symmetry to reduce the work that needs to be done - the original code already seems to do that.\
- I have massively speeded up my code by using cython, and replacing scipy.special.j0 with the GSL equivalent. It\'92s overkill to link GSL, but has helped a lot. I suspect I am genuinely limited by computation now, and would struggle to speed things up further without an algebraic change to the calculation.\
- My single-threaded run time is ~625s for z=-26, 85.9s@8x. ~7.2x speedup over original, which is pretty reasonable.\
\
\ul \ulc0 Performance - stage 2\
\ulnone z=-26 took 57s@8x. This is a 6.2x speedup over single-threaded (6m04). I imagine the reason this isn\'92t any more efficient is because there is quite a lot of data to pickle.\
\
\
\
Stage 1 (\'93computing PSFs\'94) took 27:46 to do the entire calculation for planes 26:2:0\
Stage 2 (\'93computing LF PSFs\'94) took 15:43\
}