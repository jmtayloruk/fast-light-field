{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMINDER: this notebook may now be out of sync with generate_psf.py..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a .mat file representing the light field PSF\n",
    "This code is very closely modelled on Prevedel's original Matlab code (but with a bug fix for the z=0 plane, and with correct normalisation of the H matrix)\n",
    "\n",
    "## Before running\n",
    "This code relies on the small `light_field_integrands` module. This can be installed by going to the `light-field-integrands` subfolder and running `python setup.py install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special, scipy.integrate, scipy.signal, scipy.misc\n",
    "import h5py, sys, types, time, os, h5py, warnings, cProfile, pstats, multiprocessing, psutil, gc\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import light_field_integrands\n",
    "import jutils as util\n",
    "\n",
    "# Determine how many parallel threads we should run for our calculations.\n",
    "# I want this to be based on the number of physical cores (i.e. without hyperthreading).\n",
    "# There are only tiny incremental gains from going to hyperthreading, presumably because\n",
    "# this code is highly cpu-bound.\n",
    "numJobs = util.PhysicalCoreCount() \n",
    "print('Will distribute work across {0} cores'.format(numJobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory usage monitoring\n",
    "\n",
    "At one point I found that there was an extra 3GB of memory allocated that I could not account for. e.g. usage was 8GB (even after freeing _H1 etc) whereas there was only >5GB accounted for by the global arrays. Walking the globals list, getsize still only found 5GB of objects. The missing 3GB must be hiding at an even lower level than that. I suppose I'll just need to keep an eye on whether this happens again - but I can only imagine it's something associated with the Jupyter notebook, and/or possibly to do with file caching, or maybe even a genuine memory leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for monitoring memory usage\n",
    "\n",
    "def getsize(obj, returnIfBlacklisted=None, seen_ids=set()):\n",
    "    # Copied from https://stackoverflow.com/questions/449560/how-do-i-determine-the-size-of-an-object-in-python\n",
    "    \"\"\"sum size of object & members.\"\"\"\n",
    "\n",
    "    # Custom objects know their class.\n",
    "    # Function objects seem to know way too much, including modules.\n",
    "    # Exclude modules as well.\n",
    "    BLACKLIST = type, types.ModuleType, types.FunctionType\n",
    "\n",
    "    if isinstance(obj, BLACKLIST):\n",
    "        if returnIfBlacklisted is not None:\n",
    "            return returnIfBlacklisted\n",
    "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
    "    size = 0\n",
    "    objects = [obj]\n",
    "    while objects:\n",
    "        need_referents = []\n",
    "        for obj in objects:\n",
    "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
    "                seen_ids.add(id(obj))\n",
    "                size += sys.getsizeof(obj)\n",
    "                need_referents.append(obj)\n",
    "        objects = gc.get_referents(*need_referents)\n",
    "    return size\n",
    "\n",
    "def PrintMemoryUsage(info=\"\", thresh=1e6, verbose=False):\n",
    "    array_ids = set()\n",
    "    for k, v in list(globals().items()):\n",
    "        if type(v) in [np.ndarray]:\n",
    "            array_ids.add(id(v))\n",
    "    sSum = 0\n",
    "    for k, v in list(globals().items()):\n",
    "        seen_ids = array_ids.copy()\n",
    "        seen_ids.discard(id(v))\n",
    "        s = getsize(v, returnIfBlacklisted=0, seen_ids=seen_ids)\n",
    "        if (s > thresh):\n",
    "            if type(v) is list:\n",
    "                print(type(v), k, 'len %d size %.1fMB' % (len(v), s/1e6))\n",
    "            else:\n",
    "                print(type(v), k, '%.1fMB'%(s/1e6))\n",
    "        sSum += s\n",
    "    print('Total memory usage%s: %.1fGB' % (info, sSum/1e9))\n",
    "    if verbose:\n",
    "        print(psutil.virtual_memory())\n",
    "\n",
    "PrintMemoryUsage(thresh=100e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSF generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisePSF = True\n",
    "# Set to True in the unusual case that we do actually want to reproduce\n",
    "# the bug present at plane z=0 in Prevedel's Matlab code\n",
    "reproduceMaxBug = False\n",
    "verbose = True\n",
    "\n",
    "if False:\n",
    "    M = 40\n",
    "    NA = 0.95\n",
    "    MLPitch = 150e-6\n",
    "    Nnum = 15\n",
    "    OSR = 3\n",
    "    n = 1.0\n",
    "    fml = 3000e-6\n",
    "    lam = 520e-9\n",
    "    zmin = -26e-6\n",
    "    zmax = 0\n",
    "    zspacing = 2e-6\n",
    "else:\n",
    "    M = 22.2\n",
    "    NA = 0.5\n",
    "    MLPitch = 125e-6\n",
    "    Nnum = 19\n",
    "    OSR = 3\n",
    "    n = 1.33\n",
    "    fml = 3125e-6\n",
    "    lam = 520e-9\n",
    "    zmin = -156e-6\n",
    "    zmax = 156e-6\n",
    "    zspacing = 4e-6\n",
    "\n",
    "if False:\n",
    "    warnings.warn('Reduced problem size')\n",
    "    zmin = -12e-6\n",
    "    zmax = -8e-6   \n",
    "    \n",
    "if not normalisePSF:\n",
    "    warnings.warn('We are not normalising the PSF. It should be normalised for standard deconvolution!')\n",
    "if reproduceMaxBug:\n",
    "    warnings.warn('Reproducing matlab bug!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqtol = 1e-10;\n",
    "\n",
    "k = 2*np.pi*n/lam\n",
    "k0 = 2*np.pi*1/lam\n",
    "d = fml\n",
    "ftl = 200e-3          #focal length of tube lens\n",
    "fobj = ftl/M          # focal length of objective lens\n",
    "fnum_obj = M/(2*NA)   # f-number of objective lens (imaging-side)\n",
    "fnum_ml = fml/MLPitch # f-number of microlens\n",
    "\n",
    "assert((Nnum%2)==1), 'Nnum must be an odd number'\n",
    "assert((OSR%2)==1), 'OSR must be an odd number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JT: Code to load a .mat file generated by the actual Matlab code, for comparison with ours\n",
    "def MatrixFileString():\n",
    "    # Generates the long file string that the Matlab code generates based on the PSF parameters.\n",
    "    # Currently, I just treat the variables as globals, rather than accepting them all as parameters to this function.\n",
    "    return 'M%gNA%gMLPitch%gfml%gfrom%gto%gzspacing%gNnum%glambda%gn%g'%(M, NA, MLPitch*1e6, fml*1e6, zmin*1e6, zmax*1e6, zspacing*1e6, Nnum, lam*1e9, n)\n",
    "\n",
    "def LoadRawMatrixData(matPath, expectedNnum):\n",
    "    # Load the matrices from the .mat file.\n",
    "    # This is slow since they must be decompressed and are rather large! (9.5GB each, in single-precision FP)\n",
    "    hReducedShape = []\n",
    "    htReducedShape = []\n",
    "    with h5py.File(matPath, 'r') as f:\n",
    "        print('Load CAindex')\n",
    "        sys.stdout.flush()\n",
    "        _CAindex = f['CAindex'].value.astype('int')\n",
    "        _Nnum = f['Nnum'].value.astype('int')\n",
    "        if _Nnum != expectedNnum:\n",
    "            warnings.warn('Nnum={0} from file does not match current global variable value {1}'.format(_Nnum, expectedNnum))\n",
    "        try:\n",
    "            reducedMatrix = f['ReducedMatrix'].value.astype('int')\n",
    "        except:\n",
    "            print('No reduced flag - will assume this is a full matrix')\n",
    "            reducedMatrix = False\n",
    "            \n",
    "        print('Load H')\n",
    "        sys.stdout.flush()\n",
    "        _H = f['H'].value.astype('float32')\n",
    "        \n",
    "        aabbRange = int((_Nnum+1)/2)        \n",
    "        if reducedMatrix:\n",
    "            assert(_H.shape[2] == aabbRange)            \n",
    "        else:\n",
    "            assert(_H.shape[2] == _Nnum)\n",
    "        \n",
    "        for cc in range(_H.shape[0]):\n",
    "            HCC =  _H[cc, :aabbRange, :aabbRange, _CAindex[0,cc]-1:_CAindex[1,cc], _CAindex[0,cc]-1:_CAindex[1,cc]]\n",
    "            hReducedShape.append(HCC.shape)\n",
    "\n",
    "        print('Load Ht')\n",
    "        sys.stdout.flush()\n",
    "        _Ht = f['Ht'].value.astype('float32')\n",
    "\n",
    "        for cc in range(_Ht.shape[0]):\n",
    "            HtCC =  _Ht[cc, :aabbRange, :aabbRange, _CAindex[0,cc]-1:_CAindex[1,cc], _CAindex[0,cc]-1:_CAindex[1,cc]]\n",
    "            htReducedShape.append(HtCC.shape)\n",
    "                    \n",
    "    return (_H, _Ht, hReducedShape, htReducedShape, _CAindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: imaging PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPSFFT(p3, fobj, NA, x1space, scale, lam, fml, M, n):     #âˆš\n",
    "    k = 2*np.pi*n/lam\n",
    "    alpha = np.arcsin(NA/n)\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    psfLine = np.zeros((len(x1space)))\n",
    "    integrandCython_r = scipy.LowLevelCallable.from_cython(light_field_integrands, 'integrandPSF_r')\n",
    "    integrandCython_i = scipy.LowLevelCallable.from_cython(light_field_integrands, 'integrandPSF_i')\n",
    "\n",
    "    for a in tqdm(range(len(x1space))):\n",
    "        x1 = x1space[a]\n",
    "        x2 = 0\n",
    "        xL2normsq = (((x1+M*p1)**2+(x2+M*p2)**2)**0.5)/M\n",
    "        v = k*xL2normsq*np.sin(alpha)   \n",
    "        u = 4*k*p3*(np.sin(alpha/2)**2)\n",
    "\n",
    "        Koi = M/((fobj*lam)**2)*np.exp(-1j*u/(4*(np.sin(alpha/2)**2)))\n",
    "        if False:\n",
    "            # Old, slow pure python code, left here for reference\n",
    "    #        integrand = @(theta) (sqrt(cos(theta))) .* (1+cos(theta))  .*  (exp(-(i*u/2)* (sin(theta/2).^2) / (sin(alpha/2)^2)))  .*  (besselj(0, sin(theta)/sin(alpha)*v))  .*  (sin(theta));\n",
    "            integrand = lambda theta: (np.sqrt(np.cos(theta))) * (1+np.cos(theta))  \\\n",
    "                                        *  (np.exp(-(1j*u/2)* (np.sin(theta/2)**2) / (np.sin(alpha/2)**2))) \\\n",
    "                                        *  (scipy.special.jn(0, np.sin(theta)/np.sin(alpha)*v)) \\\n",
    "                                        *  (np.sin(theta))\n",
    "    #        I0 = integral(@(theta)integrand (theta),0,alpha);  \n",
    "            integrand_r = lambda theta: np.real(integrand(theta))\n",
    "            integrand_i = lambda theta: np.imag(integrand(theta))\n",
    "            # JT: I have bumped up the subdivision limit to 80 in order to silence warnings about problems\n",
    "            # in the integration. However, I suspect it probably doesn't need this level of detail...\n",
    "            I0_r2,err_r2 = scipy.integrate.quad(integrand_r, 0, alpha, limit=80)\n",
    "            I0_i2,err_i2 = scipy.integrate.quad(integrand_i, 0, alpha, limit=80)\n",
    "        if True:\n",
    "            # New fast code (using cython for speed)\n",
    "            alphaFactor = np.sin(alpha/2)**(-2)\n",
    "            uOver2 = u/2\n",
    "            vFactor = v/np.sin(alpha)\n",
    "            I0_r,err_r = scipy.integrate.quad(integrandCython_r, 0, alpha, limit=180, args=(alphaFactor, uOver2, vFactor))\n",
    "            I0_i,err_i = scipy.integrate.quad(integrandCython_i, 0, alpha, limit=180, args=(alphaFactor, uOver2, vFactor))\n",
    "        \n",
    "        I0 = (I0_r + 1j*I0_i)\n",
    "        err = (err_r + 1j*err_i)\n",
    "        psfLine[a] =  np.abs((Koi*I0)**2)\n",
    "    return psfLine / np.max(psfLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelPitch = MLPitch/Nnum # pitch of virtual pixels\n",
    "\n",
    "# JT: not sure why these first two are created as single-element arrays \n",
    "#     - maybe a feature that they never implemented?\n",
    "x1objspace = np.array([0])\n",
    "x2objspace = np.array([0])\n",
    "x3objspace = np.arange(zmin, zmax+0.1*zspacing, zspacing)\n",
    "objspace = np.ones((len(x1objspace),len(x2objspace),len(x3objspace)))\n",
    "# JT: I am not completely sure why, but the code has to work with at least two different z coordinates.\n",
    "# Having only one ultimately leads to division-by-zero in calculating IMGSIZE_REF_IL (because p3max==0),\n",
    "# but I don't follow what IMGSIZE has to do with the total of z planes we have.\n",
    "# I wonder if it might be something to do with having a generous estimate of how rapidly the PSF will spread\n",
    "# as a function of z coordinate...\n",
    "assert(len(x3objspace) > 1)\n",
    "\n",
    "p3max = np.max(np.abs(x3objspace))\n",
    "x1testspace = (pixelPitch/OSR) * np.arange(0, Nnum*OSR*20 +1) #âˆš  [Matlab really does start at 0]\n",
    "x2testspace = [0]   \n",
    "psfLine = calcPSFFT(p3max, fobj, NA, x1testspace, pixelPitch/OSR, lam, d, M, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outArea = np.where(psfLine<0.04)[0]\n",
    "if len(outArea) == 0:  #âˆš [checked that this logic works]\n",
    "    raise ValueError('Estimated PSF size exceeds the limit')\n",
    "IMGSIZE_REF = int(np.ceil(outArea[0]/(OSR*Nnum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcML(fml, k, x1MLspace, x2MLspace, x1space, x2space):  #âˆš\n",
    "    x1length = len(x1space)\n",
    "    x2length = len(x2space)\n",
    "    x1MLdist = len(x1MLspace)\n",
    "    x2MLdist = len(x2MLspace)\n",
    "    # JT: the Matlab here is a very strange code construction, but its aim appears to be to identify\n",
    "    # one (any) index in x1space that is ==0, and take that as one 'center' in x1.\n",
    "    # It then constructs a list of indices that represents *all* the 'centers' in x1.\n",
    "    # The code relies on the fact that (x1center: -x1MLdist:1) is defined in matlab to use \n",
    "    # only the value of the first element of the array x1center when generating the range.\n",
    "    # original Matlab:\n",
    "    #   x1center = find(x1space==0);\n",
    "    #   x1centerALL = [  (x1center: -x1MLdist:1)  (x1center + x1MLdist: x1MLdist :x1length)];\n",
    "    #   x1centerALL = sort(x1centerALL);\n",
    "    x1center = np.where(x1space==0)[0][0]\n",
    "    x1centerALL_p = np.append(np.arange(x1center, -1, -x1MLdist), \\\n",
    "                              np.arange(x1center+x1MLdist, x1length, x1MLdist)) #âˆš for python array indexing\n",
    "    np.sort(x1centerALL_p)\n",
    "    x2center = np.where(x2space==0)[0][0]\n",
    "    x2centerALL_p = np.append(np.arange(x2center, -1, -x2MLdist), \\\n",
    "                              np.arange(x2center+x2MLdist, x2length, x2MLdist)) #âˆš for python array indexing\n",
    "    np.sort(x2centerALL_p)\n",
    "\n",
    "    patternML = np.zeros((len(x1MLspace), len(x2MLspace)), dtype='complex128')\n",
    "    patternMLcp = np.zeros((len(x1MLspace), len(x2MLspace)), dtype='complex128')\n",
    "    for a in range(len(x1MLspace)):\n",
    "        for b in range(len(x2MLspace)):\n",
    "            x1 = x1MLspace[a]\n",
    "            x2 = x2MLspace[b]\n",
    "            xL2norm = x1**2 + x2**2\n",
    "            patternML[a,b] = np.exp(-1j*k/(2*fml)*xL2norm)\n",
    "            patternMLcp[a,b] = np.exp(-0.05*1j*k/(2*fml)*xL2norm) \n",
    "    MLcenters = np.zeros((len(x1space), len(x2space)))\n",
    "    for a in range(len(x1centerALL_p)):\n",
    "        for b in range(len(x2centerALL_p)):\n",
    "            MLcenters[x1centerALL_p[a], x2centerALL_p[b]] = 1\n",
    "    MLARRAY = scipy.signal.fftconvolve(MLcenters.astype('complex128'), patternML, 'same')\n",
    "    return MLARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HALFWIDTH = np.maximum(Nnum*(IMGSIZE_REF + 1), 2*Nnum)\n",
    "if verbose:\n",
    "    print('Size of PSF ~= {0} [microlens pitch]'.format(IMGSIZE_REF))\n",
    "    print('Size of IMAGE = {0}x{1}'.format(IMG_HALFWIDTH*2*OSR+1, IMG_HALFWIDTH*2*OSR+1))\n",
    "x1space = (pixelPitch/OSR)*np.arange(-IMG_HALFWIDTH*OSR, IMG_HALFWIDTH*OSR+0.1, 1);   #âˆš? not sure if this is array indexing\n",
    "x2space = (pixelPitch/OSR)*np.arange(-IMG_HALFWIDTH*OSR, IMG_HALFWIDTH*OSR+0.1, 1); \n",
    "x1length = len(x1space)\n",
    "x2length = len(x2space)\n",
    "\n",
    "x1MLspace = (pixelPitch/OSR)* np.arange(-(Nnum*OSR-1)/2 , (Nnum*OSR-1)/2+0.1, 1)\n",
    "x2MLspace = (pixelPitch/OSR)* np.arange(-(Nnum*OSR-1)/2 , (Nnum*OSR-1)/2+0.1, 1)\n",
    "x1MLdist = len(x1MLspace)\n",
    "x2MLdist = len(x2MLspace)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%% FIND NON-ZERO POINTS %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "validpts = np.where(objspace>eqtol)\n",
    "numpts = len(validpts[0])\n",
    "# Matlab code:\n",
    "#  [p1indALL p2indALL p3indALL] = ind2sub( size(objspace), validpts);\n",
    "#  p1ALL = x1objspace(p1indALL)';\n",
    "(p1indALL, p2indALL, p3indALL) = validpts\n",
    "p1ALL = x1objspace[p1indALL]\n",
    "p2ALL = x2objspace[p2indALL]\n",
    "p3ALL = x3objspace[p3indALL]\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%% DEFINE ML ARRAY %%%%%%%%%%%%%%%%%%%%%%%%% \n",
    "MLARRAY = calcML(fml, k0, x1MLspace, x2MLspace, x1space, x2space)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%% Alocate Memory for storing PSFs %%%%%%%%%%%   \n",
    "LFpsfWAVE_STACK = np.zeros((x1length, x2length, numpts), dtype='complex128')\n",
    "psfWAVE_STACK = np.zeros((x1length, x2length, numpts), dtype='complex128')\n",
    "if verbose:\n",
    "    print('Allocated two psfWAVE_STACK arrays, each of size %.1fMB' % (psfWAVE_STACK.size*psfWAVE_STACK.itemsize/1e6))        \n",
    "# Note: if, when this cell is run, a warning appears about multidimensional indexing,\n",
    "# this is due to an internal issue in scipy (which I think can be fixed by upgrading to the latest scipy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: \"Projection from single point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fresnel2D(f0,dx0,z,lam):  #âˆš\n",
    "    (Nx,Ny) = f0.shape\n",
    "    k = 2*np.pi/lam\n",
    "\n",
    "    du = 1/(Nx*dx0)\n",
    "    u = np.append(np.arange(0,np.ceil(Nx/2)), np.arange(np.ceil(-Nx/2),0))*du  #âˆš\n",
    "    dv = 1/(Ny*dx0)\n",
    "    v = np.append(np.arange(0,np.ceil(Ny/2)), np.arange(np.ceil(-Ny/2),0))*dv  #âˆš\n",
    "\n",
    "    #âˆš think I checked this.\n",
    "    #(although there is probably a much more legible way to do this in python with meshgrid or similar,\n",
    "    # and indeed with fftshift as well!)\n",
    "    H = np.exp(-1j*2*np.pi**2 * (np.tile(u[:,np.newaxis],(1,len(v)))**2+np.tile(v,(len(u),1))**2)*z/k)  \n",
    "    f1 = np.exp(1j*k*z)*np.fft.ifft2(np.fft.fft2(f0) * H )\n",
    "    dx1 = dx0\n",
    "    x1 = np.arange(-Nx/2,Nx/2)*dx1\n",
    "    return f1,dx1,x1                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPSFForA(a, p1, p2, p3, fobj, NA, x1, x2space, scale, lam, fml, M, n, centerArea_p, \\\n",
    "                  x1length, x2length, centerPT_m):\n",
    "    k = 2*np.pi*n/lam\n",
    "    alpha = np.arcsin(NA/n)\n",
    "\n",
    "    patternLine = np.zeros(len(x2space), dtype='complex128')\n",
    "    integrandCython_r = scipy.LowLevelCallable.from_cython(light_field_integrands, 'integrandPSF_r')\n",
    "    integrandCython_i = scipy.LowLevelCallable.from_cython(light_field_integrands, 'integrandPSF_i')\n",
    "    for b in range(a,centerPT_m):  #âˆš\n",
    "        x2 = x2space[b]\n",
    "        xL2normsq = (((x1+M*p1)**2+(x2+M*p2)**2)**0.5)/M\n",
    "\n",
    "        v = k*xL2normsq*np.sin(alpha)\n",
    "        u = 4*k*(p3*1)*(np.sin(alpha/2)**2)\n",
    "        Koi = M/((fobj*lam)**2)*np.exp(-1j*u/(4*(np.sin(alpha/2)**2)))\n",
    "        # JT: a tradeoff is required with the tolerance.\n",
    "        # I see strangely incorrect results with tol=1e-10 (see \"digression\" cell below),\n",
    "        # but if I use 1e-15 then I get warnings about accuracy vs roundoff error.\n",
    "        # 1e-12 seems to be a workable compromise.\n",
    "        tol = 1e-12\n",
    "        if False:\n",
    "            # Old, slow pure python code for reference\n",
    "            #intgrand = @(theta) (sqrt(cos(theta))) .* (1+cos(theta))  .*  (exp(-(i*u/2)* (sin(theta/2).^2) / (sin(alpha/2)^2)))  .*  (besselj(0, sin(theta)/sin(alpha)*v))  .*  (sin(theta));\n",
    "            #I0 = integral(@(theta)intgrand (theta),0,alpha);  \n",
    "            def integrand(theta, alpha, u, v):\n",
    "                return (np.sqrt(np.cos(theta))) * (1+np.cos(theta))  \\\n",
    "                                    *  (np.exp(-(1j*u/2)* (np.sin(theta/2)**2) / (np.sin(alpha/2)**2))) \\\n",
    "                                    *  (scipy.special.jn(0, np.sin(theta)/np.sin(alpha)*v)) \\\n",
    "                                    *  (np.sin(theta))\n",
    "            integrand_r = lambda theta: np.real(integrand(theta, alpha, u, v))\n",
    "            integrand_i = lambda theta: np.imag(integrand(theta, alpha, u, v))\n",
    "            I0_r2,err_r = scipy.integrate.quad(integrand_r, 0, alpha, limit=180,epsabs=tol,epsrel=tol)\n",
    "            I0_i2,err_i = scipy.integrate.quad(integrand_i, 0, alpha, limit=180,epsabs=tol,epsrel=tol)\n",
    "        if True:\n",
    "            # New fast code\n",
    "            alphaFactor = np.sin(alpha/2)**(-2)\n",
    "            uOver2 = u/2\n",
    "            vFactor = v/np.sin(alpha)\n",
    "            I0_r,err_r = scipy.integrate.quad(integrandCython_r, 0, alpha, args=(alphaFactor, uOver2, vFactor),limit=180,epsabs=tol,epsrel=tol)\n",
    "            I0_i,err_i = scipy.integrate.quad(integrandCython_i, 0, alpha, args=(alphaFactor, uOver2, vFactor),limit=180,epsabs=tol,epsrel=tol)\n",
    "\n",
    "        I0 = (I0_r + 1j*I0_i)\n",
    "        err = (err_r + 1j*err_i)\n",
    "        patternLine[b] = Koi*I0\n",
    "    return a, patternLine\n",
    "\n",
    "def calcPSF_p(p1, p2, p3, fobj, NA, x1space, x2space, scale, lam, MLARRAY, fml, M, n, centerArea_p):  #âˆš\n",
    "    x1length = len(x1space)\n",
    "    x2length = len(x2space)\n",
    "    centerPT_m = int(np.ceil(x1length/2))     #âˆšMatlab indexing\n",
    "    pattern = np.zeros((x1length, x2length), dtype='complex128')\n",
    "    if False:\n",
    "        # Slow, single-threaded code for reference\n",
    "        for a in tqdm(range(centerArea_p[0],centerPT_m), leave=False):   #âˆš\n",
    "            (a,patternLine) = calcPSFForA(a, p1, p2, p3, fobj, NA, x1space[a], x2space, scale, \\\n",
    "                                      lam, fml, M, n, centerArea_p, x1length, x2length, centerPT_m)\n",
    "            pattern[a,:] = patternLine\n",
    "    else:\n",
    "        # Multithreaded code\n",
    "        patternLines = Parallel(n_jobs=numJobs)(delayed(calcPSFForA)(a, p1, p2, p3, fobj, NA, x1space[a], x2space, scale, lam, fml, M, n, centerArea_p, \\\n",
    "                                                                  x1length, x2length, centerPT_m) \\\n",
    "                                                  for a in tqdm(range(centerArea_p[0],centerPT_m), leave=False))\n",
    "        for a,patternLine in patternLines:\n",
    "            pattern[a,:] = patternLine\n",
    "\n",
    "    patternA = pattern[0:centerPT_m, 0:centerPT_m];   #âˆš\n",
    "    patternAt = np.fliplr(patternA)\n",
    "\n",
    "    pattern3D = np.zeros((pattern.shape[0], pattern.shape[1], 4), dtype='complex128');\n",
    "    pattern3D[:,:,0] = pattern;\n",
    "    pattern3D[:centerPT_m, centerPT_m-1:,0] = patternAt   #âˆš\n",
    "    # JT: empirically, this does rotate in the same direction as matlab (when the indexing order\n",
    "    # is identical in both cases). However, it shouldn't matter because we consider all four rotations\n",
    "    # and take the maximum!\n",
    "    pattern3D[:,:,1] = np.rot90( pattern3D[:,:,0] , -1)\n",
    "    pattern3D[:,:,2] = np.rot90( pattern3D[:,:,0] , -2)\n",
    "    pattern3D[:,:,3] = np.rot90( pattern3D[:,:,0] , -3)\n",
    "    # JT: unfortunately it is a pain to do the simple 'max' in python.\n",
    "    # Matlab takes the maximum abs(z), whereas python silently takes the maximum real(z).\n",
    "    # I can't see an obvious and tidy way to code what I need in python.\n",
    "    # I think the following should work as a quick bodge, and this shouldn't be a bottleneck\n",
    "    if reproduceMaxBug and (np.abs(p3) < 1e-10):\n",
    "        warnings.warn('Reproducing matlab bug!')\n",
    "        pattern = np.max(pattern3D, axis=2)\n",
    "    else:\n",
    "        pattern = pattern3D[:,:,0].copy()\n",
    "        pattern[pattern == 0] = pattern3D[:,:,1][pattern == 0]\n",
    "        pattern[pattern == 0] = pattern3D[:,:,2][pattern == 0]\n",
    "        pattern[pattern == 0] = pattern3D[:,:,3][pattern == 0]\n",
    "\n",
    "#    %%%%%%%%%%%%%%%%%%% CALCULATED LF PSF %%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    f1,dx1,x1 = fresnel2D(pattern*MLARRAY, scale, 1*fml, lam)\n",
    "\n",
    "    return pattern, f1, pattern3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "centerPT_m = int(np.ceil(len(x1space)/2)) #âˆš\n",
    "halfWidth =  Nnum*(IMGSIZE_REF + 0 )*OSR\n",
    "centerArea_p = np.arange(np.maximum((centerPT_m - halfWidth),1)-1,          #âˆš\n",
    "                       np.minimum((centerPT_m + halfWidth),len(x1space)-1))\n",
    "\n",
    "warnings.resetwarnings()\n",
    "\n",
    "def PSFFunc(eachpt):\n",
    "    # JT: I have separated this out into a separate function just in case I ever decide to\n",
    "    # go back to threading over eachpt. The drawback of that is that the work sizes are very different.\n",
    "    # It might still work, but for now I am happy that threading within calcPSF_p seems to be sufficient.\n",
    "    p1 = p1ALL[eachpt]\n",
    "    p2 = p2ALL[eachpt]\n",
    "    p3 = p3ALL[eachpt]\n",
    "    \n",
    "    IMGSIZE_REF_IL = np.ceil(IMGSIZE_REF*( np.abs(p3)/p3max))\n",
    "    halfWidth_IL =  np.maximum(Nnum*(IMGSIZE_REF_IL + 0 )*OSR, 2*Nnum*OSR)\n",
    "    centerArea_IL_p = np.arange(np.maximum((centerPT_m - halfWidth_IL),1)-1,\n",
    "                              np.minimum((centerPT_m + halfWidth_IL),len(x1space)), dtype=np.int)   #âˆš\n",
    "    if verbose:\n",
    "        print('Plane {0}: size of center area = {1}x{2}'.format(eachpt, len(centerArea_IL_p), len(centerArea_IL_p)))\n",
    "    \n",
    "    # excute PSF computing function\n",
    "    if True:\n",
    "        t1 = time.time()\n",
    "        psfWAVE, LFpsfWAVE, pattern3D = calcPSF_p(p1, p2, p3, fobj, NA, x1space, x2space, pixelPitch/OSR, lam, MLARRAY, d, M, n,  centerArea_IL_p)\n",
    "        psfWAVE_STACK[:,:,eachpt]  = psfWAVE\n",
    "        LFpsfWAVE_STACK[:,:,eachpt]= LFpsfWAVE\n",
    "        if verbose:\n",
    "            print('Plane {0} took {1}'.format(eachpt, time.time()-t1))\n",
    "    else:\n",
    "        warnings.warn('Not actually computing PSF!')\n",
    "     \n",
    "for eachpt in tqdm(range(numpts), desc='Computing PSFs'):\n",
    "    PSFFunc(eachpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintMemoryUsage(thresh=100e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression: accuracy of integration\n",
    "Strangely, and rather worryingly, scipy.integrate.quad seems to misbehave with certain very specific inputs. As demonstrated below, if the tolerance is 1e-10 the returned result can be wrong by 2% (despite reporting that the error is ~1e-10). I don't understand enough about what it is doing to know why on earth this might be happening! I have increased the tolerance to 1e-12 and that seems to have made the problem go away, but it is still a little worrying not to understand why it is happening (and whether 1e-12 is definitely safe under all circumstances...).\n",
    "Note that if I go to 1e-15 then I get roundoff-related warnings for certain input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def DemonstrateProblem(tol, thetas, vals):\n",
    "        alpha = 1.253235897503375\n",
    "        u = -432.1261323834447\n",
    "        v = 686.3628350636566\n",
    "        def integrand(theta, alpha, u, v, thetas, vals):\n",
    "            result = ((np.sqrt(np.cos(theta))) * (1+np.cos(theta))  \\\n",
    "                                *  (np.exp(-(1j*u/2)* (np.sin(theta/2)**2) / (np.sin(alpha/2)**2))) \\\n",
    "                                *  (scipy.special.jn(0, np.sin(theta)/np.sin(alpha)*v)) \\\n",
    "                                *  (np.sin(theta))).real\n",
    "            thetas.append(theta)\n",
    "            vals.append(result)\n",
    "            return result\n",
    "        I0_r,err_r = scipy.integrate.quad(lambda theta:integrand(theta, alpha, u, v, thetas, vals), 0, alpha, limit=180,epsabs=tol,epsrel=tol)\n",
    "        print(tol, I0_r, err_r)\n",
    "\n",
    "    thetas = []\n",
    "    vals = []\n",
    "    for tol in [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]:\n",
    "        DemonstrateProblem(tol, thetas, vals)\n",
    "\n",
    "    thetas10 = []\n",
    "    vals10 = []\n",
    "    DemonstrateProblem(1e-10, thetas10, vals10)    \n",
    "    thetas11 = []\n",
    "    vals11 = []\n",
    "    DemonstrateProblem(1e-11, thetas11, vals11)    \n",
    "\n",
    "    def PlotForOrder(thetas, vals, line=True, dots=True, new=True):\n",
    "        order = np.argsort(thetas)\n",
    "        temp1 = np.array(thetas)[order]\n",
    "        temp2 = (np.array(vals).real)[order]\n",
    "        if new is True:\n",
    "            plt.figure(figsize=(20,10))\n",
    "        if line is True:\n",
    "            plt.plot(temp1, temp2)\n",
    "        if dots is True:\n",
    "            plt.plot(temp1, temp2, '.')\n",
    "\n",
    "    # There is a range from 0.6 to 0.8 where it really does not sample the function much.\n",
    "    # I don't know how the algorithm is meant to work, but it seems rather implausible to me that\n",
    "    # it could be possible to be that confident in the integral when there is so much going on in the function\n",
    "    # that has not been sampled by the integrator at all!\n",
    "    for lim in np.arange(0, 1.2, 0.1):\n",
    "        PlotForOrder(thetas11, vals11, True, False)\n",
    "        PlotForOrder(thetas10, vals10, False, True, False)\n",
    "        plt.xlim(lim, lim+0.1)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Compute light field PSFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_shift2(img, SHIFTX, SHIFTY):  #âˆš\n",
    "    eqtol = 1e-10\n",
    "    assert (np.abs(SHIFTX%1)<eqtol and np.abs(SHIFTY%1)<eqtol), 'SHIFTX and SHIFTY should be integer numbers'\n",
    "\n",
    "    SHIFTX = int(round(SHIFTX))\n",
    "    SHIFTY = int(round(SHIFTY))\n",
    "    new_im = np.zeros_like(img);\n",
    "\n",
    "    # JT: logic for here: 0:end-SHIFTX in matlab would skip final element if SHIFTX=-1\n",
    "    # In python, :-1 would skip final element too, so :-SHIFTX will do the trick.\n",
    "    # However, care is needed to cope with SHIFTX=0, hence the use of endx\n",
    "    endx,endy = img.shape\n",
    "    if SHIFTX >=0 and SHIFTY >= 0:\n",
    "        new_im[SHIFTX:, SHIFTY:] = img[:endx-SHIFTX, :endy-SHIFTY]\n",
    "    elif SHIFTX >=0 and SHIFTY < 0:\n",
    "        new_im[SHIFTX:, :SHIFTY] = img[:endx-SHIFTX, -SHIFTY:]\n",
    "    elif SHIFTX <0 and SHIFTY >= 0:\n",
    "        new_im[:SHIFTX, SHIFTY:] = img[-SHIFTX:, :endy-SHIFTY]\n",
    "    else:\n",
    "        new_im[:SHIFTX, :SHIFTY] = img[-SHIFTX:, -SHIFTY:]\n",
    "\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelBinning(SIMG, OSR):  #âˆš\n",
    "    assert((OSR % 2) == 1)   # Should already be caught at top of script, but repeat the check here to be sure.\n",
    "    x1length, x2length = SIMG.shape\n",
    "\n",
    "    x1center_m = int((x1length-1)/2 + 1)   #âˆš I think (though I am only assuming I need to cast to int here)\n",
    "    x2center_m = int((x2length-1)/2 + 1)\n",
    "    x1centerinit_m = x1center_m - int((OSR-1)/2)\n",
    "    x2centerinit_m = x2center_m - int((OSR-1)/2)\n",
    "    x1init_m = x1centerinit_m -  int(np.floor(x1centerinit_m/OSR)*OSR)\n",
    "    x2init_m = x2centerinit_m -  int(np.floor(x2centerinit_m/OSR)*OSR)\n",
    "\n",
    "    x1shift = 0\n",
    "    x2shift = 0\n",
    "    if x1init_m<1:\n",
    "        x1init_m += OSR\n",
    "        x1shift = 1\n",
    "    if x2init_m<1:\n",
    "        x2init_m += OSR\n",
    "        x2shift = 1\n",
    "\n",
    "    # JT: commented out in MATLAB code:  SIMG_crop = SIMG( (x1init:1:end-OSR+1), (x2init:1:end-OSR+1) );\n",
    "    # JT: commented out in MATLAB code:  SIMG_crop = SIMG_crop( (1:1: floor(size(SIMG_crop,1)/OSR)*OSR) ,  (1:1: floor(size(SIMG_crop,2)/OSR)*OSR) );\n",
    "    halfWidth = len(range(x1init_m,x1center_m-1+1))  #âˆš\n",
    "    # JT: not sure why this is split into multiple separate ranges that are then concatenated:\n",
    "    # Matlab: SIMG_crop = SIMG( [ (x1init:x1center-1) x1center x1center+1:x1center+halfWidth ],  [ (x2init:x2center-1) x2center x2center+1:x2center+halfWidth ] );\n",
    "    SIMG_crop = SIMG[ x1init_m-1:x1center_m+halfWidth,  #âˆš\n",
    "                      x2init_m-1:x2center_m+halfWidth]\n",
    "\n",
    "#    %%%%%%%%%%%%%%%%%% PIXEL BINNING  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # JT: I am not totally certain I am doing the same as the matlab,\n",
    "    # but this achieves what I think I would expect the matlab to do!\n",
    "    m,n = SIMG_crop.shape \n",
    "    SIMG_crop = np.reshape(SIMG_crop, (int(m/OSR), OSR, int(n/OSR), OSR))\n",
    "    OIMG = np.sum(SIMG_crop, axis=(1,3))\n",
    "    #SIMG_crop = sum( reshape(SIMG_crop,OSR,[]) ,1 );\n",
    "    #SIMG_crop=reshape(SIMG_crop,m/OSR,[]).'; %Note transpose\n",
    "    #SIMG_crop=sum( reshape(SIMG_crop,OSR,[]) ,1);\n",
    "    #OIMG =reshape(SIMG_crop,n/OSR,[]).'; %Note transpose\n",
    "\n",
    "    return OIMG, x1shift, x2shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1objspace = (pixelPitch/M)*np.arange(-np.floor(Nnum/2), np.floor(Nnum/2)+0.1, 1)\n",
    "x2objspace = x1objspace.copy()\n",
    "XREF = int(np.ceil(len(x1objspace)/2))\n",
    "YREF = int(np.ceil(len(x1objspace)/2))\n",
    "CP_p = np.arange((centerPT_m-1)/OSR - halfWidth/OSR, (centerPT_m-1)/OSR + halfWidth/OSR +0.1, 1, dtype=np.int) #âˆšFor Python indexing\n",
    "# JT: changed so that the a,b indexing of H only runs to the centre.\n",
    "# PSFs for larger values of a,b can be deduced by symmetry.\n",
    "# This reduces the size of H by a factor of almost 4, which is very important\n",
    "# for keeping memory usage vaguely manageable for larger problem spaces!\n",
    "# JT: also defining H as float32 from the outset. This will have a *tiny* effect on rounding errors\n",
    "# when it comes to the global normalisation of H, but should otherwise make no difference.\n",
    "# It will help memory management a lot, by halving the memory footprint!\n",
    "H = np.zeros((len(CP_p), len(CP_p), XREF, YREF, len(x3objspace)), dtype='float32')\n",
    "print('Allocated H of shape %s size %.1fMB' % (str(H.shape), (H.size*H.itemsize)/1e6))\n",
    "\n",
    "def ComputeLFpsf(i):\n",
    "    (a, b, c) = np.unravel_index(i, H.shape[2:])\n",
    "    psfREF = psfWAVE_STACK[:,:,c]\n",
    "    psfSHIFT = im_shift2(psfREF, OSR*(a+1-XREF), OSR*(b+1-YREF) )   #âˆš switched to allow for a,b in python\n",
    "    f1,dx1,x1 = fresnel2D(psfSHIFT*MLARRAY, pixelPitch/OSR, d,lam)\n",
    "    f1 = im_shift2(f1, -OSR*(a+1-XREF), -OSR*(b+1-YREF) )     #âˆš switched to allow for a,b in python\n",
    "\n",
    "    xmin_p =  np.maximum( centerPT_m-1  - halfWidth, 0) #âˆš\n",
    "    xmax_p =  np.minimum( centerPT_m-1  + halfWidth, f1.shape[0]-1) #âˆš\n",
    "    ymin_p =  np.maximum( centerPT_m-1  - halfWidth, 0) #âˆš\n",
    "    ymax_p =  np.minimum( centerPT_m-1  + halfWidth, f1.shape[1]-1) #âˆš\n",
    "\n",
    "    f1_AP = np.zeros_like(f1)\n",
    "    f1_AP[xmin_p:xmax_p+1,ymin_p:ymax_p+1] = f1[xmin_p:xmax_p+1,ymin_p:ymax_p+1]   #âˆš\n",
    "    [f1_AP_resize, x1shift, x2shift] = pixelBinning(np.abs(f1_AP**2), OSR)      \n",
    "    # JT: I had to split this up into two separate commands to make it work in Python\n",
    "    temp = f1_AP_resize[ CP_p - x1shift, : ]   #âˆš\n",
    "    f1_CP = temp[ :, CP_p-x2shift ]   #âˆš\n",
    "    # JT: Do type conversion to float32 at this point. We will be using the result to fill in H,\n",
    "    # which is of type float32. Given how we accumulate results across a Parallel() call,\n",
    "    # halving the memory footprint here is definitely a good thing.\n",
    "    return (a,b,c,f1_CP.astype(np.float32)) \n",
    "    \n",
    "if True:\n",
    "    iterRange = tqdm(range(H.shape[2]*H.shape[3]*H.shape[4]), desc='Computing LF PSFs')\n",
    "    if False:\n",
    "        # Single-threaded code for reference\n",
    "        for i in iterRange:\n",
    "            (a,b,c,f1_CP) = ComputeLFpsf(i)\n",
    "            H[:,:,a,b,c] = f1_CP\n",
    "    else:\n",
    "        # Multithreaded code\n",
    "        results = Parallel(n_jobs=numJobs)(delayed(ComputeLFpsf)(i) for i in iterRange)\n",
    "        for (a,b,c,f1_CP) in results:\n",
    "            H[:,:,a,b,c] = f1_CP\n",
    "        del results\n",
    "\n",
    "    Hmax = np.max(H)\n",
    "    H /= Hmax\n",
    "else:\n",
    "    warnings.warn('Skipping LF PSF calculation, and using H from .mat file')\n",
    "    H = _H.T.copy()\n",
    "    \n",
    "# I have seen some horrendous thrashing and running out of memory somewhere towards the \n",
    "# end of this cell, after the progress bar was complete.\n",
    "# I imagine it might well have been when sorting through 'results', since that would involve the same\n",
    "# amount of memory as is required for H itself.    \n",
    "# TODO: ideally it might be good to refactor that, maybe by only multithreading over one z coordinate at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "x1space = (pixelPitch/1)*np.arange(-IMG_HALFWIDTH*1, IMG_HALFWIDTH*1+0.1, 1);\n",
    "x2space = x1space.copy()\n",
    "x1space = x1space[CP_p]\n",
    "x2space = x2space[CP_p]\n",
    "# Monitor the lead-in time because I think I once saw some VM thrashing here, for some reason\n",
    "print('Lead-in took %.2fs'%(time.time()-t1))\n",
    "\n",
    "if True:\n",
    "    # Force very small values (in each separate plane) to zero\n",
    "    tol = 0.005\n",
    "    # JT TODO: I think this may be slow due to the copying back at the end.\n",
    "    # I am almost certain that that is unnecessary in python, and could be removed.\n",
    "    # (or actually, is it really any sort of bottleneck? I think not...)\n",
    "    for i in tqdm(range(H.shape[4]), desc='clipping to zero'):\n",
    "        H4Dslice = H[:,:,:,:,i]\n",
    "        H4Dslice[H4Dslice < (tol*np.max(H4Dslice))] = 0\n",
    "    del H4Dslice   # This is actually just a view into H, but I delete it to avoid it showing up in memory usage\n",
    "else:\n",
    "    warnings.warn('Not clipping to zero')\n",
    "print('Took %.2fs'%(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JT: normalise each individual PSF, so that power is conserved during forward-projection\n",
    "if normalisePSF:\n",
    "    for cc in tqdm(range(H.shape[4]), desc='normalising'):\n",
    "        for bb in range(H.shape[3]):\n",
    "            for aa in range(H.shape[2]):\n",
    "                thisH = H[:,:,aa,bb,cc]\n",
    "                thisH /= np.sum(thisH)\n",
    "else:\n",
    "    # 'Normalise' in a way that matches Prevedel's code (but is not correct!)\n",
    "    warnings.warn('Not normalising H properly (as instructed!)')\n",
    "    H /= np.max(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(H.dtype == np.float32)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%% Estimate PSF size again  %%%%%%%%%%%%%%%%%%%%%%%\n",
    "# JT: I *DO* want to save CAindex in 1-based MATLAB indexing.\n",
    "#     My python deconvolution code expects that (since we are just loading .mat files...),\n",
    "#     and my deconvolution code will take that into account.\n",
    "centerCP_m = np.ceil(len(CP_p)/2)\n",
    "CAindex = np.zeros((2,len(x3objspace)), dtype='int')\n",
    "for i in range(len(x3objspace)):\n",
    "    IMGSIZE_REF_IL = np.ceil(IMGSIZE_REF*( np.abs(x3objspace[i])/p3max))\n",
    "    halfWidth_IL =  np.maximum(Nnum*(IMGSIZE_REF_IL + 0 ), 2*Nnum)\n",
    "    CAindex[0,i] = np.maximum( centerCP_m - halfWidth_IL , 1)\n",
    "    CAindex[1,i] = np.minimum( centerCP_m + halfWidth_IL , H.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory from variables we have now finished with.\n",
    "# These arrays are smaller than the H and Ht matrices,\n",
    "# so this is only really any help if we are close to exhausting the available RAM.\n",
    "PrintMemoryUsage(' before freeing memory', thresh=100e6)\n",
    "if True:\n",
    "    del LFpsfWAVE_STACK\n",
    "    del psfWAVE_STACK\n",
    "    PrintMemoryUsage(' after freeing memory', thresh=100e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: calculate Ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains my new code (way faster)\n",
    "def backwardProject_new(H, projection, Nnum, imcenterinit_mp, _aa, _bb):\n",
    "    # Note that with imcenterinit_mp the _mp suffix is a reminder that the same number can\n",
    "    # work for both python and matlab. Since we add aa to it to get a pixel index, that works\n",
    "    # in either language, since aa will start from 0 and 1 in the respective languages.\n",
    "    x3length = H.shape[4]\n",
    "    Backprojection2 = np.zeros((projection.shape[0], projection.shape[0], x3length))\n",
    "    imcenter = imcenterinit_mp + int(Nnum/2)\n",
    "    # Original code convolves a point at _aa,_bb with the rotated H\n",
    "    for aa in range(Nnum):\n",
    "        for bb in range(Nnum):\n",
    "            # No need to call Rotate180 - we can just mirror the axes and that has the same effect\n",
    "            _Ht = H[::-1,::-1]\n",
    "            # ... although actually we may mirror them again to account for symmetry effects if\n",
    "            # we need to access H for values of aa,bb outside the quadrant we actually generated.\n",
    "            if (aa <= Nnum//2):\n",
    "                _Ht = _Ht[:,:,aa,:]\n",
    "            else:\n",
    "                _Ht = _Ht[::-1,:,Nnum-1-aa,:]\n",
    "            if (bb <= Nnum//2):\n",
    "                _Ht = _Ht[:,:,bb]\n",
    "            else:\n",
    "                _Ht = _Ht[:,::-1,Nnum-1-bb]\n",
    "\n",
    "            # Identify the indices in _Ht that we will be keeping\n",
    "            # i.e. the ones that, in the original code, would actually be \n",
    "            # sampled by the aa::Nnum indexing of tempSlice.\n",
    "            # In this calculation, note that the central pixel of _Ht \n",
    "            # will be indexed at a multiple of Nnum\n",
    "            HtCentA = int(_Ht.shape[0]/2)\n",
    "            HtCentB = int(_Ht.shape[1]/2)\n",
    "            assert((HtCentA % Nnum) == 0)\n",
    "            aStart = (-_aa + aa) % Nnum\n",
    "            bStart = (-_bb + bb) % Nnum\n",
    "            # The next question is where these should go in the array Backprojection2. \n",
    "            # We know that the *middle pixel* of _Ht goes at imcenterinit+_aa,_bb in Backprojection2.\n",
    "            # It therefore follows that pixel 'aStart' should go\n",
    "            # at the following coordinate in Backprojection2:\n",
    "            aStartDest = imcenterinit_mp+_aa - HtCentA + aStart\n",
    "            bStartDest = imcenterinit_mp+_bb - HtCentB + bStart\n",
    "            strided = _Ht[aStart::Nnum, bStart::Nnum]\n",
    "            aEndDest = aStartDest + strided.shape[0]*Nnum\n",
    "            bEndDest = bStartDest + strided.shape[1]*Nnum\n",
    "            Backprojection2[aStartDest:aEndDest:Nnum, bStartDest:bEndDest:Nnum] = strided\n",
    "    return Backprojection2\n",
    "\n",
    "def calcHt_new(_H):\n",
    "    Hsize1,_,x1Middle,_,x3length = H.shape\n",
    "    Nnum = x1Middle*2-1   # JT: note that my H only runs up to the centre pixel in aa,bb.\n",
    "    tmpsize = int(np.ceil(_H.shape[0]/Nnum))\n",
    "    if ((tmpsize%2) == 1):\n",
    "        imgsize = (tmpsize+2)*Nnum\n",
    "    else:\n",
    "        imgsize = (tmpsize+3)*Nnum\n",
    "\n",
    "    zeroprojection = np.zeros((imgsize, imgsize))\n",
    "    imcenter_m = int(np.ceil(imgsize/2))\n",
    "    imcenterinit_m = imcenter_m - int(np.ceil(Nnum/2))\n",
    "\n",
    "    Ht = np.zeros_like(_H)\n",
    "    for aa in tqdm(range(x1Middle)):\n",
    "        for bb in tqdm(range(x1Middle), leave=False):\n",
    "            temp = zeroprojection.copy().astype('float32')\n",
    "            temp[imcenterinit_m+aa, imcenterinit_m+bb] = 1  #âˆš same arithmetic works for me, because aa starts at 0 instead of 1\n",
    "            tempback = backwardProject_new(H, temp, Nnum, imcenterinit_m, aa, bb)\n",
    "            tempback_cut = tempback[imcenter_m - int((Hsize1-1)/2) - 0*Nnum - 1 : imcenter_m + int((Hsize1-1)/2) + 0*Nnum, \n",
    "                                    imcenter_m - int((Hsize1-1)/2) - 0*Nnum - 1 : imcenter_m + int((Hsize1-1)/2) + 0*Nnum]#âˆš\n",
    "            tempback_shift = np.zeros_like(tempback_cut)\n",
    "            for cc in range(x3length):\n",
    "                Ht[:,:,aa,bb,cc] = im_shift2(tempback_cut[:,:,cc], int(np.ceil(Nnum/2)-aa-1), int(np.ceil(Nnum/2)-bb-1) ) #âˆš\n",
    "    return Ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%% Calculate Ht (transpose for backprojection) %%%%%%%%%\n",
    "print('Computing Transpose (3/3)')\n",
    "if True:\n",
    "    # JT: my new code, massively faster\n",
    "    Ht = calcHt_new(H)\n",
    "else:\n",
    "    warnings.warn('Not computing transpose!')\n",
    "    Ht = H.copy()\n",
    "Ht = Ht.astype('float32')\n",
    "# Reminder: I should not be separately normalising Ht,\n",
    "# I should just be using the normalisation already present in H (which is what I do here, implicitly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PrintMemoryUsage(thresh=100e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the matrices we have generated\n",
    "#### File size\n",
    "Note that compression is available as part of an HDF5 \"dataset\". I am not sure how \"datasets\" relate to how I am saving things here, but it is something I could consider for the future. I only use the .mat files when generating the initial PSFs; my own deconvolution code uses a different (and smaller-footprint) storage method. So, in principle we could then delete the original .mat files to save space. All that matrix compression would do is to make those original .mat files smaller, so it's not really a high priority.\n",
    "\n",
    "#### Loading with Matlab\n",
    "Some of the .mat files I generate seem to be readable by Matlab itself, but others (large ones?) aren't. I get an error saying \"Not a binary MAT-file\". I don't know what the issue is there. I am not planning on investigating this further any time soon, since I'm not that fussed about my files being readable by Matlab. If it was a real issue, I could presumably find some other way of accessing the information and reading it into Matlab.\n",
    "\n",
    "Note that I also tried using scipy.io to save, but it turns out that very large matrices cannot be saved in the old in Matlab version 5 format that this supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveMatrices(_H, _Ht, _CAindex, matPathStem):\n",
    "    # Save a .mat file just like the Matlab code does.\n",
    "    # In fact, my deconvolution code will take that and convert it to my own format,\n",
    "    # but to avoid confusion(?) I just generate the .mat file here.\n",
    "    # My deconvolution code will auto-generate the files it actually needs, when it sees they don't exist yet.\n",
    "    matPath = '%s_%s.mat'%(matPathStem, MatrixFileString())\n",
    "    print('Saving to', matPath)\n",
    "    with h5py.File(matPath, 'w') as f:\n",
    "        print('Write parameters')\n",
    "        f['M'] = M\n",
    "        f['NA'] = NA\n",
    "        f['MLPitch'] = MLPitch\n",
    "        f['Nnum'] = Nnum\n",
    "        f['OSR'] = OSR\n",
    "        f['n'] = n\n",
    "        f['fml'] = fml\n",
    "        f['lambda'] = lam\n",
    "        f['zmax'] = zmax\n",
    "        f['zmin'] = zmin\n",
    "        f['zspacing'] = zspacing\n",
    "        \n",
    "        f['fobj'] = fobj\n",
    "        f['d'] = d\n",
    "        f['x3objspace'] = x3objspace\n",
    "        f['pixelPitch'] = pixelPitch\n",
    "        f['CAindex'] = _CAindex.T    # As with H, below, we need to save the transpose to be Matlab-compatible.\n",
    "\n",
    "        # JT: maximum value of H prior to normalization. Useful for fusing multiple z ranges.\n",
    "        f['Hmax'] = Hmax   \n",
    "        # JT: flags that we are saving a reduced H matrix (not the full aa,bb range; just up to the midpoint)\n",
    "        f['ReducedMatrix'] = True  \n",
    "        \n",
    "        # Matlab works with Fortran-contiguous arrays, and writes them to disk as such.\n",
    "        # Consequently, if I want my arrays (same indexing order but different contiguity)\n",
    "        # to look exactly the same when saved to disk, I need to save their transpose.\n",
    "        print('Write H'); sys.stdout.flush()\n",
    "        print('H shape', H.shape)\n",
    "        f['H'] = H.T\n",
    "        print('Write Ht'); sys.stdout.flush()\n",
    "        f['Ht'] = Ht.T\n",
    "        print('Ht shape', Ht.shape)\n",
    "        print('Done'); sys.stdout.flush()\n",
    "\n",
    "if normalisePSF:\n",
    "    filePrefix = 'fdnorm'\n",
    "else:\n",
    "    # The 'reduced' prefix is a reminder that these files are not saving the full Nmax x Nmax PSFs,\n",
    "    # and they are therefore incompatible with the files that the Matlab code relies on.\n",
    "    # (and in fact there also seems to be some issue with the file format as well,\n",
    "    # such that Matlab cannot parse them correctly)\n",
    "    filePrefix = 'reduced'\n",
    "SaveMatrices(H, Ht, CAindex, 'PSFmatrix/%sPSFMatrix'%filePrefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check my results against the Matlab ones\n",
    "(but note that my code now normalises the H and Ht matrices properly, whereas the Matlab code does not, so these are not now expected to match unless I hamstring my code, or fix Prevedel's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Theirs:\n",
    "    (_H1, _Ht1, hReducedShape1, htReducedShape1, _CAindex1) = LoadRawMatrixData('/Volumes/Development/SIsoftware/PSFmatrix/PSFmatrix_%s.mat'%MatrixFileString(), Nnum)\n",
    "\n",
    "    # Mine:\n",
    "    (_H2, _Ht2, hReducedShape2, htReducedShape2, _CAindex2) = LoadRawMatrixData('/Volumes/Development/light-field-flow/PSFmatrix/reducedPSFMatrix_%s.mat'%MatrixFileString(), Nnum)\n",
    "    \n",
    "    print(np.max(np.abs(_H1[:,:Nnum//2+1,:Nnum//2+1,:,:] - _H2)))\n",
    "    print(np.max(np.abs(_Ht1[:,:Nnum//2+1,:Nnum//2+1,:,:] - _Ht2)))\n",
    "    \n",
    "    del _H1\n",
    "    del _H2\n",
    "    del _Ht1\n",
    "    del _Ht2    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
