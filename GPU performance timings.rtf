{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf840
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;\f2\fmodern\fcharset0 Courier-Bold;
\f3\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red25\green25\blue25;\red0\green0\blue0;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c12941\c12941\c12941;\csgray\c0;\csgray\c100000;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww26940\viewh16500\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 \ul \ulc0 Current performance numbers, 30.3.2020\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 \ulnone \
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \expnd0\expndtw0\kerning0
Run tests in lfdeconv: gpuHelper, tailored block sizes, x32, 8 iterations. Took 58s. Pretty consistently the same time for each iteration\
\pard\pardeftab720\partightenfactor0

\f2\b \cf2 \
\pard\pardeftab720\partightenfactor0
\cf2 \ul Outstanding performance issues\
\pard\pardeftab720\partightenfactor0

\f1\b0\fs24 \cf2 \ulnone - The main issue seems to be just sheer latency with me calling my kernels. It might be that I could speed things up by actually folding \'91expand\'92 into the CalculateRows kernel, even though that would be creating a little extra computational work.\
- I could look into how much GPU memory I am actually using while running my code. It\'92s not clear to me that it should be much, and yet I seem to have seen signs that it may be many GB. Is it just an issue with garbage collection, or what?\
- If I run with only 2 parallel volumes to reconstruct, it only runs at twice the speed of 32 parallel volumes(!). A surprising issue is that the fftn is barely any faster (18.4->17.2s). This is really weird - why is it not parallelising better? That seems odd enough that it\'92s worth investigating, even if there might be other more significant bottlenecks.
\fs28 \
\
\pard\pardeftab720\partightenfactor0

\f2\b \cf2 \ul CPU performance
\f1\b0 \ulnone \
For comparison, C code on my Macbook Air had projected run time 3570s for x30 (approx 65x slower!). 447s/iteration. I stopped it early to save my CPU!!\
This should be dual-threaded, even.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0

\f3\fs22 \cf3 \cb4 \kerning1\expnd0\expndtw0 \CocoaLigature0 iter 1 | 8, took 425.0 secs. Max val 0.691475                                                                                                    \
RL deconv:  12%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9615                                                                                     | 1/8 [07:05<49:35, 425.06s/it]\
iter 2 | 8, took 499.1 secs. Max val 0.945111                                                                                                    \
RL deconv:  25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                                                         | 2/8 [15:24<44:43, 447.30s/it]\
iter 3 | 8, took 445.9 secs. Max val 1.126294                                                                                                    \
RL deconv:  38%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9613                                                             | 3/8 [22:50<37:14, 446.90s/it]\
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\
\pard\pardeftab720\partightenfactor0

\f2\b \cf2 \ul Notes on GPU performance
\f1\b0 \ulnone \
Blocking turned out to be really important for performance (especially in the x dimension),\
and so I padded my arrays out to multiples of 8 (i.e. 32 bytes), allowing us to maintain\
good block sizes, good cache locality, and good alignment of subsequent rows.\
\
Profile for the GPU code. This is the current best-performing code, but note that it was not run on quite the fastest GPU option available on colab. The particular GPU it gave me here seems to have high parallelism but high latency (and FFTs seem to be a bit slower on it, as a result?)\
Note that if I disable calls to cupy.cuda.runtime.deviceSynchronize then the time drops to 41s(!!). I am not 100% sure whether this is safe [not] to do, although the results do appear to be coming out correct at the end\'85\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2    ncalls  tottime  percall  cumtime  percall filename:lineno(function)\
        1    0.005    0.005   58.425   58.425 lfdeconv.py:87(DeconvRL)\
      224    0.006    0.000   47.113    0.210 projector.py:734(ProjectForZ)\
      224    0.005    0.000   46.390    0.207 projector.py:615(ProjectForZ)\
      224    0.069    0.000   40.907    0.183 projector.py:172(ProjectForZ)\
     1792    0.056    0.000   40.826    0.023 projector.py:145(ProjectForZY)\
     8064    0.109    0.000   40.716    0.005 projector.py:113(convolve)\
    14336    0.073    0.000   40.442    0.003 projector.py:138(convolvePart2)\
    26880    0.773    0.000   38.630    0.001 projector.py:576(convolvePart3)\
    50400    0.690    0.000   37.143    0.001 projector.py:492(special_fftconvolve)\
   113344    
\f2\b 4.523
\f1\b0     0.000   32.777    0.000 projector.py:254(CallKernel)\
        8    0.000    0.000   29.257    3.657 lfdeconv.py:59(ForwardProjectACC)\
        8    0.003    0.000   29.256    3.657 projector.py:712(ForwardProjectACC)\
   113792   
\f2\b 29.086
\f1\b0     0.000   29.086    0.000 \{built-in method cupy.cuda.runtime.deviceSynchronize\}\
        8    0.001    0.000   28.745    3.593 lfdeconv.py:28(BackwardProjectACC)\
        8    0.002    0.000   28.716    3.590 projector.py:691(BackwardProjectACC)\
    26880    0.546    0.000   
\f2\b 15.173
\f1\b0     0.001 projector.py:567(special_fftconvolve2_nomirror)\
    23520    0.469    0.000   
\f2\b 13.236
\f1\b0     0.001 projector.py:558(special_fftconvolve2_mirror)\
     7168    0.065    0.000    
\f2\b 9.743
\f1\b0     0.001 fft.py:727(irfftn)\
     7168    2.109    0.000    9.659    0.001 fft.py:138(_fft)\
    50400    1.250    0.000    
\f2\b 7.945
\f1\b0     0.000 projector.py:504(special_fftconvolve2_expand)\
    14336    6.130    0.000    7.350    0.001 fft.py:73(_exec_fft)\
        8    0.241    0.030    5.452    0.681 projector.py:788(InverseTransformForwardProjection)\
        8    0.108    0.014    5.401    0.675 projector.py:773(InverseTransformBackwardProjection)\
      224    0.334    0.001    
\f2\b 3.171
\f1\b0     0.014 projector.py:604(PrecalculateFH)\
     7168    0.101    0.000    2.819    0.000 fft.py:132(_fft_c2c)\
424913/424681    0.732    0.000    2.423    0.000 \{built-in method numpy.core._multiarray_umath.implement_array_function\}\
      224    1.126    0.005    
\f2\b 2.306
\f1\b0     0.010 projector.py:594(PrecalculateFFTArray)\
   201600    1.080    0.000    2.024    0.000 projector.py:269(element_strides)\
   500720    1.953    0.000    1.953    0.000 \{built-in method numpy.array\}\
   113760    0.155    0.000    1.770    0.000 <__array_function__ internals>:2(all)\
    12544    0.054    0.000    
\f2\b 1.738
\f1\b0     0.000 projector.py:461(MirrorYArray)\
   437536    0.159    0.000    1.481    0.000 _asarray.py:16(asarray)\
   113760    0.180    0.000    1.456    0.000 fromnumeric.py:2324(all)\
   116705    0.375    0.000    1.322    0.000 fromnumeric.py:73(_wrapreduction)\
    21504    0.847    0.000    0.993    0.000 \{method 'copy' of 'cupy.core.core.ndarray' objects\}\
   307336    0.224    0.000    0.861    0.000 <__array_function__ internals>:2(can_cast)\
   116705    0.820    0.000    0.820    0.000 \{method 'reduce' of 'numpy.ufunc' objects\}\
      224    0.015    0.000    0.716    0.003 projector.py:288(__init__)\
    53794    0.176    0.000    0.715    0.000 jutils.py:19(cpuTime)\
    66048    0.593    0.000    0.593    0.000 basic.py:7(empty)\
\pard\pardeftab720\partightenfactor0
\cf2 \
\
\
\pard\pardeftab720\partightenfactor0

\f2\b\fs28 \cf2 \ul Notes on different GPUs
\f1\b0 \ulnone \
The above timings were obtained with:\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 2048 threads x 56 processors			clock speed 1.3285GHz, mem speed 0.715GHz x 512B = 366.08GB/s, L2 4.19MB		Total GPU RAM 17.07GB\
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
Other combinations I have encountered include:\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 2048 threads x 13 processors			clock speed 0.8235GHz, mem speed 2.505GHz x 48B = 120.24GB/s, L2 1.57MB							Total run time 303s(!)\
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
}