{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf840
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;\f2\fmodern\fcharset0 Courier-Bold;
\f3\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red25\green25\blue25;\red0\green0\blue0;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c12941\c12941\c12941;\csgray\c0;\csgray\c100000;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww26940\viewh22580\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 \ul \ulc0 Current performance numbers, 30.3.2020\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 \ulnone \
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \expnd0\expndtw0\kerning0
Run tests in lfdeconv: gpuHelper, tailored block sizes, x32, 8 iterations. Took 108s. Pretty consistently the same time for each iteration\
\pard\pardeftab720\partightenfactor0

\f2\b \cf2 \
\pard\pardeftab720\partightenfactor0
\cf2 \ul Outstanding performance issues\
\pard\pardeftab720\partightenfactor0

\f1\b0\fs24 \cf2 \ulnone - I think the main place I can still improve is by looking at the different FFTs, understanding when it is taking time to calculate a new plan, and whether I can speed up the FFT(H) calculation by providing a dedicated plan for it.
\f2\b\fs28 \
\pard\pardeftab720\partightenfactor0

\f1\b0\fs24 \cf2 - special_fftconvolve2_expand isn\'92t my main bottleneck, but I am still a bit intrigued as to how/why it takes as long as it does, as a fraction of the full mirror/nomirror run time.\
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
\
For comparison, C code on my Macbook Air had projected run time 3570s for x30 (approx 35x slower!). 447s/iteration. I stopped it early to save my CPU!!\
This should be dual-threaded, even.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0

\f3\fs22 \cf3 \cb4 \kerning1\expnd0\expndtw0 \CocoaLigature0 iter 1 | 8, took 425.0 secs. Max val 0.691475                                                                                                    \
RL deconv:  12%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9615                                                                                     | 1/8 [07:05<49:35, 425.06s/it]\
iter 2 | 8, took 499.1 secs. Max val 0.945111                                                                                                    \
RL deconv:  25%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9614                                                                         | 2/8 [15:24<44:43, 447.30s/it]\
iter 3 | 8, took 445.9 secs. Max val 1.126294                                                                                                    \
RL deconv:  38%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9613                                                             | 3/8 [22:50<37:14, 446.90s/it]\
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\
\pard\pardeftab720\partightenfactor0

\f2\b \cf2 \ul Notes on GPU performance
\f1\b0 \ulnone \
Blocking turned out to be really important for performance (especially in the x dimension),\
and so I padded my arrays out to multiples of 8 (i.e. 32 bytes), allowing us to maintain\
good block sizes, good cache locality, and good alignment of subsequent rows.\
\
Profile for the GPU code. This is the current best-performing code, but note that it was not run on quite the fastest GPU option available on colab. This one seems to have high parallelism but high latency (and FFTs seem to be a bit slower on it, as a result?)\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2    ncalls  tottime  percall  cumtime  percall filename:lineno(function)\
        1    3.683    3.683  116.945  116.945 lfdeconv.py:87(DeconvRL)\
       16    0.000    0.000   98.215    6.138 projector.py:715(DoMainWork)\
       16    0.022    0.001   98.215    6.138 projector.py:675(DoMainWork_singleThreaded)\
     1792    0.074    0.000   98.192    0.055 projector.py:635(ProjectForZY)\
     8064    0.170    0.000   94.480    0.012 projector.py:105(convolve)\
    14336    0.082    0.000   81.506    0.006 projector.py:137(convolvePart2)\
    26880    0.998    0.000   79.254    0.003 projector.py:491(convolvePart3)\
    50400    1.034    0.000   77.402    0.002 projector.py:426(special_fftconvolve)\
        8    0.002    0.000   55.300    6.912 lfdeconv.py:58(ForwardProjectACC)\
        8    0.015    0.002   55.297    6.912 projector.py:603(ForwardProjectACC)\
   113344    
\f2\b 4.920
\f1\b0     0.000   54.245    0.000 projector.py:216(CallKernel)\
        8    0.002    0.000   54.136    6.767 lfdeconv.py:26(BackwardProjectACC)\
        8    0.013    0.002   54.090    6.761 projector.py:572(BackwardProjectACC)\
   163744   47.499    0.000   47.499    0.000 \{built-in method cupy.cuda.runtime.deviceSynchronize\}\
    26880    0.589    0.000   
\f2\b 25.869
\f1\b0     0.001 projector.py:482(special_fftconvolve2_nomirror)\
    64736    2.328    0.000   24.848    0.000 fft.py:381(_fftn)\
    23520    0.518    0.000   
\f2\b 22.803
\f1\b0     0.001 projector.py:474(special_fftconvolve2_mirror)\
    50400    0.246    0.000   
\f2\b 15.358
\f1\b0     0.000 projector.py:421(fftn)\
    50400    0.130    0.000   14.315    0.000 fft.py:253(fftn)\
    14336    0.065    0.000   12.461    0.001 psfmatrix.py:72(fH)\
    14336    0.168    0.000   12.396    0.001 psfmatrix.py:65(fH_uncached)\
    64736    3.168    0.000   11.772    0.000 fft.py:321(_exec_fftn)\
    14336    0.028    0.000   
\f2\b 11.751
\f1\b0     0.001 myfft.py:55(myFFT2_gpu)\
    14336    0.036    0.000   11.723    0.001 fft.py:516(fft2)\
   732017    5.073    0.000   10.943    0.000 \{built-in method numpy.core._multiarray_umath.implement_array_function\}\
    50400    1.678    0.000    
\f2\b 9.514
\f1\b0     0.000 projector.py:464(special_fftconvolve2_expand)\
    71904    5.550    0.000    9.180    0.000 fft.py:35(_cook_shape)					
\f2\b Note: I should be able to avoid this by doing the padding myself
\f1\b0 \
     7168    0.063    0.000    
\f2\b 8.969
\f1\b0     0.001 fft.py:727(irfftn)\
     7168    1.673    0.000    8.888    0.001 fft.py:138(_fft)\
    14336    5.842    0.000    7.022    0.000 fft.py:73(_exec_fft)\
    14528    3.871    0.000    5.812    0.000 fft.py:179(_get_cufft_plan_nd)			
\f2\b Note: I am not sure why this is being called so many times, or taking so long. Would be good to know which are taking the time
\f1\b0 \
        8    0.241    0.030    5.061    0.633 projector.py:734(InverseTransformForwardProjection)\
        8    0.100    0.012    4.961    0.620 projector.py:719(InverseTransformBackwardProjection)\
   265449    1.102    0.000    4.754    0.000 fromnumeric.py:73(_wrapreduction)\
   122304    3.222    0.000    3.755    0.000 \{method 'astype' of 'cupy.core.core.ndarray' objects\}\
     1792    0.049    0.000    3.583    0.002 projector.py:250(__init__)\
        8    0.000    0.000    3.244    0.406 <__array_function__ internals>:2(where)\
   265449    3.202    0.000    3.202    0.000 \{method 'reduce' of 'numpy.ufunc' objects\}\
     1792    2.289    0.001    
\f2\b 2.871
\f1\b0     0.002 projector.py:55(__init__)\
   131280    1.267    0.000    2.860    0.000 basic.py:189(zeros)\
\
\
\pard\pardeftab720\partightenfactor0

\f2\b\fs28 \cf2 \ul \ulc0 Notes on different GPUs
\f1\b0 \ulnone \
The above timings were obtained with:\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 clock 1328500, memclock 715000, L2 4194304		2048 threads x 56 processors
\fs28 \cf2 \
Other combinations I have encountered include:\

\fs24 \cf2 clock 823500, memclock 2505000, L2 1572864		2048 threads x 13 processors		Total run time 303s(!)\
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
}