{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking code to generate Figs 2 and 3 of the accompanying paper\n",
    "\n",
    "Run the cells below to plot performance graphs.\n",
    "\n",
    "The following parameters are used in the benchmarking scenario, representing a typical light field workload\n",
    "\n",
    "| Parameter | Value |\n",
    "| --------------- | ------- |\n",
    "| Numerical Aperture | 0.5  |\n",
    "| Magnification | 22.222   |\n",
    "| ML Pitch ($\\mu$m) | 125 |\n",
    "| $f_\\textrm{ML}$ ($\\mu$m) | 3125  |\n",
    "| Refractive index | 1.33   |\n",
    "| $\\lambda$ (nm) | 520 |\n",
    "| $z$ range ($\\mu$m) | Â±60   |\n",
    "| | |\n",
    "| Number of planes | 25  |\n",
    "| N | 19  |\n",
    "| X | 1463   |\n",
    "| Y | 1273   |\n",
    "| $N_\\textrm{iter}$ | 4 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_light_field as plf\n",
    "import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform self-tests (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import projector\n",
    "import lfdeconv\n",
    "import projector as proj\n",
    "_ = projector.selfTest()\n",
    "_ =  lfdeconv.main(['basic', 'full', 'parallel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing thread scaling\n",
    "For just the backprojection, the tests in the next cell take about an hour to run on suil-bheag (for 1-8 threads). This will generate a file `stats.txt`, but the subsequent code actually plots a file that is already in the repository recording the performance I have recorded on my own server.\n",
    "\n",
    "Interestingly, the thread scaling data looks very similar for the smaller image testcase.\n",
    "The actual run time thread scaling looks a bit worse for the smaller testcase, but the scaling of actual CPU time used is similar in both cases (suggesting threads are idling in the smaller testcase)\n",
    "\n",
    "The anomlously slow run-time for two threads on cuinneag does seem to be reproducible. I assume this is connected with the two-processor architecture, and is connected with memory and cache usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benchmark\n",
    "if False:\n",
    "    # Smaller problem just to road-test this code and analysis\n",
    "    threadScalingResults = benchmark.main(benchmarkGPU=False,\n",
    "                                          prefix=['smaller-image', 'x4'],\n",
    "                                          prefix2=['parallel-scaling'])\n",
    "else:\n",
    "    threadScalingResults = benchmark.main(benchmarkGPU=False,\n",
    "                                          prefix=['olaf-image', 'olaf-matrix', 'x16'],\n",
    "                                          prefix2=['parallel-scaling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def PlotThreadStats(filename, rescaleAll=1, show=True):\n",
    "    # rescaleAll: multiply all values by a constant factor (useful if doing multiple plots on top of each other)\n",
    "    # show: call plt.show(). Disable this to allow plotting multiple plots on top of each other\n",
    "    # forPaper: disable various annotations I don't want on the plot for the paper\n",
    "    threadStats = []\n",
    "    with open(filename) as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            threadStats.append(row)\n",
    "    threadStats = np.array(threadStats).astype(np.float64)\n",
    "    threadStats[:,1:] *= rescaleAll\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(filename)\n",
    "    plt.xlabel(\"Number of threads\")\n",
    "    plt.ylabel(\"Benchmark time (s)\")\n",
    "\n",
    "    plt.plot(threadStats[:,0], threadStats[:,1], label='Run time')\n",
    "    plt.plot(threadStats[:,0], threadStats[:,2]+threadStats[:,3], label='CPU time')\n",
    "    plt.plot(threadStats[:,0], threadStats[:,1]*threadStats[:,0], label='Run time scaled')\n",
    "    plt.ylim(0,None)\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"{0} efficiency\".format(filename))\n",
    "    plt.plot(threadStats[:,0], threadStats[0,1]*threadStats[0,0]/(threadStats[:,1]*threadStats[:,0]), label='Efficiency')\n",
    "    plt.ylim(0,None)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "def PlotThreadStatsForPaper(filename):\n",
    "    # rescaleAll: multiply all values by a constant factor (useful if doing multiple plots on top of each other)\n",
    "    # show: call plt.show(). Disable this to allow plotting multiple plots on top of each other\n",
    "    # forPaper: disable various annotations I don't want on the plot for the paper\n",
    "    threadStats = []\n",
    "    with open(filename) as f:\n",
    "        csv_reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            threadStats.append(row)\n",
    "    threadStats = np.array(threadStats).astype(np.float64)\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    fdMapping = lambda y: y/threadStats[0,1]\n",
    "    bkMapping = lambda y: y*threadStats[0,1]\n",
    "    ax2 = ax.secondary_yaxis('right', functions=(fdMapping, bkMapping))\n",
    "    ax.set_xlabel(\"Number of threads\")\n",
    "    ax.set_ylabel(\"Benchmark time (s)\")\n",
    "    ax2.set_ylabel(\"Efficiency\")\n",
    "\n",
    "    ax.plot(threadStats[:,0], threadStats[:,1], marker='.', label='Run time')\n",
    "    ax.set_xticks([1] + list(range(2, 17, 2)))\n",
    "    ax.set_ylim(0,900)\n",
    "    ax.set_yticks(range(0, 801, 200))\n",
    "    ax2.set_yticks(np.arange(0, 1.01, 0.2))\n",
    "    ax.plot(threadStats[:,0], \n",
    "            bkMapping(threadStats[0,1]*threadStats[0,0]/(threadStats[:,1]*threadStats[:,0])), \n",
    "            marker='x',\n",
    "            label='Multithreading efficiency')\n",
    "    leg = fig.legend(loc='upper center', borderaxespad=4)\n",
    "    fig.savefig(\"thread_scaling.pdf\")\n",
    "    # The next two lines are a cosmetic workaround.\n",
    "    # For some reason I need a larger borderaxespad for the saved pdf figure,\n",
    "    # compared to the one displayed inline in this notebook\n",
    "    leg.remove()\n",
    "    fig.legend(loc='upper center', borderaxespad=2.5)\n",
    "    fig.show()\n",
    "    factor = threadStats[-1,1]/threadStats[0,1]\n",
    "    print(\"Time fraction with 16 threads: {0}%, {1}x faster\".format(100*factor, 1/factor))\n",
    "#PlotThreadStats('stats_large_suil-bheag.txt')\n",
    "#PlotThreadStats('stats_large_cuinneag.txt')\n",
    "\n",
    "PlotThreadStatsForPaper('stats_large_cuinneag.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing batch scaling\n",
    "\n",
    "Look at how runtime scales with batch size. To run this on your own machine, edit the code below to create empty arrays batchScalingResultsCPU and batchScalingResultsGPU, and the code will automatically run benchmarks for every batch size specified in batchSizesCPU/GPU.\n",
    "\n",
    "Initially I just looked at the backprojection here, to keep the run times manageable. It's satisfying to see that the results are very well modelled by a fixed overhead (presumably calculation of F(H)) plus an extra factor that scales very linearly with the batch size.\n",
    "\n",
    "Relatively speaking, the GPU baseline is much lower (i.e. we don't need as large a batch size to amortise it away). That's good news in terms of GPU RAM consumption. However, it does lead me to suspect that my GPU implementation of my special FFT is less optimised. It's *possible* the GPU is just less good at that, but it probably means I could be doing more to optimise that code, if I really put my mind to it.\n",
    "\n",
    "The same pattern seems to apply to the full deconvolution (which I've sampled for a few batch sizes), and it's gratifying to see that we don't pick up any additional overheads or penalties here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume running on suil-bheag\n",
      "No GPU available - not benchmarking\n",
      "No GPU available - not benchmarking\n"
     ]
    }
   ],
   "source": [
    "import benchmark\n",
    "import projector as proj\n",
    "import numpy as np\n",
    "\n",
    "if True:\n",
    "    # Datasets calculated earlier on suil-bheag CPU and GPU\n",
    "    print(\"Assume running on suil-bheag\")\n",
    "    batchSizesGPU = np.array(1+np.arange(24))\n",
    "    batchScalingResultsGPU = [2.0887858867645264, 2.650907516479492, 3.1582248210906982, 3.639864206314087, 4.187718629837036, 4.7083728313446045, 5.515699625015259, 6.032058954238892, 6.538254976272583, 6.759566068649292, 7.901719570159912, 7.820116281509399, 9.093420267105103, 8.917783260345459, 9.327208280563354, 9.943439483642578, 11.797546148300171, 11.179245710372925, 12.977349758148193, 12.188851594924927, 12.5778169631958, 13.543910503387451, 14.43958854675293, 14.407610893249512]\n",
    "\n",
    "    batchSizesCPU = np.array(list(range(1,8,1)) + list(range(8,16,2)) + list(range(16,33,4)))\n",
    "    batchScalingResultsCPU = [70.64740204811096,77.01506614685059,84.01113772392273,90.80544757843018,98.12121725082397,\n",
    "                                  104.44658899307251,111.3729362487793,117.50704097747803,131.72836804389954,144.44977974891663,\n",
    "                                  156.9799840450287,170.16073203086853,197.4131510257721,223.088045835495,249.77226161956787,\n",
    "                                  274.7552146911621]\n",
    "\n",
    "    batchSizesGPUdc = np.array([1, 2, 4, 8, 16, 24])\n",
    "    batchScalingResultsGPUdc = [19.04511833190918, 23.87207317352295, 36.4692268371582, 51.96277904510498, 91.1813862323761, 135.37568163871765]\n",
    "\n",
    "    batchSizesCPUdc = np.array([1, 2, 4, 8, 16, 32])\n",
    "    batchScalingResultsCPUdc = [628.8385584354401, 686.4196479320526, 811.7965116500854, 1050.5633614063263, 1522.4902625083923, 2508.4796035289764]\n",
    "else:\n",
    "    # Datasets calculated earlier on macbook CPU\n",
    "    print(\"Assume running on macbook\")\n",
    "    # Benchmarking results: [[2.9634785652160645, 2.926506519317627, 3.040677785873413, 2.7863423824310303], [1.3397929668426514, 1.3884947299957275, 1.3806533813476562, 1.3544728755950928]]\n",
    "    batchSizesCPU = np.array([1, 32])\n",
    "    batchScalingResultsCPU = [105.0, 449.6]\n",
    "\n",
    "    batchSizesCPUdc = np.array([1, 32])\n",
    "    batchScalingResultsCPUdc = [916.7, 4480.0]\n",
    "\n",
    "# These loops will only calculate any missing entries that are not yet present in the above precalculated arrays,\n",
    "# to avoid taking ages to run this cell every time you just want to analyze the results\n",
    "for batchSize in batchSizesCPU[len(batchScalingResultsCPU):]:\n",
    "    # Benchmark for this batch size\n",
    "    batchScalingResultsCPU.append(benchmark.main(benchmarkGPU=False, prefix=['olaf-image', 'olaf-matrix', 'x{0}'.format(batchSize)])[0][0])\n",
    "    \n",
    "if proj.gpuAvailable:\n",
    "    for batchSize in batchSizesGPU[len(batchScalingResultsGPU):]:\n",
    "        # Benchmark for this batch size\n",
    "        batchScalingResultsGPU.append(benchmark.main(benchmarkCPU=False, prefix=['olaf-image', 'olaf-matrix', 'x{0}'.format(batchSize), 'volumes-on-gpu'])[0][0])    \n",
    "        # These next lines clear the FFT plan cache.\n",
    "        # In every loop iteration we compute different-shaped FFTs, so the cached plans\n",
    "        # are just taking up memory without being useful. Without clearing like this,\n",
    "        # we gradually fill up the GPU memory with redundant cached data.\n",
    "        import cupy as cp\n",
    "        cp.fft.config.get_plan_cache().clear()\n",
    "else:\n",
    "    print(\"No GPU available - not benchmarking\")    \n",
    "    \n",
    "# Run the full deconvolution for a limited set of batch sizes, just to confirm that the scalings still apply\n",
    "for batchSize in batchSizesCPUdc[len(batchScalingResultsCPUdc):]:\n",
    "    batchScalingResultsCPUdc.append(benchmark.main(benchmarkGPU=False, prefix=['olaf-image', 'olaf-matrix', 'deconv', 'x{0}'.format(batchSize)])[0][0])\n",
    "    \n",
    "if proj.gpuAvailable:    \n",
    "    for batchSize in batchSizesGPUdc[len(batchScalingResultsGPUdc):]:\n",
    "        batchScalingResultsGPUdc.append(benchmark.main(benchmarkCPU=False, prefix=['olaf-image', 'olaf-matrix', 'deconv', 'x{0}'.format(batchSize), 'volumes-on-gpu'])[0][0])    \n",
    "        import cupy as cp\n",
    "        cp.fft.config.get_plan_cache().clear()\n",
    "else:\n",
    "    print(\"No GPU available - not benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def PlotResults(batchSizes, batchScalingResults, desc):\n",
    "    batchScalingResults = np.array(batchScalingResults)\n",
    "    batchSizes = batchSizes[:len(batchScalingResults)]\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Time ({0})\".format(desc))\n",
    "    plt.xlabel(\"Batch size\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.plot(batchSizes, batchScalingResults)\n",
    "    plt.plot(batchSizes, batchScalingResults, '.')\n",
    "    mc = np.polyfit(batchSizes, batchScalingResults, 1)\n",
    "    plt.plot(batchSizes, mc[0]*batchSizes+mc[1])\n",
    "    plt.ylim(0,None)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Time per image ({0})\".format(desc))\n",
    "    plt.xlabel(\"Batch size\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.plot(batchSizes, batchScalingResults / batchSizes)\n",
    "    plt.plot(batchSizes, batchScalingResults / batchSizes, '.')\n",
    "    plt.ylim(0,None)\n",
    "    plt.show()\n",
    "    print(\"{0} baseline: {1:.2f}s. Gradient: {2:.2f}s/image\".format(desc, mc[1], mc[0]))\n",
    "    \n",
    "def PlotResultsPaper(batchSizes, batchScalingResults, filename, desc):\n",
    "    batchScalingResults = np.array(batchScalingResults)\n",
    "    batchSizes = batchSizes[:len(batchScalingResults)]\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.set_xlabel(\"Batch size\")\n",
    "    ax.set_ylabel(\"Time (s)\")\n",
    "    fdMapping = lambda y: y*batchScalingResults[0]/batchScalingResults[-1]\n",
    "    bkMapping = lambda y: y/batchScalingResults[0]*batchScalingResults[-1]\n",
    "    ax2 = ax.secondary_yaxis('right', functions=(fdMapping, bkMapping))\n",
    "    ax2.set_ylabel(\"Time per image (s)\")\n",
    "\n",
    "    ax.plot(batchSizes, batchScalingResults, '.')\n",
    "    mc = np.polyfit(batchSizes, batchScalingResults, 1)\n",
    "    ax.plot(batchSizes, mc[0]*batchSizes+mc[1], color='blue', label=f'Total elapsed time ({desc})')\n",
    "    ax.set_ylim(0,None)\n",
    "    \n",
    "    ax.plot(batchSizes, bkMapping(batchScalingResults / batchSizes), '.')\n",
    "    batchSizes2 = np.arange(1, batchSizes[-1]+1e-6, 0.01)\n",
    "    ax.plot(batchSizes2, bkMapping((mc[0]*batchSizes2+mc[1])/batchSizes2), color='orange', label='Time per image (s)')\n",
    "\n",
    "    leg = fig.legend(loc='upper center', borderaxespad=4)\n",
    "    fig.savefig(filename)\n",
    "    # The next two lines are a cosmetic workaround.\n",
    "    # For some reason I need a larger borderaxespad for the saved pdf figure,\n",
    "    # compared to the one displayed inline in this notebook\n",
    "    leg.remove()\n",
    "    fig.legend(loc='upper center', borderaxespad=2.5)\n",
    "    fig.show()\n",
    "    print(\"{0} baseline: {1:.2f}s. Gradient: {2:.2f}s/image\".format(filename, mc[1], mc[0]))\n",
    "    \n",
    "if False:\n",
    "    PlotResults(batchSizesCPU, batchScalingResultsCPU, \"CPU\")\n",
    "    PlotResults(batchSizesGPU, batchScalingResultsGPU, \"GPU\")\n",
    "    PlotResults(batchSizesCPUdc, batchScalingResultsCPUdc, \"CPU deconv\")\n",
    "    PlotResults(batchSizesGPUdc, batchScalingResultsGPUdc, \"GPU deconv\")\n",
    "\n",
    "PlotResultsPaper(batchSizesCPUdc, batchScalingResultsCPUdc, \"cpu_deconv.pdf\", \"CPU\")\n",
    "PlotResultsPaper(batchSizesGPUdc, batchScalingResultsGPUdc, \"gpu_deconv.pdf\", \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Work in progress: look at forward projection slowness\n",
    "I've noticed that forward projection is slower than backprojection on my macbook (although I have not seen this on other platforms). Let's try and investigate why...\n",
    "\n",
    "I am struggling to reproduce this, actually. With olaf and x16 I maybe have slightly elevated rusage for forward projection, but no clear difference in overall run time. With olaf x32 on cuinneag I see no trend at all.\n",
    "\n",
    "I haven't actually rerun x32 on macbook pro, I wonder if that will show the problem again or if it will have gone away??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See if I see this on a smaller testcase that's easier to play with\n",
    "\n",
    "''' \n",
    "    Olaf, x16, macbook\n",
    "        Running with batch image shape (16, 1463, 1273), batch x16\n",
    "        work elapsed wallclock time 260.581940\n",
    "        Total work delta rusage: [1002.017153   25.666372]\n",
    "        work elapsed wallclock time 267.333674\n",
    "        Total work delta rusage: [1024.208123   28.725325]\n",
    "        work elapsed wallclock time 267.539262\n",
    "        Total work delta rusage: [993.014693  33.542868]\n",
    "        work elapsed wallclock time 264.884809\n",
    "        Total work delta rusage: [1011.825912   29.671406]\n",
    "        work elapsed wallclock time 267.695029\n",
    "        Total work delta rusage: [1001.905267   29.950696]\n",
    "    time: 1329.85. overall delta rusage: [5033.941084  148.399634]\n",
    "    \n",
    "    Olaf, x32, cuinneag\n",
    "        Running with batch image shape (32, 1463, 1273), batch x32\n",
    "        work elapsed wallclock time 104.480644\n",
    "        Total work delta rusage: [1511.719767   44.051956]\n",
    "        work elapsed wallclock time 104.862145\n",
    "        Total work delta rusage: [1518.188842   53.990656]\n",
    "        work elapsed wallclock time 109.053886\n",
    "        Total work delta rusage: [1531.75683    52.955494]\n",
    "        work elapsed wallclock time 104.540129\n",
    "        Total work delta rusage: [1531.171942   41.989661]\n",
    "        work elapsed wallclock time 108.401333\n",
    "        Total work delta rusage: [1519.177746   56.641623]\n",
    "    time: 533.41. overall delta rusage: [7613.053808  250.65336 ]\n",
    "\n",
    "'''\n",
    "import benchmark\n",
    "import projector as proj\n",
    "import numpy as np\n",
    "\n",
    "batchSizesCPUdc = np.array([1, 16])\n",
    "batchScalingResultsCPUdc = []\n",
    "\n",
    "import py_light_field as plf\n",
    "plf.SetProgressReportingInterval(10.0)\n",
    "\n",
    "# Run the full deconvolution for a limited set of batch sizes, just to confirm that the scalings still apply\n",
    "for batchSize in batchSizesCPUdc[len(batchScalingResultsCPUdc):]:\n",
    "    # Doesn't seem to occur for non-olaf x16:\n",
    "    #batchScalingResultsCPUdc.append(benchmark.main(benchmarkGPU=False, prefix=['deconv', 'i2', 'x{0}'.format(batchSize)])[0][0])\n",
    "    #batchScalingResultsCPUdc.append(benchmark.main(benchmarkGPU=False, prefix=['olaf-image', 'olaf-matrix', 'i2', 'deconv', 'x{0}'.format(batchSize)])[0][0])\n",
    "    batchScalingResultsCPUdc.append(benchmark.main(benchmarkGPU=False, prefix=['olaf-image', 'olaf-matrix', 'i1', 'deconv', 'x{0}'.format(batchSize)])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benchmark\n",
    "# Testing same mini-scenario as I was testing in matlab\n",
    "benchmark.main(benchmarkGPU=False,\n",
    "               prefix=['olaf-image', 'olaf-matrix-reduced', 'i4', 'deconv', 'x4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
