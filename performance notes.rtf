{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fswiss\fcharset0 Helvetica-Oblique;
\f3\fnil\fcharset0 Menlo-Regular;\f4\fnil\fcharset0 Menlo-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red47\green180\blue29;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c20238\c73898\c14947;}
\paperw11900\paperh16840\margl1440\margr1440\vieww23740\viewh21600\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Backprojection code run times\ulnone \
Command-line parameters are for lf_performance_analysis.py - 27 z planes. Remember to prepend with prime-cache \
\
Times on my Mac Pro:\
\pard\tx7633\pardirnatural\partightenfactor0
\cf0 \
\pard\tx5465\pardirnatural\partightenfactor0
\cf0 MATLAB 2019a (multithreaded, backprojection)	27s (using 4-500% CPU)\
old (x1), old code:	
\f1\b 243.5s
\f0\b0 \
new (x1):	43.2s\
	8.2s with 8 threads (5.3x speedup) \
new-batch (x30):	332 (95% in C ProjectForZ)\
	93s with 4 threads (3.6x speedup compared to single-threaded).\
	55.9s with 8 threads (5.9x speedup compared to single-threaded). \
\
Times on on my Macbook Pro:\
MATLAB 2020a (multithreaded, backprojection)	14s (same performance on 2019a)\
new (x1):	35.4 with 1 thread\
	11s with 4 threads (3.2x speedup compared to single-threaded)\
new-batch (x30):	178s with 1 thread\
	56s with 4 threads (3.2x speedup compared to single-threaded)\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
For the near-focal-plane deconvolution problem I am currently working with, it takes 3.2s on my mac pro and 2.4s on beag-shuil [these times are outdated, but are with caching of F(H) and its transpose]. In both cases, ~78% of the time is spent in convolve. I suspect these are heavily memory-bound, since the speedup with 8 threads is only 6x (mac pro) and 4x (beag-shuil).\

\f2\i \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary benchmarks on various machines\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone Mac pro 2009 [8]		
\f3\fs22 \CocoaLigature0 [[55.92917013168335], [7.395716190338135, 7.230978965759277], [3.470026969909668, 3.47963285446167]]\

\f0\fs24 \CocoaLigature1 Macbook Pro 2019 [4] 
\f3\fs22 \cf2 \CocoaLigature0 	[[56.85784101486206], [11.341279029846191, 11.382513046264648], [3.6087489128112793, 3.3346760272979736, 3.2811148166656494]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=2400, min=2400, max=2400)
\f0\fs24 \cf0 \CocoaLigature1 \
new-brutha [24]
\f2\i  		
\f3\i0\fs22 \cf2 \CocoaLigature0 [[17.89953589439392], [3.4818685054779053, 3.828882932662964], [1.6241559982299805, 1.5127336978912354, 1.2638728618621826]]\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					Machine specs:
\f3\b0 \cf2  physical_cpus:24 logical_cpus:48 scpufreq(current=1762.0258124999993, min=1200.0, max=2500.0)\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \CocoaLigature1 suil-bheag [16]
\f2\i 			
\f3\i0\fs22 \cf2 \CocoaLigature0 [[13.396210432052612], [2.9962103366851807, 3.0246641635894775], [1.5529916286468506, 1.3653972148895264, 1.1758782863616943]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:16 logical_cpus:32 scpufreq(current=3200.0, min=0.0, max=0.0)\

\f0\fs24 \cf0 \CocoaLigature1 optic-tectum [4]			
\f3\fs22 \cf2 \CocoaLigature0 [[54.097476959228516], [9.557530164718628, 9.490315914154053], [3.5699703693389893, 3.5606672763824463, 3.5801541805267334], \
					 [5.8761820793151855], [1.5330743789672852, 1.4889581203460693], [1.5329170227050781, 1.5079047679901123, 1.507934331893921]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=1961.49075, min=1600.0, max=4100.0)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					GPU specs:
\f3\b0 \cf2 \
					 2048 threads x 20 processors\
					 Clock speed 1.7335GHz, mem speed 5.005GHz x 32B = 160.16GB/s, L2 2.10MB\
					 Total GPU RAM 8.51GB\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 Jose\'92s computer [8]
\f3 		
\fs22 [[26.19478750228882], [5.381789445877075, 5.477412462234497], [1.9234905242919922, 1.9264616966247559, 1.9024109840393066], 					 [4.183220148086548], [1.2685494422912598, 1.2638397216796875], [1.2534205913543701, 1.2854418754577637, 1.3035414218902588]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:8 logical_cpus:32 scpufreq(current=2102.4765937499997, min=2200.0, max=3750.0)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					GPU specs:
\f3\b0 \cf2 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
					 2048 threads x 28 processors\
					 Clock speed 1.582GHz, mem speed 5.505GHz x 44B = 242.22GB/s, L2 2.88MB\
					 Total GPU RAM 11.70GB\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f2\i\fs24 \cf0 \CocoaLigature1 \
Note that the GPU implementation of the large-scale deconvolution is blazing fast compared to the CPU! 
\f0\i0 I may also be able to improve the speed of the small PIV case if I revisit that - when it\'92s only taking 2s there are probably significant python overheads.\
For now I\'92ve confirmed that x30 parallelism is sufficient on the GPU, but I should keep an eye on that if I make further improvements to the GPU code\
\
Interestingly, padding the convolution seems to have given no improvement (or degradation) whatsoever on the Mac Pro, for the x30 case. It does for x1. It looks like the FFTs are indeed faster (48s instead of 67s) but convolution is slower (360s instead of 348s) and there is 10s more rusage for the same run time.\
\

\f2\i \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - A single thread of my code is ~2.4x faster than the (parallel) MATLAB code, if I am allowed to process batch of 30 frames. With 4 threads it is 7.5x faster; with 8 threads it is 14x faster. It will be hard to speed the code up any more, but see comments below for possible things to try. I\'92m not quite sure why it scales better on my mac pro than on my macbook pro, but one possibility is that my parallelisation might be better than Matlab\'92s?\
- My code is primarily focused on processing batches, but I shouldn\'92t forget that I am personally interested in deconvolving small numbers of planes from one or two images, for my PIV work. \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Memory consumption\ulnone \
Jos\'e9\'92s example tiff file (1770x1755px) was large enough to cause memory consumption challenges.\
\
If we run on the CPU we use several tens of GB of memory. The reason is explained in the comment where the Compare() function is used in py_light_field.cpp. If I switch to the alternative ordering that is disabled in the code (which has mutex contention problems, but lower memory consumption) we use much less memory, as expected. The ideal fix would be to define a new ordering that gives enough flexibility to reduce mutex contention but does not demand quite so much memory. 
\f1\b I still want to fix this
\f0\b0 . One option would be to batch up by the same factor as the number of CPUs\'85\
\
If we run on the GPU then the FFT plan was using huge amounts of memory. I decided we don\'92t need to run such big batches, and the new global flag reducedGPUMemoryUsage=True brings it down to 8GB of memory for a 1500x1500px image (but note that it just runs out of 8.5GB of memory if I 
\f2\i don\'92t
\f0\i0  query the memory using while running[!?]). As expected, the FFT plan change has zero impact on performance.\
\
That reduction in consumption should enable Jos\'e9 to do the full deconvolution on his 11.7GB GPU. If we wanted to scale to even bigger images I would need to think about the best strategy. It\'92s now the actual cache of F(H) that is the main memory consumer. It would require some rejigging to change the ordering such that we didn\'92t have to cache those.\
\
I should also think about how the memory consumption grows with batch size - and total problem size. Batch size will inevitably increase the requirements - in Jos\'e9\'92s test case by about 10x the deconvolved .tif file size for each timepoint - and that should at least be documented. As explained in separate file \'93gpu-ram-consumption-notes.rtf\'94 I have not managed to fully understand the behaviour.\
\
At the moment it doesn\'92t matter how long the overall time series is, although that might change if I wanted to save the output all into one big file instead of in separate files for each timepoint.\
\
\ul Things I should still investigate\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - 
\f1\b [Reminder to check separate notes about GPU performance, and GPU RAM consumption]\

\f0\b0 \
Can I improve the speedup obtained from 8-way threading? I am getting 800% CPU load, but am only getting 5-6x speedup overall. I and increasingly convinced the issue is memory bandwidth (e.g. on my 2cpu x 4core machine). Basically, everything just takes a little bit longer. I did see a ~10% speedup by combining operations into CalculateRowBoth() to reduce memory bandwidth demands, but I reckon any further batching would be a lot of effort for marginal gain. For the combining operations, I eliminated 25% of what I think are the uncached reads, so the net 10% speedup is perhaps a bit disappointing, but not completely out of the ballpark I would have expected. Even for small problem sets, I would need to batch things up row-by-row or I would be overflowing caches - I don\'92t think it\'92s enough to process items one after the other on the same thread.\
\
- I tried changing the sequencing of my operations to prevent batching up of convolves. The new order took a tiny bit longer to run overall, but the parallelism was poor for small batch sizes due to mutex contention. In the absence of mutex contention, it runs slightly faster, but it doesn\'92t seem to be a huge change. Looking at histograms of block run times, removing the sorting doesn\'92t make a huge difference. In either case, things just generally take \'93a bit longer\'94 when multithreaded. It would be nice to be able to improve the thread scaling, but my suspicion is that we are simply limited by memory bandwidth, in which case multithreading is never going to have an ideal scaling.\
\
- For a full problem, I really am doing huge numbers of individual convolve operations. It will be very unwieldy to fuse multiple operations, but I reckon I might get a significant speedup if I process 2 or 4 timepoints simultaneously (which makes it easier to apply \ul vectorised\ulnone  multiplications), 
\f2\i and
\f0\i0  treat four fourier arrays simultaneously (less overall memory pressure on accum and on reading FH). I\'92m always optimistic with these things, but it seems to be like a >2x speedup should be within grasp, in principle\'85\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Dead ends\ulnone \
- If I set fft planning mode to patient, it may be marginally faster. Certainly it seems to about 10% faster for the irfft, although there seems to be no significant change for ProjectForZ, which is by far the bottleneck. And patient planning takes about 5 minutes to set up! I wonder if the lack of overall improvement is because precise measurements are meaningless without all the other threads running in parallel and using up memory bandwidth? Certainly it seems to only deliver marginal gains, if any.\
\ul \
Miscellaneous notes\ulnone \
- Note that there is detailed examination of performance in deconvolution-performance-analysis.ipynb.\
- Hit a weird issue with implementation of complex operator* with LLVM which led to a factor of 2 slowdown in my entire projection code(!). Workaround added to py_light_field source code.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 - I have included support for caching F(H) in my C code. It\'92s impractical for large problems, but just about fits in memory with the small problem I am looking at for PIV.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 - I have completely dropped multithreading for fftw, and am doing the threading myself. Multithreaded fftw seems to be buggy (or I am somehow using it wrong) - takes way 
\f2\i longer
\f0\i0  with multiple threads, in a rather random way.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Low priority\ulnone \
- In principle I could teach fftw to work with higher prime values, including 19. The details are here http://www.fftw.org/fftw3_doc/Generating-your-own-code.html#Generating-your-own-code but it will involve a bit of playing around to see if it is possible.\
\
- I could amortise out the multipliers in the convolution, and apply them to the FFT array just once regardless of batch size. I would need to watch how I handle MirrorRow (would that require different multipliers?) and the multiplications only seem to take 5% of the total run time on my Macbook Pro, so it is probably not high priority.\
\
- [Apr 2021: I\'92m not sure what this old note means - I would need to look back through the code to try and make sense of it] I can probably speed up my ifft a bit by doing it in one go (it wastes a lot of time at the moment, and shouldn\'92t need to clear the whole array - we only need a little bit of zero-padding))}