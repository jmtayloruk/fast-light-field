{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fswiss\fcharset0 Helvetica-Oblique;
\f3\fnil\fcharset0 Menlo-Regular;\f4\fnil\fcharset0 Menlo-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red47\green180\blue29;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c20238\c73898\c14947;}
\paperw11900\paperh16840\margl1440\margr1440\vieww21520\viewh16580\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Backprojection code run times\ulnone \
Command-line parameters are for lf_performance_analysis.py - 27 z planes. Remember to prepend with prime-cache \
\
Times on my Mac Pro:\
\pard\tx7633\pardirnatural\partightenfactor0
\cf0 \
\pard\tx5465\pardirnatural\partightenfactor0
\cf0 MATLAB 2019a (multithreaded, backprojection)	27s (using 4-500% CPU)\
old (x1), old code:	
\f1\b 243.5s
\f0\b0 \
new (x1):	43.2s\
	8.2s with 8 threads (5.3x speedup) \
new-batch (x30):	332 (95% in C ProjectForZ)\
	93s with 4 threads (3.6x speedup compared to single-threaded).\
	55.9s with 8 threads (5.9x speedup compared to single-threaded). \
\
Times on on my Macbook Pro:\
MATLAB 2020a (multithreaded, backprojection)	14s (same performance on 2019a)\
new (x1):	35.4 with 1 thread\
	11s with 4 threads (3.2x speedup compared to single-threaded)\
new-batch (x30):	178s with 1 thread\
	56s with 4 threads (3.2x speedup compared to single-threaded)\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
For the near-focal-plane deconvolution problem I am currently working with, it takes 3.2s on my mac pro and 2.4s on beag-shuil [these times are outdated, but are with caching of F(H) and its transpose]. In both cases, ~78% of the time is spent in convolve. I suspect these are heavily memory-bound, since the speedup with 8 threads is only 6x (mac pro) and 4x (beag-shuil).\
\

\f2\i \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary benchmarks on various machines\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone Mac pro 2009 [8]		
\f3\fs22 \CocoaLigature0 [[55.92917013168335], [7.395716190338135, 7.230978965759277], [3.470026969909668, 3.47963285446167]]\

\f0\fs24 \CocoaLigature1 Macbook Pro 2019 [4] 
\f3\fs22 \cf2 \CocoaLigature0 	\cf2 [[56.85784101486206], [11.341279029846191, 11.382513046264648], [3.6087489128112793, 3.3346760272979736, 3.2811148166656494]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=2400, min=2400, max=2400)
\f0\fs24 \cf0 \CocoaLigature1 \
optic-tectum [4]			
\f3\fs22 \cf2 \CocoaLigature0 [[54.097476959228516], [9.557530164718628, 9.490315914154053], [3.5699703693389893, 3.5606672763824463, 3.5801541805267334], \
					 [5.648859024047852], [1.7718734741210938, 1.7605414390563965], [1.755242109298706, 1.7526016235351562, 1.763211965560913]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=1961.49075, min=1600.0, max=4100.0)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					GPU specs:
\f3\b0 \cf2 \
					 2048 threads x 20 processors\
					 Clock speed 1.7335GHz, mem speed 5.005GHz x 32B = 160.16GB/s, L2 2.10MB\
					 Total GPU RAM 8.51GB\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \CocoaLigature1 new-brutha [24]
\f2\i  		
\f3\i0\fs22 \cf2 \CocoaLigature0 [\cf2 [17.89953589439392], [3.4818685054779053, 3.828882932662964], [1.6241559982299805, 1.5127336978912354, 1.2638728618621826]]\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					Machine specs:
\f3\b0 \cf2  physical_cpus:24 logical_cpus:48 scpufreq(current=1762.0258124999993, min=1200.0, max=2500.0)
\f2\i\fs24 \cf0 \CocoaLigature1 	\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 Note that the GPU implementation of the large-scale deconvolution is blazing fast compared to the CPU! 
\f0\i0 I may also be able to improve the speed of the small PIV case if I revisit that - when it\'92s only taking 2s there are probably significant python overheads.\
For now I\'92ve confirmed that x30 parallelism is sufficient on the GPU, but I should keep an eye on that if I make further improvements to the GPU code\
\
Interestingly, padding the convolution seems to have given no improvement (or degradation) whatsoever on the Mac Pro, for the x30 case. It does for x1. 
\f2\i I wonder if the extra memory access is impacting the convolutions more than the gain in FFT speed?
\f0\i0 \
\

\f2\i \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - A single thread of my code is ~2.4x faster than the (parallel) MATLAB code, if I am allowed to process batch of 30 frames. With 4 threads it is 7.5x faster; with 8 threads it is 14x faster. It will be hard to speed the code up any more, but see comments below for possible things to try.\
- My code is primarily focused on processing batches, but I shouldn\'92t forget that I am personally interested in deconvolving small numbers of planes from one or two images, for my PIV work. \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Things I should still investigate\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone Although this may not be important, 
\f1\b there\'92s something weird going on
\f0\b0  with lf_performance_analysis.py old. It\'92s running implausibly quickly compared to my new code (and matlab). I think it must be doing less work somehow, although it\'92s not immediately obvious to me how/why that\'92s happening.
\f1\b \
\
Does the GPU code scale up
\f0\b0  to a full-size problem, or do I find that memory restrictions prevent me from doing a x30 reconstruction? Each volume is 100MB, so doing x30 might be optimistic. However, if needed I could do things one plane at a time (doing the iFFT and copying back to RAM) without that incurring a huge overhead I think.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f1\b \cf0 Can I further improve GPU performance
\f0\b0  somehow?
\f1\b \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\b0 \cf0 \
Can I improve the speedup obtained from 8-way threading? I am getting 800% CPU load, but am only getting 5-6x speedup overall. I and increasingly convinced the issue is memory bandwidth (e.g. on my 2cpu x 4core machine). Basically, everything just takes a little bit longer. I did see a ~10% speedup by combining operations into CalculateRowBoth() to reduce memory bandwidth demands, but I reckon any further batching would be a lot of effort for marginal gain. For the combining operations, I eliminated 25% of what I think are the uncached reads, so the net 10% speedup is perhaps a bit disappointing, but not completely out of the ballpark I would have expected. Even for small problem sets, I would need to batch things up row-by-row or I would be overflowing caches - I don\'92t think it\'92s enough to process items one after the other on the same thread.\
\
- I tried changing the sequencing of my operations to prevent batching up of convolves. The new order took a tiny bit longer to run overall, but the parallelism was poor due to mutex contention. If I could reduce the mutex contention then I get the run time under 6s (just over 10% speedup). However, it\'92s possible I\'92m just gaining performance due to reduced bandwidth contention when some threads are blocked on mutexes! If I want to try this new ordering, I will need to sort out the mutex issue somehow. However, I think the main reason I was trying the reorder was just because I didn\'92t understand why things were happening in bursts. Since I don\'92t think the bursts are a problem for cache residency, it may be that they aren\'92t causing a problem. My only thought had been that it might well help (for cases with few timepoints) to spread out the FFTs more evenly (and therefore possibly ease memory bandwidth when running with max threads) rather than doing them in bigger batches.\
\
- I could amortise out the multipliers in the convolution, and apply them to the FFT array just once regardless of batch size. I would need to watch how I handle MirrorRow (would that require different multipliers?) and the multiplications only seem to take 5% of the total run time on my Macbook Pro\
\
- I notice with single-threaded macbook air there is a huge variation in the run time of both convolves and FFTs. Could that be because, for this small problem size, the CPU caches *are* being used effectively (sometimes)? If so, then I should perhaps revisit that and think about whether I can use them more effectively.\
\
- For a full problem, I really am doing huge numbers of individual convolve operations. It will be very unwieldy to fuse multiple operations, but I reckon I might get a significant speedup if I process 2 or 4 timepoints simultaneously (which makes it easier to apply \ul vectorised\ulnone  multiplications), 
\f2\i and
\f0\i0  treat four fourier arrays simultaneously (less overall memory pressure on accum and on reading FH). I\'92m always optimistic with these things, but it seems to be like a >2x speedup should be within grasp, in principle\'85\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Dead ends\ulnone \
- If I set fft planning mode to patient, it may be marginally faster. Certainly it seems to about 10% faster for the irfft, although there seems to be no significant change for ProjectForZ, which is by far the bottleneck. And patient planning takes about 5 minutes to set up! I wonder if the lack of overall improvement is because precise measurements are meaningless without all the other threads running in parallel and using up memory bandwidth? Certainly it seems to only deliver marginal gains, if any.\
\ul \
Miscellaneous notes\ulnone \
- Note that there is detailed examination of performance in deconvolution-performance-analysis.ipynb.\
- Hit a weird issue with implementation of complex operator* with LLVM which led to a factor of 2 slowdown in my entire projection code(!). Workaround added to py_light_field source code.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 - I have included support for caching F(H) in my C code. It\'92s impractical for large problems, but just about fits in memory with the small problem I am looking at for PIV.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 - I have completely dropped multithreading for fftw, and am doing the threading myself. Multithreaded fftw seems to be buggy (or I am somehow using it wrong) - takes way 
\f2\i longer
\f0\i0  with multiple threads, in a rather random way.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Low priority\ulnone \
- [Apr 2021: I\'92m not sure what this old note means - I would need to look back through the code to try and make sense of it] I can probably speed up my ifft a bit by doing it in one go (it wastes a lot of time at the moment, and shouldn\'92t need to clear the whole array - we only need a little bit of zero-padding))}