{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Oblique;\f2\fnil\fcharset0 Menlo-Regular;
\f3\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww21520\viewh19580\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Jun 2020 status\ulnone \
The entire projection operation is implemented in pure C code, with a lot of work put in to making the parallelism efficient.\
I have completely dropped multithreading for fftw, and am doing the threading myself. Multithreaded fftw seems to be buggy (or I am somehow using it wrong) - takes way 
\f1\i longer
\f0\i0  with multiple threads, in a rather random way.\
I have included support for caching F(H) in my C code. It\'92s impractical for large problems, but just about fits in memory with the small problem I am looking at for PIV.\
\
\ul Backprojection code run times\ulnone \
Command-line parameters are for lf_performance_analysis.py - 27 z planes). All times are on Mac Pro.\
\pard\tx7633\pardirnatural\partightenfactor0
\cf0 \
MATLAB (multithreaded, backprojection)	27s (using 4-500% CPU)\
\pard\tx7633\tx13337\pardirnatural\partightenfactor0
\cf0 profile-old: Single video frame, old code:	243.5s\
profile-new: Single video frame:	50.7s\
	8.8s with 8 threads (5.8x speedup)       [8.4 with patient planning - which takes 227s for the irfft!]\
profile-new-batch: Batch of 10 video frames:	145s\
profile-new-batch: Batch of 30 video frames:	339s (95% in C ProjectForZ)\
	96s with 4 threads (3.5x speedup).\
	55s with 8 threads (5.7x speedup). \
Note: at one point I found performance had dropped on my mac pro. Reinstalling the py_light_field module brought things back to previous performance. I don\'92t know why that would be (maybe I forgot I had a slower experimental version installed?), but it\'92s worth watching out for.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
For the near-focal-plane deconvolution problem I am currently working with, it takes 3.2s on my mac pro and 2.4s on beag-shuil [both those times now include the inverse FFT, and are with caching of F(H) and its transpose]. In both cases, ~78% of the time is spent in convolve. I suspect these are heavily memory-bound, since the speedup with 8 threads is only 6x (mac pro) and 4x (beag-shuil).\
\
Note some timing anomalies with the old code:\
- It just uses fftconvolve, and evidently the underpinning implementation 
\f2\fs22 \CocoaLigature0 numpy.fft.fftpack_lite
\f0\fs24 \CocoaLigature1  (macbook air) is a 
\f1\i lot
\f0\i0  slower than the MKL implementation (mac pro). I shouldn\'92t read too much into the macbook air time for the old code.\
- I re-remembered that on my mac pro, the FFT backend is MKL, which runs multithreaded (and deadlocks with itself and with Parallel()) unless I disable MKL threading! I have now disabled it again.
\f1\i \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary benchmarks on various machines\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone Mac pro 2009 [8]	
\f2\fs22 \CocoaLigature0 [[55.92917013168335], [7.395716190338135, 7.230978965759277], [3.470026969909668, 3.47963285446167]]\

\f0\fs24 \CocoaLigature1 Macbook pro 2019 [4] 
\f2\fs22 \cf2 \CocoaLigature0  [[69.92228198051453], [12.731037139892578, 13.498766899108887], [4.1240808963775635, 3.237820863723755]]
\f0\fs24 \cf0 \CocoaLigature1 \
optic-tectum [4]		
\f2\fs22 \CocoaLigature0 [[45.88980317115784], [5.7575602531433105, 5.582576274871826], [3.3827452659606934, 3.3187263011932373], \
				 [6.2813591957092285], [1.7826769351959229, 1.803778886795044], [1.8202064037322998, 1.812258005142212]]
\f0\fs24 \CocoaLigature1 \
old-brutha [8]		
\f2\fs22 \CocoaLigature0 [[55.28426742553711], [11.448898792266846, 11.582984209060669], [4.616901636123657, 4.039685964584351]]
\f0\fs24 \CocoaLigature1 \
old-brutha [16]
\f1\i  		
\f2\i0\fs22 \CocoaLigature0 [[37.72136449813843], [6.984548568725586, 6.956429958343506], [3.386533737182617, 3.025592088699341]]\

\f0\fs24 \CocoaLigature1 old-brutha [32]
\f1\i  		
\f2\i0\fs22 \CocoaLigature0 [[30.813824892044067], [5.391945838928223, 5.221055269241333], [3.0579516887664795, 2.902742624282837]]\

\f0\fs24 \CocoaLigature1 new-brutha [24]
\f1\i  	
\f2\i0\fs22 \cf2 \CocoaLigature0 [[23.076451539993286], [4.891697406768799, 4.976486682891846], [2.0639312267303467, 1.6893420219421387]]
\f1\i\fs24 \cf0 \CocoaLigature1 	\
Note that the GPU implementation of the large-scale deconvolution is blazing fast compared to the CPU! 
\f0\i0 I may also be able to improve the speed of the small PIV case if I revisit that - when it\'92s only taking 2s there are probably significant python overheads.\
For the larger benchmark, x15 takes 3.35 on the GPU, so x30 parallelism seems more than sufficient at the moment. It might become more of an issue if I did manage to further optimize my custom kernels so they don\'92t take as long\'85
\f1\i \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - My best-case scenario is now over 20x faster than the original naive code (single threaded). With 8 threads, it is ~125x faster!!\
- A single thread of my code is ~2.4x faster than the (parallel) MATLAB code, if I am allowed to process batch of 30 frames. With threads is it almost 14x faster. It will be hard to speed the code up any more, but see comments below for possible things to try.\
- My code is primarily focused on processing batches, but I shouldn\'92t forget that I am personally interested in deconvolving small numbers of planes from one or two images, for my PIV work. \
- Note that there is detailed examination of performance in deconvolution-performance-analysis.ipynb.\
- Hit a weird issue with implementation of complex operator* with LLVM which led to a factor of 2 slowdown in my entire projection code(!). Workaround added to py_light_field source code.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Dead ends\ulnone \
- If I set fft planning mode to patient, it may be marginally faster. Certainly it seems to about 10% faster for the irfft, although there seems to be no significant change for ProjectForZ, which is by far the bottleneck. And patient planning takes about 5 minutes to set up! I wonder if the lack of overall improvement is because precise measurements are meaningless without all the other threads running in parallel and using up memory bandwidth? 
\f3\b I should revisit this
\f0\b0  now I have realised the multithreaded FFTW is problematic.\
\
\ul Things I should still investigate\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f3\b \cf0 \ulnone Does the GPU code scale up
\f0\b0  to a full-size problem, or do I find that memory restrictions prevent me from doing a x30 reconstruction? Each volume is 100MB, so doing x30 might be optimistic. However, if needed I could do things one plane at a time (doing the iFFT and copying back to RAM) without that incurring a huge overhead I think.\
\
Can I improve the speedup obtained from 8-way threading? I am getting 800% CPU load, but am only getting 5.7x speedup overall. I and increasingly convinced the issue is memory bandwidth (e.g. on my 2cpu x 4core machine). Basically, everything just takes a little bit longer. I did see a ~10% speedup by combining operations into CalculateRowBoth() to reduce memory bandwidth demands, but I reckon any further batching would be a lot of effort for marginal gain. For the combining operations, I eliminated 25% of what I think are the uncached reads, so the net 10% speedup is perhaps a bit disappointing, but not completely out of the ballpark I would have expected. Even for small problem sets, I would need to batch things up row-by-row or I would be overflowing caches - I don\'92t think it\'92s enough to process items one after the other on the same thread.\
\
- I tried changing the sequencing of my operations to prevent batching up of convolves. The new order took a tiny bit longer to run overall, but the parallelism was poor due to mutex contention. If I could reduce the mutex contention then I get the run time under 6s (just over 10% speedup). However, it\'92s possible I\'92m just gaining performance due to reduced bandwidth contention when some threads are blocked on mutexes! If I want to try this new ordering, I will need to sort out the mutex issue somehow. However, I think the main reason I was trying the reorder was just because I didn\'92t understand why things were happening in bursts. Since I don\'92t think the bursts are a problem for cache residency, it may be that they aren\'92t causing a problem. My only thought had been that it might well help (for cases with few timepoints) to spread out the FFTs more evenly (and therefore possibly ease memory bandwidth when running with max threads) rather than doing them in bigger batches.\
\
- I notice with single-threaded macbook air there is a huge variation in the run time of both convolves and FFTs. Could that be because, for this small problem size, the CPU caches *are* being used effectively (sometimes)? If so, then I should perhaps revisit that and think about whether I can use them more effectively.\
- For a full problem, I really am doing huge numbers of individual convolve operations. It will be very unwieldy to fuse multiple operations, but I reckon I might get a significant speedup if I process 2 or 4 timepoints simultaneously (which makes it easier to apply \ul vectorised\ulnone  multiplications), 
\f1\i and
\f0\i0  treat four fourier arrays simultaneously (less overall memory pressure on accum and on reading FH). I\'92m always optimistic with these things, but it seems to be like a >2x speedup should be within grasp, in principle\'85\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Low priority\ulnone \
- I can probably speed up my ifft a bit by doing it in one go (it wastes a lot of time at the moment, and shouldn\'92t need to clear the whole array - we only need a little bit of zero-padding))\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 - Is fft performance impacted by the prime factors of the input array? (FFTs aren\'92t the main bottleneck, though, for large batch sizes)\ul \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f3\b \cf0 \ulnone \
TODO
\f0\b0 : check if any more work is required on the GPU implementation.}