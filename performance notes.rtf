{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf840
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww20440\viewh16500\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Jan 2020 status\ulnone \
I have now implemented special_fftconvolve in pure C code, and have implemented caching of the H matrices (though that does require a 
\b lot
\b0  of RAM, and is probably impractical for real-world scenarios with large numbers of z planes).\
I re-remembered that on my mac pro, the FFT backend is MKL, which runs multithreaded (and deadlocks with itself and with Parallel()) unless I disable MKL threading! I have now disabled it again.\
\
\ul Code run times\ulnone \
Command-line parameters are for lf_performance_analysis.py). All times are on Mac Pro.\
Note that I use a different image size for the \'91batch\'92 runs, so times can\'92t be directly compared\
\pard\tx7633\pardirnatural\partightenfactor0
\cf0 \
MATLAB (multithreaded, backprojection)	27s\
\pard\tx7633\tx13337\pardirnatural\partightenfactor0
\cf0 profile-old: Single video frame, old code:	123s\
profile-new: Single video frame:	97s (76% in FFTW and mmap)\
profile-new-2: Single video frame, cached H matrices:	18s (75% in special_fftconvolve/mirror/expand)\
profile-new-batch: Batch of 10 video frames:	191s (47% in special_fftconvolve, 32% in FFTW)	71s with 8 threads, rusage 290. ~140s with 2 threads, rusage 258.\
profile-new-batch2: Batch of 10 video frames, cached H matrices:	121s (75% in special_fftconvolve)\
profile-new-batch: Batch of 30 video frames:	406s (62% in special_fftconvolve, 19% in FFTW)\
profile-new-batch2: Batch of 30 video frames, cached H matrices:	334s (75% in special_fftconvolve)\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\i \cf0 What I measure on macbook air is 107s new code, of which 70% in FFTW and mmap, which is fairly consistent with what I had on the mac pro (slightly slower)\
However, the old code takes 403s, which is insanely slow (even with FFTW). Could it be an issue of the cache size on the Air?
\i0 \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Summary\
\ulnone - My best-case scenario is now 11x faster than the original naive code (single threaded). With 8 threads, it is 17x faster. \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\i \cf0 - However, it is actually kind of depressing that I got less than a 25% single-threaded speedup in all my initial rewriting!
\i0 \ul \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - My code is ~4x faster than the MATLAB code, if I am allowed to process batch of 10 frames. My optimized code can almost match the MATLAB run time if I run with 8 threads.\
- Matrix caching makes a huge difference for single images. It actually still makes a small difference even with significant batch sizes (>10), though the gain is smaller.\
- Batch processing is somewhat worth it with cached H matrices, but really comes into its own 
\i without
\i0  caching, where we can get >7x speedup if we batch process. Effectively we are amortising ~70s of FFTs across an additional time of ~11s per frame.\
- Even in the cached scenario, I am still getting (slight) performance gains with a batch size of 30 or more. \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\b \cf0 \
TODO
\b0 : I am still puzzled how/why the vanilla MATLAB code manages to keep up with my optimized code. It manages to use about 4-500% CPU, so it\'92s possible that it is just making better use of the parallelism than I am managing to do from within Python.\

\b TODO: 
\b0 multithreading could clearly be more efficient (e.g. there is about 50% extra rusage, with 8 threads, and average load is only about 400% CPU. That will be hard to investigate, but this is probably where I should be investing my effort if I want to speed up the CPU implementation further.\

\b TODO
\b0 : look into writing a GPU implementation.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
\ul \
\
Old notes about thread performance - may need updating/revisiting\ulnone \
\
Actual thread execution time seems to grow considerably with the number of threads, i.e. efficiency falls. I am not sure how to try and work out what the cause of that is. I could go back to working on dummy data (no transfers between processes) and see if that makes a difference to *that* in particular. (I think I may have looked only at the dead time overheads - which are also an issue).\
I looked at user and system cpu time, and with Instruments. Looks like 20% of time is spent in madvise (macbook, 2 threads). I am not sure exactly why or where that is happening. It seems to be related to python memory management in some way. I should check if that grows with number of threads on mac pro, and if it is the same when I use dummy work blocks rather than passing to subprocesses\
\
-> revisit this now I am using mmap rather than pickle - hopefully much of this is now fixed.\
\
I made these later notes:\
	Stats on mac pro across increasing numbers of threads show that dead time is no longer a significant problem.\
	However, work time does still increase with number of threads - around 1.5x by the time I hit 8 threads.\
	I can only imagine this is a memory bandwidth thing, whether in terms of sheer RAM bandwidth, cache performance, or overheads due to allocating memory.\
	I imagine that doing as many in-place operations as possible (and/or working in scratch space?) can only be a good thing for this.\
\
\ul Performance improvements to make\ulnone \
\
I could move the transpose to final operation, in the case of square arrays (since it's probably faster than reversing an array - although it may impact subsequent fft performance?). Or, I could just figure that images are not usually guaranteed to be square.\
\
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf0 \expnd0\expndtw0\kerning0
\
\
I should try:\
- would it help to change the axis order in rfftn? Raw speed, but also could be better for tiling to complete the matrix with conjugate\
- am I tiling in the most efficient order? \
- investigate whether fft performance is impacted by the prime factors of the input array}