{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fswiss\fcharset0 Helvetica-Oblique;
\f3\fnil\fcharset0 Menlo-Regular;\f4\fnil\fcharset0 Menlo-Bold;\f5\fnil\fcharset0 Menlo-Italic;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red47\green180\blue29;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c20238\c73898\c14947;}
\paperw11900\paperh16840\margl1440\margr1440\vieww27380\viewh16580\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Backprojection code run times\ulnone \
Command-line parameters are for lf_performance_analysis.py - 27 z planes. Remember to prepend with prime-cache \
\
Times on my Mac Pro:\
\pard\tx7633\pardirnatural\partightenfactor0
\cf0 \
\pard\tx5465\pardirnatural\partightenfactor0
\cf0 MATLAB 2019a (multithreaded, backprojection)	27s (using 4-500% CPU)\
old (x1), old code:	
\f1\b 243.5s
\f0\b0 \
new (x1):	43.2s\
	8.2s with 8 threads (5.3x speedup) \
new-batch (x30):	332 (95% in C ProjectForZ)\
	93s with 4 threads (3.6x speedup compared to single-threaded).\
	55.9s with 8 threads (5.9x speedup compared to single-threaded). \
\
Times on on my Macbook Pro:\
MATLAB 2020a (multithreaded, backprojection)	14s (same performance on 2019a)\
new (x1):	35.4 with 1 thread\
	11s with 4 threads (3.2x speedup compared to single-threaded)\
new-batch (x30):	178s with 1 thread\
	56s with 4 threads (3.2x speedup compared to single-threaded)\
\

\f1\b I should rerun the x1 numbers with caching of FH
\f0\b0  - I think that should beat Matlab significantly\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
For the near-focal-plane deconvolution problem I am currently working with, it takes 3.2s on my mac pro and 2.4s on beag-shuil [these times are outdated, but are with caching of F(H) and its transpose]. In both cases, ~78% of the time is spent in convolve. I suspect these are heavily memory-bound, since the speedup with 8 threads is only 6x (mac pro) and 4x (beag-shuil).\

\f2\i \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary benchmarks on various machines\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone Mac pro 2009 [8]		
\f3\fs22 \CocoaLigature0 [[55.92917013168335], [7.395716190338135, 7.230978965759277], [3.470026969909668, 3.47963285446167]]\

\f0\fs24 \CocoaLigature1 Macbook Pro 2019 [4] 
\f3\fs22 \cf2 \CocoaLigature0 	[[56.85784101486206], [11.341279029846191, 11.382513046264648], [3.6087489128112793, 3.3346760272979736, 3.2811148166656494]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=2400, min=2400, max=2400)
\f0\fs24 \cf0 \CocoaLigature1 \
new-brutha [24]
\f2\i  		
\f3\i0\fs22 \cf2 \CocoaLigature0 [[17.89953589439392], [3.4818685054779053, 3.828882932662964], [1.6241559982299805, 1.5127336978912354, 1.2638728618621826]]\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					Machine specs:
\f3\b0 \cf2  physical_cpus:24 logical_cpus:48 scpufreq(current=1762.0258124999993, min=1200.0, max=2500.0)\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \CocoaLigature1 suil-bheag [8]		
\f2\i 	
\f3\i0\fs22 \cf2 \CocoaLigature0 [[44.83419370651245], [10.16748595237732, 8.937126398086548], [2.464088201522827, 2.4436919689178467]]\
					
\f4\b Need to rerun to get GPU benchmarks
\f3\b0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					Machine specs:
\f3\b0 \cf2  physical_cpus:8 logical_cpus:16 scpufreq(current=1740.087375, min=1200.0, max=3000.0)\

\f4\b \cf3 					GPU specs:
\f3\b0 \cf2 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
					 1536 threads x 48 processors\
					 Clock speed 1.56GHz, mem speed 7.001GHz x 32B = 224.03GB/s, L2 4.19MB\
					 Total GPU RAM 16.90GB\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \CocoaLigature1 cuinneag [16]
\f2\i 			
\f3\i0\fs22 \cf2 \CocoaLigature0 [[13.396210432052612], [2.9962103366851807, 3.0246641635894775], [1.5529916286468506, 1.3653972148895264, 1.1758782863616943]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:16 logical_cpus:32 scpufreq(current=3200.0, min=0.0, max=0.0)\

\f0\fs24 \cf0 \CocoaLigature1 optic-tectum [4]			
\f3\fs22 \cf2 \CocoaLigature0 [[54.097476959228516], [9.557530164718628, 9.490315914154053], [3.5699703693389893, 3.5606672763824463, 3.5801541805267334], \
					 [5.8761820793151855], [1.5330743789672852, 1.4889581203460693], [1.5329170227050781, 1.5079047679901123, 1.507934331893921]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=1961.49075, min=1600.0, max=4100.0)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					GPU specs:
\f3\b0 \cf2 \
					 2048 threads x 20 processors\
					 Clock speed 1.7335GHz, mem speed 5.005GHz x 32B = 160.16GB/s, L2 2.10MB\
					 Total GPU RAM 8.51GB\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 Jose\'92s computer [8]
\f3 		
\fs22 [[26.19478750228882], [5.381789445877075, 5.477412462234497], [1.9234905242919922, 1.9264616966247559, 1.9024109840393066], 					 [4.183220148086548], [1.2685494422912598, 1.2638397216796875], [1.2534205913543701, 1.2854418754577637, 1.3035414218902588]]\
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:8 logical_cpus:32 scpufreq(current=2102.4765937499997, min=2200.0, max=3750.0)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					GPU specs:
\f3\b0 \cf2 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
					 2048 threads x 28 processors\
					 Clock speed 1.582GHz, mem speed 5.505GHz x 44B = 242.22GB/s, L2 2.88MB\
					 Total GPU RAM 11.70GB\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \CocoaLigature1 archer [4]	
\f3\fs22 \cf2 \CocoaLigature0 			[[37.759514808654785], [7.835170269012451, 7.826495170593262], [2.4770398139953613, 2.4861226081848145, 2.481673002243042], \
					 [2.896545171737671], [0.9577460289001465, 0.9572238922119141], [0.9543330669403076, 0.9559807777404785, 0.9498038291931152]]\
					
\f5\i Note that the GPU that was in here is now in cuinneag. Performance on cuinneag is marginally slower on these small testcases.\
					I presume that is because there are some CPU overheads
\f3\i0 \
					
\f4\b \cf3 Machine specs:
\f3\b0 \cf2  physical_cpus:4 logical_cpus:8 scpufreq(current=3.6692500000000003, min=800.0, max=4200.0)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f4\b \cf3 					GPU specs: 
\f5\i\b0 This is my new GPU when it was in Archer
\f3\i0 \cf2 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
					 1536 threads x 48 processors\
					 Clock speed 1.56GHz, mem speed 7.001GHz x 32B = 224.03GB/s, L2 4.19MB\
					 Total GPU RAM 16.90GB\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f2\i\fs24 \cf0 \CocoaLigature1 \
Note that the GPU implementation of the large-scale deconvolution is blazing fast compared to the CPU! 
\f0\i0 I may also be able to improve the speed of the small PIV case if I revisit that - when it\'92s only taking 2s there are probably significant python overheads.\
For now I\'92ve confirmed that x30 parallelism is sufficient on the GPU, but I should keep an eye on that if I make further improvements to the GPU code\
\
Interestingly, padding the convolution seems to have given no improvement (or degradation) whatsoever on the Mac Pro, for the x30 case. It does for x1. It looks like the FFTs are indeed faster (48s instead of 67s) but convolution is slower (360s instead of 348s) and there is 10s more rusage for the same run time.\
\
Jos\'e9\'92s performance notes:\
N=31: 170 min/frame on CPU, 9 min/frame on GPU\
N=21 25 min/frame on CPU.\

\f2\i \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\i0 \cf0 \ul Summary 
\f2\i (numbers may be outdated)
\f0\i0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - A single thread of my code is ~2.4x faster than the (parallel) MATLAB code, if I am allowed to process batch of 30 frames. With 4 threads it is 7.5x faster; with 8 threads it is 14x faster. It will be hard to speed the code up any more, but see comments below for possible things to try. I\'92m not quite sure why it scales better on my mac pro than on my macbook pro, but one possibility is that my parallelisation might be better than Matlab\'92s?\
- MATLAB doesn\'92t do particularly well on the GPU, and my code can do a batch of 24 in a similar time to what it takes MATLAB to do a single frame\
- My code is primarily focused on processing batches, but I shouldn\'92t forget that I am personally interested in deconvolving small numbers of planes from one or two images, for my PIV work. \
- There used to be two choices in py_light_field.cpp about what order we do the processing in (see comment where the Compare() function is used in py_light_field.cpp). One uses an enormous amount of RAM, the other risks mutex contention. I have added code that looks at how many threads we have, and the batch size, and picks a suitable ordering that should ensure no mutex contention. It looks like this is working, but should keep an eye on it.
\f1\b \

\f0\b0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Memory consumption\ulnone \
I have significantly reduced memory consumption on the GPU. By default we store all projected volumes on the CPU, and just move them on to the GPU one plane at a time, for processing. This is about 10% slower compared to setting projector.storeVolumesOnGPU, but means that the GPU can be applied to just about any task as long as there is a generous (~5x the batch volume size) amount of CPU RAM available. \
\
At the moment it doesn\'92t matter how long the overall time series is, although that might change if I wanted to save the output all into one big file instead of in separate files for each timepoint.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 Isn\'92t there a different formulation for the Richardson-Lucy that doesn\'92t require us to use Htf in each iteration!? 
\f1\b That would save on memory footprint.
\f0\b0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Things I should still investigate\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - 
\f1\b [Reminder to check separate notes about GPU performance, and GPU RAM consumption]\
\

\f0\b0 On macbook pro it is notably slower on forward projection compared to backprojection. 
\f1\b It might be worth investigating that briefly. Might it be mutex contention?\
\

\f0\b0 I have a slight lingering worry that my allocating of numpy arrays from multithreaded code may not be threadsafe. It doesn\'92t seem to complain about it even in debug mode, but it might be a problem. I should probably ask stackoverflow. Functions that do this are AllocateResultArray, AllocatePaddedMatrix, ConvolvePart4Mirror. But I might just need to call the function that confirms GIL held, before doing the allocation call.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f1\b \cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\f0\b0 \cf0 It would be good to have a \'93walkthrough\'94 mode that gives a running commentary about memory usage, and suggestions about things like batch processing that would speed up the processing.
\f1\b \

\f0\b0 \
I could consider looking into the efficiency of the scaling with number of threads. For a large problem size, I get almost 80% efficiency with as many as 16 threads on suil-bheag, but more like 70% with 8 threads on cuinneag (and I think about the same with 4 threads on my macbook). I presume the issue is memory bandwidth (e.g. on my 2cpu x 4core machine) and architecture. I did see a ~10% speedup by combining operations into CalculateRowBoth() to reduce memory bandwidth demands, but I reckon any further batching would be a lot of effort for marginal gain. For the combining operations, I eliminated 25% of what I think are the uncached reads, so the net 10% speedup is perhaps a bit disappointing, but not completely out of the ballpark I would have expected. Even for small problem sets, I would need to batch things up row-by-row or I would be overflowing caches - I don\'92t think it\'92s enough to process items one after the other on the same thread.\
\
- I tried changing the sequencing of my operations to prevent batching up of convolves. The new order took a tiny bit longer to run overall, but the parallelism was poor for small batch sizes due to mutex contention. In the absence of mutex contention, it runs slightly faster, but it doesn\'92t seem to be a huge change. Looking at histograms of block run times, removing the sorting doesn\'92t make a huge difference. In either case, things just generally take \'93a bit longer\'94 when multithreaded. It would be nice to be able to improve the thread scaling, but my suspicion is that we are simply limited by memory bandwidth, in which case multithreading is never going to have an ideal scaling.\
\
- For a full problem, I really am doing huge numbers of individual convolve operations. It will be very unwieldy to fuse multiple operations, but I reckon I might get a significant speedup if I process 2 or 4 timepoints simultaneously (which makes it easier to apply \ul vectorised\ulnone  multiplications), 
\f2\i and
\f0\i0  treat four fourier arrays simultaneously (less overall memory pressure on accum and on reading FH). I\'92m always optimistic with these things, but it seems to be like a >2x speedup should be within grasp, in principle\'85\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 It looks like I might be able to do asynchronous GPU transfers using https://docs.cupy.dev/en/stable/reference/generated/cupy.ndarray.html#cupy.ndarray.get. That might remove some of the transfer overhead when I store backprojections on the CPU. I had a bit of a look at this and didn\'92t seem to get a huge improvement, but I might have not been doing it properly.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Dead ends\ulnone \
- If I set fft planning mode to patient, it may be marginally faster. Certainly it seems to about 10% faster for the irfft, although there seems to be no significant change for ProjectForZ, which is by far the bottleneck. And patient planning takes about 5 minutes to set up! I wonder if the lack of overall improvement is because precise measurements are meaningless without all the other threads running in parallel and using up memory bandwidth? Certainly it seems to only deliver marginal gains, if any.\
\ul \
Miscellaneous notes\ulnone \
- Note that there is detailed examination of performance in deconvolution-performance-analysis.ipynb.\
- Hit a weird issue with implementation of complex operator* with LLVM which led to a factor of 2 slowdown in my entire projection code(!). Workaround added to py_light_field source code.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 - I have included support for caching F(H) in my C code. It\'92s impractical for large problems, but just about fits in memory with the small problem I am looking at for PIV.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 - I have completely dropped multithreading for fftw, and am doing the threading myself. Multithreaded fftw seems to be buggy (or I am somehow using it wrong) - takes way 
\f2\i longer
\f0\i0  with multiple threads, in a rather random way. The same is true when using the python wrapper for fftw, so I don\'92t think it\'92s just me.\
- cupy FFT does cache ~1GB of plan data. That\'92s ok unless we\'92re switching between job sizes\'a0a lot, in which case we need to clear the cache or we will run out of GPU RAM.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Low priority\ulnone \
- In principle I could teach fftw to work with higher prime values, including 19. The details are here http://www.fftw.org/fftw3_doc/Generating-your-own-code.html#Generating-your-own-code but it will involve a bit of playing around to see if it is possible.\
\
- I could amortise out the multipliers in the convolution, and apply them to the FFT array just once regardless of batch size. I would need to watch how I handle MirrorRow (would that require different multipliers?) and the multiplications only seem to take 5% of the total run time on my Macbook Pro, so it is probably not high priority.}