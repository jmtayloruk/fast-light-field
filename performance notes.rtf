{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww18780\viewh22220\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 Jun 2020 status\ulnone \
The entire projection operation is implemented in pure C code, with a lot of work put in to making the parallelism efficient.\
\
\ul Code run times\ulnone \
Command-line parameters are for lf_performance_analysis.py - 27 z planes). All times are on Mac Pro.\
\pard\tx7633\pardirnatural\partightenfactor0
\cf0 \
MATLAB (multithreaded, backprojection)	27s (using 4-500% CPU)\
\pard\tx7633\tx13337\pardirnatural\partightenfactor0
\cf0 profile-old: Single video frame, old code:	243.5s\
profile-new: Single video frame:	50.7s\
	8.8s with 8 threads (5.8x speedup)       [8.4 with patient planning - which takes 227s for the irfft!]\
profile-new-batch: Batch of 10 video frames:	145s\
profile-new-batch: Batch of 30 video frames:	339s (95% in C ProjectForZ)\
	96s with 4 threads (3.5x speedup).\
	59s with 8 threads (5.7x speedup). \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \
Note some timing anomalies with the old code:\
- It just uses fftconvolve, and evidently the underpinning implementation 
\f1\fs22 \CocoaLigature0 numpy.fft.fftpack_lite
\f0\fs24 \CocoaLigature1  (macbook air) is a 
\i lot
\i0  slower than the MKL implementation (mac pro). I shouldn\'92t read too much into the macbook air time for the old code.\
- I re-remembered that on my mac pro, the FFT backend is MKL, which runs multithreaded (and deadlocks with itself and with Parallel()) unless I disable MKL threading! I have now disabled it again.
\i \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\i0 \cf0 \ul Summary\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - My best-case scenario is now over 20x faster than the original naive code (single threaded). With 8 threads, it is ~125x faster!!\
- A single thread of my code is ~2.4x faster than the MATLAB code, if I am allowed to process batch of 30 frames. With threads is it almost 14x faster. It will be hard to speed the code up any more, but see comments below for possible things to try.\
- My code is primarily focused on processing batches, but I shouldn\'92t forget that I am personally interested in deconvolving small numbers of planes from one or two images, for my PIV work. \
- Note that there is detailed examination of performance in deconvolution-performance-analysis.ipynb.\
- Hit a weird issue with implementation of complex operator* with LLVM which led to a factor of 2 slowdown in my entire projection code(!). Workaround added to py_light_field source code.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Dead ends\ulnone \
- I considered caching F(H) arrays, but that uses a tremendous amount of memory. When I am batch-processing many video frames, F(H) is amortised away anyway, and caching does not fit particularly well with my new C code anyway.\
- If I set fft planning mode to patient, it may be marginally faster. Certainly it seems to about 10% faster for the irfft, although there seems to be no significant change for ProjectForZ, which is by far the bottleneck. And patient planning takes about 5 minutes to set up! I wonder if the lack of overall improvement is because precise measurements are meaningless without all the other threads running in parallel and using up memory bandwidth?\
\
\ul Things I should still investigate\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ulnone - Can I improve the speedup obtained from 8-way threading? I am getting 800% CPU load, but am only getting 5.7x speedup overall. I believe the issue is memory bandwidth (on my 2cpu x 4core machine). Basically, everything just takes a little bit longer. I did see a ~10% speedup by combining operations into CalculateRowBoth() to reduce memory bandwidth demands, but I reckon any further batching would be a lot of effort for marginal gain. For the combining operations, I eliminated 25% of what I think are the uncached reads, so the net 10% speedup is perhaps a bit disappointing, but not completely out of the ballpark I would have expected.\
- Why am I getting away without calling fftwf_init_threads when running as part of my jps module?? Shouldn\'92t I be calling that somewhere?\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0
\cf0 \ul Low priority\ulnone \
- I can probably speed up my ifft a bit by doing it in one go (it wastes a lot of time at the moment, and shouldn\'92t need to clear the whole array - we only need a little bit of zero-padding))\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 - Is fft performance impacted by the prime factors of the input array? (FFTs aren\'92t the main bottleneck, though, for large batch sizes)\ul \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx8232\pardirnatural\partightenfactor0

\b \cf0 \ulnone \
TODO
\b0 : check if any more work is required on the GPU implementation.}