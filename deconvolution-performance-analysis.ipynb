{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import tifffile\n",
    "import sys, time, os, csv, warnings\n",
    "import cProfile, pstats\n",
    "from jutils import tqdm_alias as tqdm\n",
    "\n",
    "import psfmatrix, lfimage\n",
    "import projector, lfdeconv\n",
    "import special_fftconvolve as special\n",
    "import jutils as util\n",
    "import py_light_field as plf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectorClass = projector.Projector_allC\n",
    "inputImage = lfimage.LoadLightFieldTiff('/Users/jonny/Movies/Nils files/Rectified/Left/Cam_Left_40_X1_N19.tif')\n",
    "hMatrix = psfmatrix.LoadMatrix('PSFmatrix/reducedPSFMatrix_M22.2NA0.5MLPitch125fml3125from-156to156zspacing4Nnum19lambda520n1.33.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Timing measurements\n",
    "    \n",
    "    # Number of timepoints to process\n",
    "    tileFactor = 1\n",
    "    # Number of z planes to process\n",
    "    planesToRun = range(0,1)\n",
    "\n",
    "    inputImageXN = np.tile(inputImage[np.newaxis,:,:],(tileFactor,1,1))\n",
    "    # Prime the cache first\n",
    "    Htf = lfdeconv.BackwardProjectACC(hMatrix, inputImageXN, planes=planesToRun, progress=tqdm, logPrint=False, projector=projectorClass())\n",
    "    # Now actually measure\n",
    "    Htf = lfdeconv.BackwardProjectACC(hMatrix, inputImageXN, planes=planesToRun, progress=tqdm, logPrint=False, projector=projectorClass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadThreadInfo(path):\n",
    "    rows = []\n",
    "    with open(path) as f:\n",
    "        cf = csv.reader(f, delimiter='\\t')\n",
    "        for row in cf:\n",
    "            rows.append(np.array(row).astype(np.double))\n",
    "    rows = np.array(rows)\n",
    "    t0 = rows[0,2]\n",
    "    dt = rows[:,3]-rows[:,2]\n",
    "    mt = (rows[:,5]-rows[:,4]) + (rows[:,7]-rows[:,6])\n",
    "    return (rows, t0, dt, mt)\n",
    "\n",
    "col = ['red', 'green', 'blue']\n",
    "lab = ['FFT', 'mirror', 'convolve', 'convolve (1st)', 'convolve (2nd)', 'convolve (mutex)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(rows, t0, dt, mt) = LoadThreadInfo('threads8b.txt')\n",
    "reportWaits = 10\n",
    "runTime = 0\n",
    "mutexTime = 0\n",
    "numThreads = int(np.max(rows[:,0]))+1\n",
    "(xStart, xEnd) = (29.2, 29.6)\n",
    "(xStart, xEnd) = (0, 10)\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.title('Thread breakdown')\n",
    "for w in tqdm([2,1,0]):#range(3)):\n",
    "    for t in tqdm(range(numThreads), leave=False):\n",
    "        x = []; y = []\n",
    "        mx = []; my = []\n",
    "        intermediateTimes = []\n",
    "        for r in tqdm(rows, leave=False):\n",
    "            if (r[0] == t) and (r[1] == w):\n",
    "                x.extend([r[2], r[2], r[3], r[3]])\n",
    "                y.extend([0,1,1,0])\n",
    "                runTime += r[3]-r[2]\n",
    "                if ((w == 2) and (r[3] >= xStart) and (r[2] <= xEnd)):\n",
    "                    if ((r[5]-r[4]) > 5e-6):\n",
    "                        if (reportWaits):\n",
    "                            print(\"Mutex wait: %le\"%(r[5]-r[4]))\n",
    "                            reportWaits = reportWaits - 1\n",
    "                        mx.extend([r[4], r[4], r[5], r[5]])\n",
    "                        my.extend([0,1,1,0])\n",
    "                    mutexTime += r[5]-r[4]\n",
    "                    if (r[6] != 0) and ((r[7]-r[6]) > 1e-5):\n",
    "                        if (reportWaits):\n",
    "                            print(\"Mutex wait: %le\"%(r[7]-r[6]))\n",
    "                            reportWaits = reportWaits - 1\n",
    "                        mx.extend([r[6], r[6], r[7], r[7]])\n",
    "                        my.extend([0,1,1,0])\n",
    "                    mutexTime += r[7]-r[6]\n",
    "                if (w == 0):\n",
    "                    intermediateTimes.extend([r[5], r[7]])\n",
    "\n",
    "                        \n",
    "        x = np.array(x) - t0\n",
    "        y = np.array(y)\n",
    "        mx = np.array(mx) - t0\n",
    "        my = np.array(my)\n",
    "        intermediateTimes = np.array(intermediateTimes) - t0\n",
    "        if t == 0:\n",
    "            thisLabel = lab[w]\n",
    "        else:\n",
    "            thisLabel = None\n",
    "        # Show the time spent working on a task.\n",
    "        # Note that this code is very slow for the convolve operation, just because there are\n",
    "        # massive numbers of separate blocks. I could fuse adjacent blocks where there is just\n",
    "        # a few µs of gap between them. But that would be getting distracted over code that\n",
    "        # I am only running occasionally, for diagnostic purposes!!\n",
    "        plt.fill_between(x, t, t+y/2, color=col[w], where=y.astype(np.bool), label=thisLabel)\n",
    "        # Show the time spent waiting to acquire an accum mutex.\n",
    "        plt.fill_between(mx, t+0.1, t+0.1+my*0.3, where=my.astype(np.bool), color='gray')\n",
    "        if (w == 0) and False:\n",
    "            plt.plot(intermediateTimes, [t+0.25]*len(intermediateTimes), 'x', color='black')\n",
    "if (reportWaits == 0):\n",
    "    warnings.warn('Did not report all waits')\n",
    "\n",
    "# Do a separate plot to examine what fraction of the time *somebody* is using an accumulator (useful for t=z=1)\n",
    "def PlotAccumulatorUse(tFactor):\n",
    "    for t in range(numThreads):\n",
    "        x = []; y = []\n",
    "        for r in rows:\n",
    "            if (r[0] == t) and (r[1] == 2):\n",
    "                if (r[6] != 0):\n",
    "                    x.extend([r[5], r[5], r[6], r[6], r[7], r[7], r[3], r[3]])\n",
    "                    y.extend([0,1,1,0,0,1,1,0])\n",
    "                else:\n",
    "                    x.extend([r[5], r[5], r[3], r[3]])\n",
    "                    y.extend([0,1,1,0])\n",
    "        x = np.array(x) - t0\n",
    "        y = np.array(y)\n",
    "        if t == 0:\n",
    "            thisLabel = 'accum'\n",
    "        else:\n",
    "            thisLabel = None\n",
    "        # Show the time spent holding any accumulator mutex\n",
    "        plt.fill_between(x, t*tFactor+0.25, t*tFactor+y/2, color='orange', where=y.astype(np.bool), label=thisLabel)\n",
    "        \n",
    "#PlotAccumulatorUse(1)\n",
    "plt.xlim(xStart, xEnd)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if False:\n",
    "    # Plot a separate figure showing how the accumulator is being used.\n",
    "    # This is really only informative in the 1z,1t case, since I do not currently distinguish between\n",
    "    # the different mutexes, and so the plot just shows whether *any* mutex is held at any given time\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.title('Accumulator mutex')\n",
    "    PlotAccumulatorUse(0)\n",
    "    plt.show()\n",
    "\n",
    "wallclockRunTime = np.max(rows[:,3]) - np.min(rows[:,2])\n",
    "convolveTime = (np.sum(dt[rows[:,1]==2]-mt[rows[:,1]==2]))\n",
    "dta = rows[:,3]-rows[:,4]   # Only the accumulator stage of the convolution (including any mutex blocking time)\n",
    "convolveAccumTime = (np.sum(dta[rows[:,1]==2]-mt[rows[:,1]==2])) # Time spent actively working on the accumulator\n",
    "print('Wall %.2f, run %.2f, active %.2f, mutex %.2f, idle frac %.2f' % (wallclockRunTime, runTime, runTime-mutexTime, mutexTime, mutexTime/runTime))\n",
    "# Report total CPU load (*not* actual speedup compared to single-threaded),\n",
    "# and also report the average number of CPUs that are working on convolution operations\n",
    "print('Parallelism %.1f (of which convolve: %.2f of which accum: %.2f)' % ((runTime-mutexTime)/runTime*numThreads, convolveTime/wallclockRunTime, convolveAccumTime/wallclockRunTime))\n",
    "print('Time breakdown: FH %.2f, mirror %.2f, convolve %.2f, convolve/mutex %.2f' \n",
    "          % ((np.sum(dt[rows[:,1]==0])), (np.sum(dt[rows[:,1]==1])),\n",
    "             convolveTime, (np.sum(mt[rows[:,1]==2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore how run times vary as a function of number of threads\n",
    "- FFT simply takes consistently a little longer with 8 threads. \n",
    "- Mirror has some interesting patterns, but it takes negligible time anyway\n",
    "- Convolve also has interesting patterns, but it looks like there's a dominant trend underneath all that. I think the most important point is that there is clearly a minimum run time, and both 1 and 8 threads cases *can* achieve that; either *might* take longer than that, but the 8 thread case is more likely to. I imagine this must be due either to cache residency or memory bandwidth contention. I would guess cache residency isn't such a big deal, simply because I think the work sizes are already larger than the caches.\n",
    "\n",
    "It could possibly be an alignment issue I suppose, but memory bandwidth certainly seems plausible. If that is indeed the case, I suppose there isn't much I can do about it. For the most part, the bandwidth is unavoidable, though I did achieve a ~10% speedup by chunking together two convolves (the ones with and without x-mirroring) to reduce the read bandwidth slightly. I reckon anything further along those lines would be complicating the code quite a bit, without much prospect of getting a significant improvement.\n",
    "\n",
    "Important observation: the two-thread case has pretty much exactly double performance of the one-thread case. I reckon that supports my theory about memory bandwidth, or at least something related to the number of actual physical CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rows1, t01, dt1, mt1) = LoadThreadInfo('threads1.txt')\n",
    "(rows8, t08, dt8, mt8) = LoadThreadInfo('threads8.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ranges = [[0, 0.05], [0, 0.01], [0, 0.015], [0, 0.002], [0, 0.015], [0, 5e-5]]\n",
    "for _workType in range(6):\n",
    "    workType = np.minimum(_workType, 2)\n",
    "    if (_workType == 3):\n",
    "        # Convolve first part (up to the first mutex acquisition)\n",
    "        selector = rows1[:,1]==workType\n",
    "        initialTime8 = rows8[:,4]-rows8[:,2]\n",
    "        _dt8 = initialTime8[selector]\n",
    "        initialTime1 = rows1[:,4]-rows1[:,2]\n",
    "        _dt1 = initialTime1[selector]\n",
    "        problem = _dt1 > 1e9\n",
    "    elif (_workType == 4):\n",
    "        # Convolve second part (excluding mutex acquisitions)\n",
    "        # First consider case with two mutex acquisitions [which is actually obsolete now]\n",
    "        selector = (rows8[:,1]==workType) & (rows8[:,6]!=0)\n",
    "        initialTime8 = rows8[:,6]-rows8[:,5] + rows8[:,3]-rows8[:,7]\n",
    "        _dt8 = initialTime8[selector]\n",
    "        selector = (rows1[:,1]==workType) & (rows1[:,6]!=0)\n",
    "        initialTime1 = rows1[:,6]-rows1[:,5] + rows1[:,3]-rows1[:,7]\n",
    "        _dt1 = initialTime1[selector]\n",
    "        # Now consider case with one mutex acquisition\n",
    "        # Note that this graph won't make much sense if we compare new code with old, \n",
    "        # since the work has been reshuffled a bit\n",
    "        selector = (rows8[:,1]==workType) & (rows8[:,6]==0)\n",
    "        initialTime8 = rows8[:,3]-rows8[:,5]\n",
    "        _dt8 = np.append(_dt8, initialTime8[selector])\n",
    "        selector = (rows1[:,1]==workType) & (rows1[:,6]==0)\n",
    "        initialTime1 = rows1[:,3]-rows1[:,5]\n",
    "        _dt1 = np.append(_dt1, initialTime1[selector])\n",
    "    elif (_workType == 5):\n",
    "        # Convolve mutex time\n",
    "        selector = (rows1[:,1]==workType)\n",
    "        _dt8 = mt8[selector]\n",
    "        _dt1 = mt1[selector]\n",
    "    else:\n",
    "        selector = rows1[:,1]==workType\n",
    "        _dt8 = dt8[selector]\n",
    "        _dt1 = dt1[selector]\n",
    "    av8 = np.average(_dt8)\n",
    "    av1 = np.average(_dt1)\n",
    "    gradient = av8 / av1\n",
    "    # Clip outliers\n",
    "    _dt1 = np.minimum(_dt1, ranges[_workType][1])\n",
    "    _dt8 = np.minimum(_dt8, gradient*ranges[_workType][1])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.title(lab[_workType])\n",
    "    plt.xlabel('1 thread')\n",
    "    plt.ylabel('8 threads')\n",
    "    plt.plot(_dt1, _dt8, '.')\n",
    "    plt.plot(ranges[_workType], ranges[_workType])\n",
    "    plt.plot(ranges[_workType], np.array(ranges[_workType])*gradient)\n",
    "    plt.plot([av1, av1, 0], [0, av8, av8])\n",
    "    plt.show()\n",
    "    print(lab[_workType], np.sum(_dt1), np.sum(_dt8), gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These timings are old ones before I speeded up the convolution code a little bit.\n",
    "As a result, these specific numbers are outdated, but I reckon the general theme remains, and I am not going to rerun them all right now...\n",
    "\n",
    "## With one timepoint\n",
    "\n",
    "### 1 z plane:\n",
    "Wall 2.44, run 19.32, active 12.59, mutex 6.73, idle frac 0.35\n",
    "Parallelism 5.2 (Convolve: 1.03, Accum: 0.97)\n",
    "\n",
    "Time breakdown: FH 9.27, mirror 0.81, convolve 2.51, convolve/mutex 6.73\n",
    "### 2 z planes:\n",
    "Wall 3.68, run 28.93, active 26.25, mutex 2.68, idle frac 0.09\n",
    "Parallelism 7.3 (Convolve: 1.44, Accum: 1.37)\n",
    "Time breakdown: FH 19.01, mirror 1.92, convolve 5.31, convolve/mutex 2.68\n",
    "\n",
    "### 4 z planes:\n",
    "Run 49.41, active 49.11, mutex 0.30, idle frac 0.01\n",
    "Parallelism 8.0\n",
    "\n",
    "Time breakdown: FH 34.83, mirror 4.01, convolve 10.27, convolve/mutex 0.30\n",
    "### Next 4 z planes:\n",
    "Run 49.40, active 49.13, mutex 0.27, idle frac 0.01\n",
    "Parallelism 8.0\n",
    "### 8 z planes:\n",
    "Run 108.06, active 108.06, mutex 0.00, idle frac 0.00\n",
    "Parallelism 8.0\n",
    "\n",
    "## With two timepoints\n",
    "\n",
    "### 1 z plane:\n",
    "Wall 2.90, run 23.01, active 15.99, mutex 7.02, idle frac 0.31\n",
    "Parallelism 5.6 (Convolve: 1.67, Accum: 1.59)\n",
    "Time breakdown: FH 10.18, mirror 0.96, convolve 4.85, convolve/mutex 7.02\n",
    "\n",
    "### 2 z planes:\n",
    "Run 30.14, active 28.15, mutex 2.00, idle frac 0.07\n",
    "Parallelism 7.5\n",
    "\n",
    "Time breakdown: FH 15.84, mirror 2.37, convolve 9.94, convolve/mutex 2.00\n",
    "\n",
    "### 3 z planes:\n",
    "Run 35.87, active 34.34, mutex 1.53, idle frac 0.04\n",
    "Parallelism 7.7\n",
    "### 4 z planes:\n",
    "Run 64.77, active 64.75, mutex 0.02, idle frac 0.00\n",
    "Parallelism 8.0 (Convolve: 2.80)\n",
    "\n",
    "Time breakdown: FH 37.32, mirror 4.55, convolve 22.87, convolve/mutex 0.02\n",
    "\n",
    "## With 16 timepoints\n",
    "### 1 z plane\n",
    "Run 51.78, active 51.78, mutex 0.01, idle frac 0.00\n",
    "Parallelism 8.0 (Convolve: 6.51)\n",
    "\n",
    "Time breakdown: FH 8.25, mirror 1.36, convolve 42.16, convolve/mutex 0.01\n",
    "\n",
    "## With 32 timepoints\n",
    "### 1 z plane\n",
    "Run 87.08, active 87.07, mutex 0.01, idle frac 0.00\n",
    "Parallelism 8.0 (Convolve: 7.11)\n",
    "\n",
    "Time breakdown: FH 8.36, mirror 1.26, convolve 77.46, convolve/mutex 0.01\n",
    "\n",
    "## Summary\n",
    "More time is being lost waiting for a mutex for 1z,2t compared to 2z,1t. This might just be because there is less FH work to be getting on with, and so there are more threads ready to do work.\n",
    "\n",
    "In fact, with 1z,1t, while threads are inevitably sitting idle I am actually making pretty good use of time - the accumulator mutex is held by somebody almost all the time. However, we are being less efficient with the 2z,t1 and 1z,2t cases. There, we often only hold one of the mutexes at a time. It might be possible to adjust the scheduling code to be more effective with the mutexes (which I think in practice means getting a bit ahead of ourselves with FH rather than waiting until we run out of work entirely). An alternative strategy would be to create additional temporary accumulators and combine them at the end, to support more parallelism. Some perspective though: I do care about the 1z,2t case, but I could only speed it up by 33% (respectable but not earth-shattering); the 4z,2t case is probably more representative of Nils' data, and that has no idle time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
