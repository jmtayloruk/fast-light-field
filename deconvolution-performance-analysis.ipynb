{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance analysis for CPU-based deconvolution code\n",
    "Set plf.SetThreadFileName(\"threads.txt\") or similar to dump information about how long each work chunk is taking to run, in the multithreaded C code. This notebook examines the information in that file and compares it between runs (e.g. when running with different numbers of threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import tifffile\n",
    "import sys, time, os, csv, warnings, glob\n",
    "import cProfile, pstats\n",
    "from jutils import tqdm_alias as tqdm\n",
    "\n",
    "import psfmatrix, lfimage\n",
    "import projector, lfdeconv\n",
    "import special_fftconvolve as special\n",
    "import jutils as util\n",
    "import py_light_field as plf\n",
    "import lf_performance_analysis as perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadThreadInfo(path):\n",
    "    # Load a file (e.g. \"threads.txt\") saved after a previous projection operation\n",
    "    rows = []\n",
    "    with open(path) as f:\n",
    "        cf = csv.reader(f, delimiter='\\t')\n",
    "        for row in cf:\n",
    "            rows.append(np.array(row).astype(np.double))\n",
    "    rows = np.array(rows)\n",
    "    tStart = rows[0,2]\n",
    "    tEnd = np.max(rows[:,3])\n",
    "    dt = rows[:,3]-rows[:,2]\n",
    "    mt = (rows[:,5]-rows[:,4]) + (rows[:,7]-rows[:,6])\n",
    "    return (rows, tStart, tEnd, dt, mt)\n",
    "\n",
    "col = ['red', 'yellow', 'green', 'blue']\n",
    "lab = ['FFT', 'transpose', 'mirror', 'convolve', 'convolve (1st)', 'convolve (2nd)', 'convolve (mutex)']\n",
    "numWorkTypes = 4\n",
    "workNames = { 0:'fft', 1:'transpose', 2:'mirror', 3:'convolve' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory bandwidth benchmarks\n",
    "### Information on my mac pro\n",
    "RAM is 1066 MHz DDR3, which according to wikipedia should give 68GB/s(!). But reported bandwidths for the mac pro seems to be 2GB/sec/core, so I am not sure what the real limiting factor is here, but the reported bandwidth seems more plausible!\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Mac_Pro#Memory says up to 16GB/s total for early mac pros.\n",
    "\n",
    "Interestingly though, https://macperformanceguide.com/MacPro2019-MemoryBandwidth.html quotes 2.5GB/s/core for the 2019 mac pro, which is not actually that much faster.\n",
    "\n",
    "### My performance results\n",
    "- My read performance seems pretty close to that quoted 2GB/sec/core.\n",
    "- My read performance falls with number of threads when I have two independent reads going on simultaneously - but for a single thread it seems to be almost unchanged from single-read performance. How on earth can I explain all this!?\n",
    "- My write performance is perplexingly *high* (but falling with number of threads). I can't explain that. I suppose it could be something like the bandwidth limitation is between the L3 cache (per processor) and the RAM? The speed with 8 threads is 4.4x slower than with 1 thread, and sits a bit below the implied maximum bandwidth.\n",
    "- Single-thread read-write is *faster* than single-thread read! That makes no sense! Unless, I suppose, the arithmetic is causing me a problem somehow (messing up the pipeline, perhaps??). **Should think about this one**\n",
    "- In-place increment is even faster (which feels plausible compared to single-thread read-write, even if I can't put my finger on exactly what this would change). It is close to write-only performance, which perhaps makes sense if it's limited by the same factor ultimately.\n",
    "- It is utterly bizarre that single-thread performance for CalculateRow is actually **higher** than single-thread read performance!!\n",
    "- I don't understand why calculate-row performance falls with number of threads. It falls by a factor of about 2 (from 1->8 threads). **I should think more** about why that could be. It seems as if this might be the underlying explanation for why I am not getting a greater speed boost overall from multithreading. \n",
    "- CalculateRow is two reads and an increment. \n",
    "\n",
    "Remember that, even if I don't understand exactly what is going on, I can still benchmark performance for various \"batched\" implementations of CalculateRow and observe empirically how their performance varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeHistory = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run memory benchmarks\n",
    "bmName = ['read', '2xread', 'write', 'read/write', 'increment', 'calc-row', 'calc-row-dummy', 'calc-row-2']\n",
    "threadsToUse = [1, 2, 4, 6, 8]\n",
    "\n",
    "for numThreads in threadsToUse:\n",
    "    # Sizes <1e5 elements have too much overhead (thread setup etc?), and the numbers don't make sense.\n",
    "    problemSizes = np.array([1e5, 1e6, 5e6, 1e7, 2e7, 4e7, 6e7])\n",
    "    #problemSizes = np.array([1e7, 2e7])\n",
    "    #np.array([1e4, 1e5, 2e5, 5e5, 1e6, 5e6, 1e7, 2e7])\n",
    "\n",
    "    bms = np.arange(len(bmName))\n",
    "    times = np.zeros((problemSizes.shape[0], bms.shape[0]))\n",
    "    t0 = time.time()\n",
    "    for p in range(len(problemSizes)):\n",
    "        problemSize = problemSizes[p]\n",
    "        dts = plf.MemoryBenchmark(numThreads, int(problemSize))\n",
    "        times[p] = np.array(dts)\n",
    "    print('For info: calculating benchmarks took %.1fs (x%d)' % (time.time()-t0, numThreads))\n",
    "    timeHistory[numThreads] = times.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (1 and 8 threads) for calculate-row-dummy benchmark, based on measurements for simpler benchmarks.\n",
    "# The 8-thread prediction from the dual-read benchmark is actually close to reality, \n",
    "#  although the 1-thread prediction is half the reality!\n",
    "# That seems to come back to the issue where a single thread is over-performing as far as\n",
    "#  the increment (and dual-read) benchmarks go.\n",
    "\n",
    "# Naive predictions from 2x single-read + 1x increment\n",
    "print(1/(2/1800+1/4500))\n",
    "print(1/(2/1800+1/1400))\n",
    "# Predictions from dual-read + 1x increment\n",
    "print(1/(1/1800+1/4500))\n",
    "print(1/(1/1250+1/1400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for b in range(len(bms)):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.title(bmName[b])\n",
    "    for t in threadsToUse:\n",
    "        _times = timeHistory[t]\n",
    "        bm = bms[b]\n",
    "        elementSize = 8\n",
    "        mElementsProcessed = problemSizes*elementSize/1e6\n",
    "        mElementsPerSec = mElementsProcessed / _times[:,b]\n",
    "        plt.plot(problemSizes/1e6, mElementsPerSec, label='%s[x%d]'%(bmName[bm], t))\n",
    "    plt.ylabel('MB/sec/thread')\n",
    "    plt.xlabel('Melements')\n",
    "    plt.ylim(0,None)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectorClass = projector.Projector_allC\n",
    "if False:\n",
    "    inputImage = lfimage.LoadLightFieldTiff('/Users/jonny/Movies/Nils files/Rectified/Left/Cam_Left_40_X1_N19.tif')\n",
    "    hMatrix = psfmatrix.LoadMatrix('PSFmatrix/reducedPSFMatrix_M22.2NA0.5MLPitch125fml3125from-156to156zspacing4Nnum19lambda520n1.33.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Timing measurements\n",
    "if False:\n",
    "    inputImage = np.zeros((19*19,19*19), dtype='float32')\n",
    "    for numJobs in [8, 4, 2, 1]:\n",
    "        # Run the test, saving thread performance information\n",
    "        plf.SetThreadFileName(\"threads_new_%d.txt\" % numJobs)\n",
    "        perf.main(['profile-prime-cache', 'profile-new-batch'],\n",
    "                  inputImage=None, numJobs=numJobs, printProfileOutput=False)\n",
    "    plf.SetThreadFileName(\"\")\n",
    "    \n",
    "if True:\n",
    "    inputImage = np.zeros((19*19,19*19), dtype='float32')\n",
    "    for numJobs in [8, 4, 2, 1]:\n",
    "        # Run the test, saving thread performance information\n",
    "        plf.SetThreadFileName(\"threads_square_%d.txt\" % numJobs)\n",
    "        perf.main(['profile-prime-cache', 'profile-new-batch'],\n",
    "                  matPath='PSFmatrix/fdnormPSFMatrix_M22.2NA0.5MLPitch125fml3125from-56to56zspacing4Nnum19lambda520n1.33.mat',\n",
    "                  inputImage=inputImage, batchSize=2, numJobs=numJobs, printProfileOutput=False)\n",
    "    plf.SetThreadFileName(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual plots of what the activity of each thread looks like over time\n",
    "Note that for large operations, if zoomed out, we will not get a clear impression because (I think) the plot over-draws subsequent work types with a minimum width of one pixel. So, for example, the time on FFT appears to be exaggerated because that is drawn last.\n",
    "\n",
    "Note also that this code is not very well structured - I should separate the parsing from the plotting to save re-parsing when e.g. I just want to change the range of the plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(rows, t0, tEnd, dt, mt) = LoadThreadInfo('threads_square_8_copy.txt')\n",
    "(rows, t0, tEnd, dt, mt) = LoadThreadInfo('threads_square_8_neworder.txt')\n",
    "#(rows, t0, tEnd, dt, mt) = LoadThreadInfo('threads_square_8_oldorder.txt')\n",
    "\n",
    "#(rows, t0, tEnd, dt, mt) = LoadThreadInfo('brutha-benchmarks-2/threads_square_16.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numThreads = int(np.max(rows[:,0]))+1\n",
    "\n",
    "def ParseThreadInfo(xStart=None, xEnd=None, monitorMutexes=True, minMutexWaitTime=5e-6):\n",
    "    # minMutexWaitTime:            Do not bother displaying mutex wait times less than this (which is effectively an instant return)\n",
    "\n",
    "    runTime = 0\n",
    "    mutexTime = 0\n",
    "    __intermediateTimes = []\n",
    "    __boundaryTimes = []\n",
    "    __x = []; __y = []\n",
    "    __mx = []; __my = []\n",
    "    reportWaits = 10 # Print info about the first N mutex wait intervals\n",
    "    if (xStart is None):\n",
    "        xStart = 0\n",
    "    if (xEnd is None):\n",
    "        xEnd = 1e100\n",
    "\n",
    "    for w in tqdm(range(numWorkTypes)):\n",
    "        workName = workNames[w]\n",
    "        _intermediateTimes = []\n",
    "        _boundaryTimes = []\n",
    "        _x = []; _y = []\n",
    "        _mx = []; _my = []\n",
    "\n",
    "        for t in tqdm(range(numThreads), leave=False):\n",
    "            x = []; y = []\n",
    "            mx = []; my = []\n",
    "            intermediateTimes = []\n",
    "            boundaryTimes = []\n",
    "            for r in tqdm(rows[:], leave=False):\n",
    "                if (r[0] == t) and (r[1] == w):\n",
    "                    x.extend([r[2], r[2], r[3], r[3]])\n",
    "                    y.extend([0,1,1,0])\n",
    "                    runTime += r[3]-r[2]\n",
    "                    if monitorMutexes:\n",
    "                        if ((workName == 'convolve') and (r[3] >= xStart) and (r[2] <= xEnd)):\n",
    "                            if ((r[5]-r[4]) > minMutexWaitTime):\n",
    "                                if (reportWaits):\n",
    "                                    print(\"Mutex wait: %le\"%(r[5]-r[4]))\n",
    "                                    reportWaits = reportWaits - 1\n",
    "                                mx.extend([r[4], r[4], r[5], r[5]])\n",
    "                                my.extend([0,1,1,0])\n",
    "                            mutexTime += r[5]-r[4]\n",
    "                            if (r[6] != 0) and ((r[7]-r[6]) > minMutexWaitTime):\n",
    "                                if (reportWaits):\n",
    "                                    print(\"Mutex wait: %le\"%(r[7]-r[6]))\n",
    "                                    reportWaits = reportWaits - 1\n",
    "                                mx.extend([r[6], r[6], r[7], r[7]])\n",
    "                                my.extend([0,1,1,0])\n",
    "                            mutexTime += r[7]-r[6]\n",
    "                    if (workName == 'fft'):\n",
    "                        intermediateTimes.extend([r[5], r[7]])\n",
    "                    elif (workName == 'convolve'):\n",
    "                        intermediateTimes.append(r[5])\n",
    "\n",
    "                    boundaryTimes.append([r[3]])\n",
    "            _intermediateTimes.append(np.array(intermediateTimes)-t0)\n",
    "            _boundaryTimes.append(np.array(boundaryTimes)-t0)\n",
    "            _x.append(np.array(x) - t0)\n",
    "            _y.append(np.array(y))\n",
    "            _mx.append(np.array(mx) - t0)\n",
    "            _my.append(np.array(my))\n",
    "        __intermediateTimes.append(_intermediateTimes)\n",
    "        __boundaryTimes.append(_boundaryTimes)\n",
    "        __x.append(_x)\n",
    "        __y.append(_y)\n",
    "        __mx.append(_mx)\n",
    "        __my.append(_my)\n",
    "    if (reportWaits == 0):\n",
    "        warnings.warn('Did not report all waits')\n",
    "    return (__x, __y, __mx, __my, __intermediateTimes, __boundaryTimes, runTime, mutexTime)\n",
    "\n",
    "def PlotWorkThread(w, t, x, y, mx, my, intermediateTimes, boundaryTimes, \n",
    "                   monitorMutexes=True,\n",
    "                   plotBoundaries=True,\n",
    "                   plotIntermediateCheckpoints=True):\n",
    "    # monitorMutexes:              Grey block to mark time spent waiting to acquire an accum mutex\n",
    "    # plotBoundaries:              Black 'x' to mark boundaries between individual operations\n",
    "    # plotIntermediateCheckpoints: Grey 'x' to mark intermediate checkpoints within individual operations\n",
    "\n",
    "    if t == 0:\n",
    "        thisLabel = lab[w]\n",
    "    else:\n",
    "        thisLabel = None\n",
    "    # Show the time spent working on a task.\n",
    "    # Note that this code is very slow for the convolve operation, just because there are\n",
    "    # massive numbers of separate blocks. I could fuse adjacent blocks where there is just\n",
    "    # a few µs of gap between them. But that would be getting distracted over code that\n",
    "    # I am only running occasionally, for diagnostic purposes!!\n",
    "    plt.fill_between(x, t, t+y/2, color=col[w], where=y.astype(np.bool), label=thisLabel)\n",
    "    if monitorMutexes:\n",
    "        # Show the time spent waiting to acquire an accum mutex.\n",
    "        plt.fill_between(mx, t+0.1, t+0.1+my*0.3, where=my.astype(np.bool), color='gray')\n",
    "    if (len(intermediateTimes) > 0) and plotIntermediateCheckpoints:\n",
    "        plt.plot(intermediateTimes, [t+0.25]*len(intermediateTimes), '|', color='brown')\n",
    "    if plotBoundaries:\n",
    "        plt.plot(boundaryTimes, [t+0.25]*len(boundaryTimes), '|', color='black')\n",
    "\n",
    "(x, y, mx, my, intermediateTimes, boundaryTimes, runTime, mutexTime) = ParseThreadInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wallclockRunTime = np.max(rows[:,3]) - np.min(rows[:,2])\n",
    "(xStart, xEnd) = (0, wallclockRunTime) # Full range of data\n",
    "\n",
    "def PlotForTimeRange(xStart, xEnd,                    \n",
    "                     monitorMutexes=True,\n",
    "                     plotBoundaries=True,\n",
    "                     plotIntermediateCheckpoints=True,\n",
    "                     minMutexWaitTime=1e-5):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.xlim(xStart, xEnd)\n",
    "    plt.title('Thread breakdown')\n",
    "    for w in tqdm(range(numWorkTypes-1,-1,-1)):  # (tqdm does not seem to like the 'reversed' syntax, so I do it this way!)\n",
    "        workName = workNames[w]\n",
    "        for t in tqdm(range(numThreads), leave=False):\n",
    "            PlotWorkThread(w, t, x[w][t], y[w][t], mx[w][t], my[w][t], intermediateTimes[w][t], boundaryTimes[w][t],\n",
    "                           monitorMutexes, plotBoundaries, plotIntermediateCheckpoints)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if True:\n",
    "    # Overview of the whole run\n",
    "    PlotForTimeRange(xStart, xEnd, \n",
    "                     monitorMutexes=False, plotBoundaries=False, plotIntermediateCheckpoints=False)\n",
    "if True:\n",
    "    # Examine the run in more zoomed-in detail\n",
    "    plotRange = 0.5\n",
    "    for xStart in np.arange(0,wallclockRunTime,plotRange):\n",
    "        PlotForTimeRange(xStart, xStart+plotRange, plotIntermediateCheckpoints=False)\n",
    "\n",
    "cRows = rows[:,1] == numWorkTypes-1\n",
    "convolveTime = (np.sum(dt[cRows]-mt[cRows]))\n",
    "dta = rows[:,3]-rows[:,4]   # Only the accumulator stage of the convolution (including any mutex blocking time)\n",
    "convolveAccumTime = (np.sum(dta[cRows]-mt[cRows])) # Time spent actively working on the accumulator\n",
    "print('Wall %.2f, run %.2f, active %.2f, mutex %.2f, idle frac %.2f' % (wallclockRunTime, runTime, runTime-mutexTime, mutexTime, mutexTime/runTime))\n",
    "# Report total CPU load (*not* actual speedup compared to single-threaded),\n",
    "# and also report the average number of CPUs that are working on convolution operations\n",
    "print('Parallelism %.1f (of which convolve: %.2f of which accum: %.2f)' % ((runTime-mutexTime)/runTime*numThreads, convolveTime/wallclockRunTime, convolveAccumTime/wallclockRunTime))\n",
    "print('Time breakdown:')\n",
    "for i in range(numWorkTypes-1):\n",
    "    print(' %s %.2f [%d]' % (lab[i], np.sum(dt[rows[:,1]==i]), np.sum(rows[:,1]==i)))\n",
    "print(' convolve %.2f [%d]' % (convolveTime, np.sum(rows[:,1]==i)))\n",
    "print(' convolve/mutex %.2f' % (np.sum(mt[cRows])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Temporary code looking at the early stuff in more detail\n",
    "PlotForTimeRange(0, 0.1)\n",
    "for xStart in np.arange(0,4,0.4):\n",
    "    PlotForTimeRange(xStart, xStart+0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotAccumulatorUse(tFactor):\n",
    "    # Plot to examine what fraction of the time *somebody* is using an accumulator \n",
    "    # (useful for t=z=1)\n",
    "\n",
    "    for t in range(numThreads):\n",
    "        x = []; y = []\n",
    "        for r in rows:\n",
    "            if (r[0] == t) and (r[1] == 2):\n",
    "                if (r[6] != 0):\n",
    "                    x.extend([r[5], r[5], r[6], r[6], r[7], r[7], r[3], r[3]])\n",
    "                    y.extend([0,1,1,0,0,1,1,0])\n",
    "                else:\n",
    "                    x.extend([r[5], r[5], r[3], r[3]])\n",
    "                    y.extend([0,1,1,0])\n",
    "        x = np.array(x) - t0\n",
    "        y = np.array(y)\n",
    "        if t == 0:\n",
    "            thisLabel = 'accum'\n",
    "        else:\n",
    "            thisLabel = None\n",
    "        # Show the time spent holding any accumulator mutex\n",
    "        plt.fill_between(x, t*tFactor+0.25, t*tFactor+y/2, color='orange', where=y.astype(np.bool), label=thisLabel)\n",
    "        \n",
    "if False:\n",
    "    # Plot a separate figure showing how the accumulator is being used.\n",
    "    # This is really only informative in the 1z,1t case, since I do not currently distinguish between\n",
    "    # the different mutexes, and so the plot just shows whether *any* mutex is held at any given time\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.title('Accumulator mutex')\n",
    "    PlotAccumulatorUse(0)\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore how run times vary as a function of number of threads\n",
    "- FFT simply takes consistently a little longer with 8 threads. \n",
    "- Mirror has some interesting patterns, but it takes negligible time anyway\n",
    "- Convolve also has interesting patterns, but it looks like there's a dominant trend underneath all that. I think the most important point is that there is clearly a minimum run time, and both 1 and 8 threads cases *can* achieve that; either *might* take longer than that, but the 8 thread case is more likely to. I imagine this must be due either to cache residency or memory bandwidth contention. I would guess cache residency isn't such a big deal, simply because I think the work sizes are already larger than the caches.\n",
    "\n",
    "It could possibly be an alignment issue I suppose, but memory bandwidth certainly seems plausible. If that is indeed the case, I suppose there isn't much I can do about it. For the most part, the bandwidth is unavoidable, though I did achieve a ~10% speedup by chunking together two convolves (the ones with and without x-mirroring) to reduce the read bandwidth slightly. I reckon anything further along those lines would be complicating the code quite a bit, without much prospect of getting a significant improvement.\n",
    "\n",
    "Important observation: the two-thread case has pretty much exactly double performance of the one-thread case. I reckon that supports my theory about memory bandwidth, or at least something related to the number of actual physical CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetScaling(filenames):\n",
    "    runTimes = []\n",
    "    threadNumbers = []\n",
    "    for f in filenames:\n",
    "        (rows, tStart, tEnd, dt, mt) = LoadThreadInfo(f)\n",
    "        runTimes.append(tEnd-tStart)\n",
    "        threadNumbers.append(np.max(rows[:,0]+1))\n",
    "    return (np.array(threadNumbers), np.array(runTimes))\n",
    "    \n",
    "(threadNumbers, runTimes) = GetScaling(glob.glob(\"brutha-benchmarks-2/threads_square_*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(threadNumbers, np.array(runTimes)*threadNumbers, 'x')\n",
    "plt.ylim(0,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileX = 'threads_square_1.txt'\n",
    "fileY = 'threads_square_8.txt'\n",
    "(rowsX, t0X, dtX, mtX) = LoadThreadInfo(fileX)\n",
    "(rowsY, t0Y, dtY, mtY) = LoadThreadInfo(fileY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check we are comparing two equivalent runs.\n",
    "# This code assumes that both fileX and fileY are runs on the same problem\n",
    "# (but perhaps e.g. using different numbers of threads).\n",
    "assert(rowsX.shape == rowsY.shape)\n",
    "# Watch out for files from old code, which I do not support here any more\n",
    "if (np.max(rowsX[:,1]) == 2):\n",
    "    warnings.warn('This looks like an old file (only 3 work types). This code will not work correctly.')\n",
    "numWorkTypes = 4\n",
    "\n",
    "ranges = [[0, 0.05], [0, 0.01], [0, 0.01], [0, 0.015], [0, 0.002], [0, 0.015], [0, 5e-5]]\n",
    "for _workType in range(numWorkTypes+3):\n",
    "    workType = np.minimum(_workType, numWorkTypes-1)\n",
    "    if (_workType == numWorkTypes):\n",
    "        # Convolve first part (up to the first mutex acquisition)\n",
    "        selector = rowsX[:,1]==workType\n",
    "        initialTimeY = rowsY[:,4]-rowsY[:,2]\n",
    "        _dtY = initialTimeY[selector]\n",
    "        initialTimeX = rowsX[:,4]-rowsX[:,2]\n",
    "        _dtX = initialTimeX[selector]\n",
    "        problem = _dtX > 1e9\n",
    "    elif (_workType == numWorkTypes+1):\n",
    "        # Convolve second part (excluding mutex acquisitions)\n",
    "        # First consider case with two mutex acquisitions [which is actually obsolete now]\n",
    "        selector = (rowsY[:,1]==workType) & (rowsY[:,6]!=0)\n",
    "        initialTimeY = rowsY[:,6]-rowsY[:,5] + rowsY[:,3]-rowsY[:,7]\n",
    "        _dtY = initialTimeY[selector]\n",
    "        selector = (rowsX[:,1]==workType) & (rowsX[:,6]!=0)\n",
    "        initialTimeX = rowsX[:,6]-rowsX[:,5] + rowsX[:,3]-rowsX[:,7]\n",
    "        _dtX = initialTimeX[selector]\n",
    "        # Now consider case with one mutex acquisition\n",
    "        # Note that this graph won't make much sense if we compare new code with old, \n",
    "        # since the work has been reshuffled a bit\n",
    "        selector = (rowsY[:,1]==workType) & (rowsY[:,6]==0)\n",
    "        initialTimeY = rowsY[:,3]-rowsY[:,5]\n",
    "        _dtY = np.append(_dtY, initialTimeY[selector])\n",
    "        selector = (rowsX[:,1]==workType) & (rowsX[:,6]==0)\n",
    "        initialTimeX = rowsX[:,3]-rowsX[:,5]\n",
    "        _dtX = np.append(_dtX, initialTimeX[selector])\n",
    "    elif (_workType == numWorkTypes+2):\n",
    "        # Convolve mutex time\n",
    "        selector = (rowsX[:,1]==workType)\n",
    "        _dtY = mtY[selector]\n",
    "        _dtX = mtX[selector]\n",
    "    else:\n",
    "        assert(workType < numWorkTypes)\n",
    "        selector = rowsX[:,1]==workType\n",
    "        _dtY = dtY[selector]\n",
    "        _dtX = dtX[selector]\n",
    "    avY = np.average(_dtY)\n",
    "    avX = np.average(_dtX)\n",
    "    gradient = avY / avX\n",
    "    # Clip outliers, to avoid lots of whitespace in the plots\n",
    "    _dtX = np.minimum(_dtX, ranges[_workType][1])\n",
    "    _dtY = np.minimum(_dtY, gradient*ranges[_workType][1])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.title(lab[_workType])\n",
    "    plt.xlabel(fileX)\n",
    "    plt.ylabel(fileY)\n",
    "    plt.plot(_dtX, _dtY, '.')\n",
    "    plt.plot(ranges[_workType], ranges[_workType])\n",
    "    plt.plot(ranges[_workType], np.array(ranges[_workType])*gradient)\n",
    "    plt.plot([avX, avX, 0], [0, avY, avY])\n",
    "    plt.show()\n",
    "    print(lab[_workType], np.sum(_dtX), np.sum(_dtY), gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These timings are old ones before I speeded up the convolution code a little bit.\n",
    "As a result, these specific numbers are outdated, but I reckon the general theme remains, and I am not going to rerun them all right now...\n",
    "\n",
    "## With one timepoint\n",
    "\n",
    "### 1 z plane:\n",
    "Wall 2.44, run 19.32, active 12.59, mutex 6.73, idle frac 0.35\n",
    "Parallelism 5.2 (Convolve: 1.03, Accum: 0.97)\n",
    "\n",
    "Time breakdown: FH 9.27, mirror 0.81, convolve 2.51, convolve/mutex 6.73\n",
    "### 2 z planes:\n",
    "Wall 3.68, run 28.93, active 26.25, mutex 2.68, idle frac 0.09\n",
    "Parallelism 7.3 (Convolve: 1.44, Accum: 1.37)\n",
    "Time breakdown: FH 19.01, mirror 1.92, convolve 5.31, convolve/mutex 2.68\n",
    "\n",
    "### 4 z planes:\n",
    "Run 49.41, active 49.11, mutex 0.30, idle frac 0.01\n",
    "Parallelism 8.0\n",
    "\n",
    "Time breakdown: FH 34.83, mirror 4.01, convolve 10.27, convolve/mutex 0.30\n",
    "### Next 4 z planes:\n",
    "Run 49.40, active 49.13, mutex 0.27, idle frac 0.01\n",
    "Parallelism 8.0\n",
    "### 8 z planes:\n",
    "Run 108.06, active 108.06, mutex 0.00, idle frac 0.00\n",
    "Parallelism 8.0\n",
    "\n",
    "## With two timepoints\n",
    "\n",
    "### 1 z plane:\n",
    "Wall 2.90, run 23.01, active 15.99, mutex 7.02, idle frac 0.31\n",
    "Parallelism 5.6 (Convolve: 1.67, Accum: 1.59)\n",
    "Time breakdown: FH 10.18, mirror 0.96, convolve 4.85, convolve/mutex 7.02\n",
    "\n",
    "### 2 z planes:\n",
    "Run 30.14, active 28.15, mutex 2.00, idle frac 0.07\n",
    "Parallelism 7.5\n",
    "\n",
    "Time breakdown: FH 15.84, mirror 2.37, convolve 9.94, convolve/mutex 2.00\n",
    "\n",
    "### 3 z planes:\n",
    "Run 35.87, active 34.34, mutex 1.53, idle frac 0.04\n",
    "Parallelism 7.7\n",
    "### 4 z planes:\n",
    "Run 64.77, active 64.75, mutex 0.02, idle frac 0.00\n",
    "Parallelism 8.0 (Convolve: 2.80)\n",
    "\n",
    "Time breakdown: FH 37.32, mirror 4.55, convolve 22.87, convolve/mutex 0.02\n",
    "\n",
    "## With 16 timepoints\n",
    "### 1 z plane\n",
    "Run 51.78, active 51.78, mutex 0.01, idle frac 0.00\n",
    "Parallelism 8.0 (Convolve: 6.51)\n",
    "\n",
    "Time breakdown: FH 8.25, mirror 1.36, convolve 42.16, convolve/mutex 0.01\n",
    "\n",
    "## With 32 timepoints\n",
    "### 1 z plane\n",
    "Run 87.08, active 87.07, mutex 0.01, idle frac 0.00\n",
    "Parallelism 8.0 (Convolve: 7.11)\n",
    "\n",
    "Time breakdown: FH 8.36, mirror 1.26, convolve 77.46, convolve/mutex 0.01\n",
    "\n",
    "## Summary\n",
    "More time is being lost waiting for a mutex for 1z,2t compared to 2z,1t. This might just be because there is less FH work to be getting on with, and so there are more threads ready to do work.\n",
    "\n",
    "In fact, with 1z,1t, while threads are inevitably sitting idle I am actually making pretty good use of time - the accumulator mutex is held by somebody almost all the time. However, we are being less efficient with the 2z,t1 and 1z,2t cases. There, we often only hold one of the mutexes at a time. It might be possible to adjust the scheduling code to be more effective with the mutexes (which I think in practice means getting a bit ahead of ourselves with FH rather than waiting until we run out of work entirely). An alternative strategy would be to create additional temporary accumulators and combine them at the end, to support more parallelism. Some perspective though: I do care about the 1z,2t case, but I could only speed it up by 33% (respectable but not earth-shattering); the 4z,2t case is probably more representative of Nils' data, and that has no idle time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
