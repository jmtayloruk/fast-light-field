{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww21260\viewh16500\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Flow recovery - what I\'92ve done\

\b0 - I am using a local optimizer, of course, so I need to choose a plausible starting point or things go crazy. That\'92s ok: I should have a decent estimate from PIV analysis of artefact-corrupted images\
- It turns out that the naive method works surprisingly well with large blurry features - I suppose that\'92s not surprising, since effectively the resolution of the flow analysis is greater than the lenslet pitch. It struggles more when I run it with small blobs (thank goodness!).\
- BFGS actually doesn\'92t seem to converge to the solution (it fails for some reason). Powell seems more reliable.\
- I discovered that edge effects with warp() were distorting the solution in random ways. That may well explain why BFGS was failing to find a solution!\
\

\b Flow recovery - next steps\

\b0 \
- warp() has some anomalies with the edge of the image. Specifically, pixels are either \'91in\'92 or \'91out\'92, and the switch between the two states causes jumps in the score. It\'92s not clear what the best thing to do about this is. mode=\'92edge\'92 might work, but I would worry it might introduce odd biasing behaviour. Extending the object size and then cropping it down (i.e. filling with real data) might be more realistic. I think that would be a reasonable thing to do - I can\'92t see a problem with it (aside from non-square images being slightly slower, although other issues are keeping me well away from that being the limiting factor at the moment).\
\
- Aargh, warp still has edge anomalies for the joint recovery! It\'92s not enough to ignore the edge pixels, because the edge pixels affect a large area when I forward project. A combination of forcing zero edge shifts 
\i and
\i0  specifying edge padding will hopefully suppress edge artefacts enough to stop instability in the scores I get for basically-the-same shift.\
\
- See how the naive algorithm fares with a larger peak velocity (12 instead of 7). Does it still consistently underestimate? By more\'85? Actually no, it does ok. I should maybe investigate somewhat systematically how it fares with different shift values. And also go back to uniform shifts, because I thought I\'92d observed that it really didn\'92t perform properly in that case - but maybe I\'92ve since fixed a bug in that code.\
\
- I should maybe revisit BFGS now I have excluded edge effects from warp()\'85\
\
- Save the input image (maybe with a timestamp in the filename to avoid overwriting?), so that I can reproduce previous runs\
\
- Plot the results I am returning to the Powell algorithm. I think there may be something odd about how it\'92s behaving, and that may be affecting convergence. It seems as if tiny changes to the shifts (~1e-4) are making a reasonably large difference to the overall score actually. I should investigate why that is. e.g. changing parameter #5 by 1e-4 or less from its designated value.\
\
- Look a bit at the internals of Powell. Does it really need as many trial values to reach an optimum, given that I think my cost function is pretty smooth and quadratic?
\b \

\b0 \
- Think about convergence criterion, and ensuring behaviour is similar for different input intensities. How much does it help to choose a more relaxed xtol for Powell? Actually, choosing 1e-1 instead of 1e-2 seems to take 
\i longer
\i0  to converge. Also, it seems to be evaluating shifts to a smaller granularity than e.g. 1e-2. Is that definitely correct, or have I got the settings wrong somehow? See comment above about tiny changes.\
\
- Consider expanding the PIV code to more than one z plane\

\b \
Things to investigate
\b0 \
- deconvRL and _PIV actually give slightly different results (deconvRL seems better). Is there an actual bug in deconvRL_PIV, or is Prevedel\'92s implementation of RL actually slightly different (and more efficient)?\
\
I noted a while back that forwardProjectACC returns float64. I should track down where that happens and reduce to float32, for performance reasons.\
\

\b \
Performance
\b0 \
- For much of the time, my PIV code is not using multithreading. It looks as if it takes ~0.2s to fire up multiprocessing (and about 0.1s to tear down). 0.15s seems to be spent after the multithreaded part.\
- Need to upgrade code to get performance gains when processing multiple images simultaneously, instead of just iterating over them. This would also help with the overheads I identified above, I hope.\
- The optimizer calculates the full gradients even if some variables are bounded to a constant. I should be able to do better if I write my own gradient function. That would also have the benefit that I should be able to parallelize the computations much better. Similarly, with a bit of faffing around rewriting the code myself, I should be able to run a Powell optimizer that tweaks all individual flow components in parallel.\
}