{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww21260\viewh16500\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Single-pixel imaging
\f1\b0 \
This seems as good as anywhere to add a note about single-pixel imaging (which is another application where I think my approach is plausible). I need to remember this as a reference:\
https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-6-7889\
They are just using a moving target over a static illumination pattern (don\'92t know if they referenced the japanese Science flow cytometry paper\'85!), but I should still bear it in mind as a citation.
\f0\b \
\
Flow recovery - what I\'92ve done\

\f1\b0 - I am using a local optimizer, of course, so I need to choose a plausible starting point or things go crazy. That\'92s ok: I should have a decent estimate from PIV analysis of artefact-corrupted images\
- It turns out that the naive method works surprisingly well with large blurry features - I suppose that\'92s not surprising, since effectively the resolution of the flow analysis is greater than the lenslet pitch. It struggles more when I run it with small blobs (thank goodness!).\
- BFGS actually doesn\'92t seem to converge to the solution (it fails for some reason). Powell seems more reliable.\
- I discovered that edge effects with warp() were distorting the solution in random ways. That may well explain why BFGS was failing to find a solution! I battled with warp() issues a lot. These included:\
	- warp() leads to anomalies at the edge of the image. Specifically, pixels are either \'91in\'92 or \'91out\'92, and the switch between the two states causes jumps in the score. It\'92s not clear what the best thing to do about this is. mode=\'92edge\'92 might work, but I would worry it might introduce odd biasing behaviour. I tried extending the object size and then cropping it down (i.e. filling with real data).\
	- It turned out it wasn\'92t enough to ignore the edge pixels, because the edge pixels affect a large area when I forward project. A combination of forcing zero edge shifts and specifying edge padding seems to have suppressed edge artefacts enough to stop instability in the scores I get for two virtually-identical shift values.\
\

\f0\b Flow recovery - next steps
\f1\b0 \
- See how the naive algorithm fares with a larger peak velocity (12 instead of 7). Does it still consistently underestimate? By more\'85? Actually no, it does ok. I should maybe investigate somewhat systematically how it fares with different shift values. And also go back to uniform shifts, because I thought I\'92d observed that it really didn\'92t perform properly in that case - but maybe I\'92ve since fixed a bug in that code.\
\
- I should maybe revisit BFGS now I have excluded edge effects from warp()\'85\
\
- Save the input image (maybe with a timestamp in the filename to avoid overwriting?), so that I can reproduce previous runs\
\
- Can I estimate the 
\f2\i gradients
\f1\i0  of the problem by just warping the 
\f2\i same
\f1\i0  recovered object and scoring it? I am pretty sure that will be imperfect, but I have a feeling it might be close enough to the truth to send the optimizer in the right direction. Worth a try! Should be pretty quick to implement, I think. I can also compare it to reality by running some full recoveries with W+dW and determining the true gradient that way.\
\
- Look a bit at the internals of Powell. Does it really need as many trial values to reach an optimum, given that I think my cost function is pretty smooth and quadratic?
\f0\b \

\f1\b0 \
- Think about convergence criterion, and ensuring behaviour is similar for different input intensities. How much does it help to choose a more relaxed xtol for Powell? Actually, choosing 1e-1 instead of 1e-2 seems to take 
\f2\i longer
\f1\i0  to converge. Also, it seems to be evaluating shifts to a smaller granularity than e.g. 1e-2. Is that definitely correct, or have I got the settings wrong somehow? See comment above about tiny changes.\
\
- Consider expanding the PIV code to more than one z plane\

\f0\b \
Things to debug\

\f1\b0 - There is an issue with the dual deconvolution self-test failing on the GPU (optic-tectum). 
\f0\b I need to debug that. 
\f1\b0 It\'92s suspicious that it\'92s exactly 0.5 disagreement\'85\
- There also seems to be a disagreement of exactly 0.25 on the CPU on beag-shuil\'85\
- Performance on optic-tectum does not seem to match my recorded benchmark any more. Seems to match on other machines. I am not sure why this is, and will probably not investigate further\
- I had to relax the accuracy condition for lfdeconv parallel-threading with 3 or 6 threads, otherwise I see a disagreement of 1.2e-4 for 6 threads (always) and for 3 threads (sometimes?). I think that is probably OK, but I should look at that more closely.\
- deconvRL and _PIV actually give slightly different results (deconvRL seems better). Is there an actual bug in deconvRL_PIV, or is Prevedel\'92s implementation of RL actually slightly different (and more efficient)?\
- If I keep seeing problems where skimage sometimes crashes while running (fix by reinstalling skimage) then I may have to try and debug this further\'85\

\f0\b Enhancements\

\f1\b0 - Simple UI improvement: warn or somehow cope if caller provides superfluous trailing digits on the .mat filename, e.g. n1.0. That will lead to inscrutable errors during matrix generation.\
- I would like to update the test code in lfdeconv.py so it behaves like lf_performance_analysis.py - see TODO note.
\f0\b \

\f1\b0 - Should probably make the default output from the self-test routines less verbose\
- Decide what to do in the presence of multiple GPUs. Probably ask the user which one to use - using multiple GPUs would be a big undertaking for me.\

\f0\b \
Performance
\f1\b0 \
- The optimizer calculates the full gradients even if some variables are bounded to a constant. I should be able to do better if I write my own gradient function. Writing my own gradient function would also have the benefit that I should be able to parallelize the computations much better. Similarly, with a bit of faffing around rewriting the code myself, I should be able to run a Powell optimizer that tweaks all individual flow components in parallel.\
}